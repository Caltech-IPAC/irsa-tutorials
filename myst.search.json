{"version":"1","records":[{"hierarchy":{"lvl1":"Caltech/IPAC—IRSA Python Notebook Tutorials"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Caltech/IPAC—IRSA Python Notebook Tutorials"},"content":"These Python Jupyter Notebook tutorials demonstrate access methods and techniques for working with data served by the \n\nNASA/IPAC Infrared Science Archive (IRSA).\nThey cover topics like querying IRSA, working with catalogs in Parquet format, visualizing with Firefly, and general other techniques.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Caltech/IPAC—IRSA Python Notebook Tutorials","lvl2":"About these notebooks"},"type":"lvl2","url":"/#about-these-notebooks","position":2},{"hierarchy":{"lvl1":"Caltech/IPAC—IRSA Python Notebook Tutorials","lvl2":"About these notebooks"},"content":"Authors: IRSA Scientists and Developers wrote and maintain these notebooks.\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/#about-these-notebooks","position":3},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images"},"type":"lvl1","url":"/sia-2mass-allsky","position":0},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images"},"content":"This notebook tutorial demonstrates the process of querying IRSA’s Simple Image Access (SIA) service for the 2MASS All-Sky Atlas, making a cutout image (thumbnail), and displaying the cutout.\n\n\n\n","type":"content","url":"/sia-2mass-allsky","position":1},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Learning Goals"},"type":"lvl2","url":"/sia-2mass-allsky#learning-goals","position":2},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nLearn how to search the NASA Astronomical Virtual Observatory Directory web portal for a service that provides access to IRSA’s 2MASS images.\n\nUse the Python pyvo package to identify which of IRSA’s 2MASS images cover a specified coordinate.\n\nDownload one of the identified images.\n\nCreate and display a cutout of the downloaded image.\n\n","type":"content","url":"/sia-2mass-allsky#learning-goals","position":3},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Introduction"},"type":"lvl2","url":"/sia-2mass-allsky#introduction","position":4},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Introduction"},"content":"The Two Micron All Sky Survey (2MASS) project uniformly scanned the entire sky in three near-infrared bands to detect and characterize point sources brighter than about 1 mJy in each band, with signal-to-noise ratio (SNR) greater than 10. More information about 2MASS can be found at:\n\nhttps://​irsa​.ipac​.caltech​.edu​/Missions​/2mass​.html\n\nThe \n\nNASA/IPAC Infrared Science Archive (IRSA) at Caltech is the archive for 2MASS images and catalogs. The 2MASS images that are the subject of this tutorial are made accessible via the \n\nInternational Virtual Observatory Alliance (IVOA) \n\nSimple Image Access (SIA) protocol. IRSA’s 2MASS SIA service is registered in the NASA Astronomical Virtual Observatory (NAVO) \n\nDirectory. Based on the registered information, the Python package \n\npyvo can be used to query the 2MASS SIA service for a list of images that meet specified criteria, and standard Python libraries can be used to download and manipulate the images.\nOther datasets at IRSA are available through other SIA services:\n\nhttps://​irsa​.ipac​.caltech​.edu​/docs​/program​_interface​/api​_images​.html\n\n","type":"content","url":"/sia-2mass-allsky#introduction","position":5},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Imports"},"type":"lvl2","url":"/sia-2mass-allsky#imports","position":6},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Imports"},"content":"pyvo for querying IRSA’s 2MASS SIA service\n\nastropy.coordinates for defining coordinates\n\nastropy.nddata for creating an image cutout\n\nastropy.wcs for interpreting the World Coordinate System header keywords of a fits file\n\nastropy.units for attaching units to numbers passed to the SIA service\n\nmatplotlib.pyplot for plotting\n\nastropy.utils.data for downloading files\n\nastropy.io to manipulate FITS files\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install matplotlib astropy pyvo\n\n\n\n\n\nimport pyvo as vo\nfrom astropy.coordinates import SkyCoord\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom astropy.utils.data import download_file\nfrom astropy.io import fits\n\n\n\n","type":"content","url":"/sia-2mass-allsky#imports","position":7},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 1 - Setup"},"type":"lvl2","url":"/sia-2mass-allsky#section-1-setup","position":8},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 1 - Setup"},"content":"\n\nSet images to display in the notebook\n\n%matplotlib inline\n\n\n\nDefine coordinates of a bright star\n\nra = 314.30417\ndec = 77.595559\npos = SkyCoord(ra=ra, dec=dec, unit='deg')\n\n\n\n","type":"content","url":"/sia-2mass-allsky#section-1-setup","position":9},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 2 - Lookup and define a service for 2MASS images"},"type":"lvl2","url":"/sia-2mass-allsky#section-2-lookup-and-define-a-service-for-2mass-images","position":10},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 2 - Lookup and define a service for 2MASS images"},"content":"\n\nStart at STScI VAO Registry at \n\nhttps://​vao​.stsci​.edu​/keyword​-search/\n\nLimit by Publisher “NASA/IPAC Infrared Science Archive” and Capability Type “Simple Image Access Protocol” then search on “2MASS”\n\nLocate the SIA URL \n\nhttps://​irsa​.ipac​.caltech​.edu​/cgi​-bin​/2MASS​/IM​/nph​-im​_sia​?type​=​at​&​ds​=​asky&​\n\ntwomass_service = vo.dal.SIAService(\"https://irsa.ipac.caltech.edu/cgi-bin/2MASS/IM/nph-im_sia?type=at&ds=asky&\")\n\n\n\n","type":"content","url":"/sia-2mass-allsky#section-2-lookup-and-define-a-service-for-2mass-images","position":11},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 3 - Search the service"},"type":"lvl2","url":"/sia-2mass-allsky#section-3-search-the-service","position":12},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 3 - Search the service"},"content":"\n\nSearch for images covering within 1 arcsecond of the star\n\nim_table = twomass_service.search(pos=pos, size=1.0*u.arcsec)\n\n\n\nExamine the table of images that is returned\n\nim_table.to_table()\n\n\n\n","type":"content","url":"/sia-2mass-allsky#section-3-search-the-service","position":13},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 4 - Locate and download an image of interest"},"type":"lvl2","url":"/sia-2mass-allsky#section-4-locate-and-download-an-image-of-interest","position":14},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 4 - Locate and download an image of interest"},"content":"\n\nLocate the first H-band image and display its URL\n\nfor i in range(len(im_table)):\n    if im_table[i]['band'] == 'H':\n        break\nprint(im_table[i].getdataurl())\n\n\n\nDownload the image and open it in Astropy\n\nfname = download_file(im_table[i].getdataurl(), cache=True)\nimage1 = fits.open(fname)\n\n\n\n","type":"content","url":"/sia-2mass-allsky#section-4-locate-and-download-an-image-of-interest","position":15},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 5 - Extract a cutout and plot it"},"type":"lvl2","url":"/sia-2mass-allsky#section-5-extract-a-cutout-and-plot-it","position":16},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Section 5 - Extract a cutout and plot it"},"content":"\n\nwcs = WCS(image1[0].header)\n\n\n\n\n\ncutout = Cutout2D(image1[0].data, pos, (60, 60), wcs=wcs)\nwcs = cutout.wcs\n\n\n\n\n\nfig = plt.figure()\n\nax = fig.add_subplot(1, 1, 1, projection=wcs)\nax.imshow(cutout.data, cmap='gray_r', origin='lower',\n          vmax = 1000)\nax.scatter(ra, dec, transform=ax.get_transform('fk5'), s=500, edgecolor='red', facecolor='none')\n\n\n\n","type":"content","url":"/sia-2mass-allsky#section-5-extract-a-cutout-and-plot-it","position":17},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Exercise"},"type":"lvl2","url":"/sia-2mass-allsky#exercise","position":18},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Exercise"},"content":"\n\nRepeat the steps above to retrieve a cutout from the AllWISE Atlas images\n\n\n\n\n\n\n\n","type":"content","url":"/sia-2mass-allsky#exercise","position":19},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"About this notebook"},"type":"lvl2","url":"/sia-2mass-allsky#about-this-notebook","position":20},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"About this notebook"},"content":"\n\nAuthor: David Shupe, IRSA Scientist, and the IRSA Science Team\n\nUpdated: 2023-02-16\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.\n\n","type":"content","url":"/sia-2mass-allsky#about-this-notebook","position":21},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Citations"},"type":"lvl2","url":"/sia-2mass-allsky#citations","position":22},{"hierarchy":{"lvl1":"Searching for 2MASS All-Sky Atlas Images","lvl2":"Citations"},"content":"\n\nIf you use astropy for published research, please cite the authors. Follow these links for more information about citing astropy:\n\nCiting astropy\n\nIf you use 2MASS data in published research, please cite the canonical paper \n\nSkrutskie et al (2006), and include the following standard acknowledgment:\n\n“This publication makes use of data products from the Two Micron All Sky Survey, which is a joint project of the University of Massachusetts and the Infrared Processing and Analysis Center/California Institute of Technology, funded by the National Aeronautics and Space Administration and the National Science Foundation.”\n\nPlease also cite the doi for the 2MASS All-Sky Atlas Image Service at \n\nIPAC (2020)","type":"content","url":"/sia-2mass-allsky#citations","position":23},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images"},"type":"lvl1","url":"/sia-cosmos","position":0},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images"},"content":"This notebook tutorial demonstrates the process of querying IRSA’s Simple Image Access (SIA) service for the COSMOS images, making a cutout image (thumbnail), and displaying the cutout.\n\n","type":"content","url":"/sia-cosmos","position":1},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Learning Goals"},"type":"lvl2","url":"/sia-cosmos#learning-goals","position":2},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nLearn how to search the NASA Astronomical Virtual Observatory Directory web portal for a service that provides access to IRSA’s COSMOS images.\n\nUse the Python pyvo package to identify which of IRSA’s COSMOS images cover a specified coordinate.\n\nDownload one of the identified images.\n\nCreate and display a cutout of the downloaded image.\n\n","type":"content","url":"/sia-cosmos#learning-goals","position":3},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Introduction"},"type":"lvl2","url":"/sia-cosmos#introduction","position":4},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Introduction"},"content":"The COSMOS Archive serves data taken for the Cosmic Evolution Survey with HST (COSMOS) project, using IRSA’s general search service, Atlas. COSMOS is an HST Treasury Project to survey a 2 square degree equatorial field with the ACS camera. For more information about COSMOS, see:\n\nhttps://​irsa​.ipac​.caltech​.edu​/Missions​/cosmos​.html\n\nThe \n\nNASA/IPAC Infrared Science Archive (IRSA) at Caltech is one of the archives for COSMOS images and catalogs. The COSMOS images that are the subject of this tutorial are made accessible via the \n\nInternational Virtual Observatory Alliance (IVOA) \n\nSimple Image Access (SIA) protocol. IRSA’s SEIP SIA service is registered in the NASA Astronomical Virtual Observatory (NAVO) \n\nDirectory. Based on the registered information, the Python package \n\npyvo can be used to query the SIA service for a list of images that meet specified criteria, and standard Python libraries can be used to download and manipulate the images.\nOther datasets at IRSA are available through other SIA services:\n\nhttps://​irsa​.ipac​.caltech​.edu​/docs​/program​_interface​/api​_images​.html\n\n","type":"content","url":"/sia-cosmos#introduction","position":5},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Imports"},"type":"lvl2","url":"/sia-cosmos#imports","position":6},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Imports"},"content":"pyvo for querying IRSA’s COSMOS SIA service\n\nastropy.coordinates for defining coordinates\n\nastropy.nddata for creating an image cutout\n\nastropy.wcs for interpreting the World Coordinate System header keywords of a fits file\n\nastropy.units for attaching units to numbers passed to the SIA service\n\nmatplotlib.pyplot for plotting\n\nastropy.utils.data for downloading files\n\nastropy.io to manipulate FITS files\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install matplotlib astropy pyvo\n\n\n\n\n\nimport pyvo as vo\nfrom astropy.coordinates import SkyCoord\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom astropy.utils.data import download_file\nfrom astropy.io import fits\n\n\n\n","type":"content","url":"/sia-cosmos#imports","position":7},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 1 - Setup"},"type":"lvl2","url":"/sia-cosmos#section-1-setup","position":8},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 1 - Setup"},"content":"\n\nSet images to display in the notebook\n\n%matplotlib inline\n\n\n\nDefine coordinates of a bright source\n\nra = 149.99986\ndec = 2.24875\npos = SkyCoord(ra=ra, dec=dec, unit='deg')\n\n\n\n","type":"content","url":"/sia-cosmos#section-1-setup","position":9},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 2 - Lookup and define a service for COSMOS images"},"type":"lvl2","url":"/sia-cosmos#section-2-lookup-and-define-a-service-for-cosmos-images","position":10},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 2 - Lookup and define a service for COSMOS images"},"content":"\n\nStart at STScI VAO Registry at \n\nhttps://​vao​.stsci​.edu​/keyword​-search/\n\nLimit by Publisher “NASA/IPAC Infrared Science Archive” and Capability Type “Simple Image Access Protocol” then search on “COSMOS”\n\nLocate the SIA2 URL \n\nhttps://​irsa​.ipac​.caltech​.edu​/cgi​-bin​/Atlas​/nph​-atlas​?mission​=​COSMOS​&​hdr​_location​=​\\COSMOSDataPath\\​&​collection​_desc​=​Cosmic+Evolution+Survey+with+HST+(COSMOS)​&​SIAP​_ACTIVE​=​1&​\n\ncosmos_service = vo.dal.SIAService(\"https://irsa.ipac.caltech.edu/cgi-bin/Atlas/nph-atlas?mission=COSMOS&hdr_location=%5CCOSMOSDataPath%5C&collection_desc=Cosmic+Evolution+Survey+with+HST+%28COSMOS%29&SIAP_ACTIVE=1&\")\n\n\n\n","type":"content","url":"/sia-cosmos#section-2-lookup-and-define-a-service-for-cosmos-images","position":11},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 3 - Search the service"},"type":"lvl2","url":"/sia-cosmos#section-3-search-the-service","position":12},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 3 - Search the service"},"content":"\n\nSearch for images covering within 1 arcsecond of the star\n\nim_table = cosmos_service.search(pos=pos, size=1.0*u.arcsec)\n\n\n\nInspect the table of images that is returned\n\nim_table\n\n\n\n\n\nim_table.to_table().colnames\n\n\n\nView the first ten entries of the table\n\nim_table.to_table()[:10]\n\n\n\n","type":"content","url":"/sia-cosmos#section-3-search-the-service","position":13},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 4 - Locate and download an image of interest"},"type":"lvl2","url":"/sia-cosmos#section-4-locate-and-download-an-image-of-interest","position":14},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 4 - Locate and download an image of interest"},"content":"\n\nLocate the first image in the band_name of i+\n\nfor i in range(len(im_table)):\n    if im_table[i]['band_name'] == 'i+':\n        break\nprint(im_table[i].getdataurl())\n\n\n\nDownload the image\n\nfname = download_file(im_table[i].getdataurl(), cache=True)\nimage1 = fits.open(fname)\n\n\n\n","type":"content","url":"/sia-cosmos#section-4-locate-and-download-an-image-of-interest","position":15},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 5 - Extract a cutout and plot it"},"type":"lvl2","url":"/sia-cosmos#section-5-extract-a-cutout-and-plot-it","position":16},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Section 5 - Extract a cutout and plot it"},"content":"\n\nwcs = WCS(image1[0].header)\n\n\n\nMake a cutout centered on the position\n\ncutout = Cutout2D(image1[0].data, pos, (60, 60), wcs=wcs)\nwcs = cutout.wcs\n\n\n\n\n\nfig = plt.figure()\n\nax = fig.add_subplot(1, 1, 1, projection=wcs)\nax.imshow(cutout.data, cmap='gray_r', origin='lower')\nax.scatter(ra, dec, transform=ax.get_transform('fk5'), s=500, edgecolor='red', facecolor='none')\n\n\n\n\n\n","type":"content","url":"/sia-cosmos#section-5-extract-a-cutout-and-plot-it","position":17},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"About this notebook"},"type":"lvl2","url":"/sia-cosmos#about-this-notebook","position":18},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"About this notebook"},"content":"\n\nAuthor: David Shupe, IRSA Scientist, and the IRSA Science Team\n\nUpdated: 2022-02-14\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.\n\n","type":"content","url":"/sia-cosmos#about-this-notebook","position":19},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Citations"},"type":"lvl2","url":"/sia-cosmos#citations","position":20},{"hierarchy":{"lvl1":"Searching for contributed COSMOS images","lvl2":"Citations"},"content":"\n\nIf you use astropy for published research, please cite the authors. Follow these links for more information about citing astropy:\n\nCiting astropy\n\nIf you use COSMOS ACS imaging data in published research,  please cite the dataset Digital Object Identifier (DOI): \n\nCOSMOS Project (2020).","type":"content","url":"/sia-cosmos#citations","position":21},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics"},"type":"lvl1","url":"/euclid-intro-mer-images","position":0},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics"},"content":"","type":"content","url":"/euclid-intro-mer-images","position":1},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-intro-mer-images#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Learning Goals"},"content":"\n\nBy the end of this tutorial, you will:\n\nunderstand the basic characteristics of Euclid Q1 MER mosaics;\n\nknow how to download full MER mosaics;\n\nknow how to make smaller cutouts of MER mosaics;\n\nknow how to use matplotlib to plot a grid of cutouts;\n\nknow how to identify sources in the cutouts and make basic measurements.\n\n","type":"content","url":"/euclid-intro-mer-images#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Introduction"},"type":"lvl2","url":"/euclid-intro-mer-images#introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Introduction"},"content":"\n\nEuclid launched in July 2023 as a European Space Agency (ESA) mission with involvement by NASA.\nThe primary science goals of Euclid are to better understand the composition and evolution of the dark Universe.\nThe Euclid mission is providing space-based imaging and spectroscopy as well as supporting ground-based imaging to achieve these primary goals.\nThese data will be archived by multiple global repositories, including IRSA, where they will support transformational work in many areas of astrophysics.\n\nEuclid Quick Release 1 (Q1) consists of consists of ~30 TB of imaging, spectroscopy, and catalogs covering four non-contiguous fields:\nEuclid Deep Field North (22.9 sq deg), Euclid Deep Field Fornax (12.1 sq deg), Euclid Deep Field South (28.1 sq deg), and LDN1641.\n\nAmong the data products included in the Q1 release are the Level 2 MER mosaics.\nThese are multiwavelength mosaics created from images taken with the Euclid instruments (VIS and NISP), as well as a variety of ground-based telescopes.\nAll of the mosaics have been created according to a uniform tiling on the sky, and mapped to a common pixel scale.\nThis notebook provides a quick introduction to accessing MER mosaics from IRSA.\nIf you have questions about it, please contact the \n\nIRSA helpdesk.\n\n","type":"content","url":"/euclid-intro-mer-images#introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Data volume"},"type":"lvl2","url":"/euclid-intro-mer-images#data-volume","position":6},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Data volume"},"content":"Each MER image is approximately 1.47 GB. Downloading can take some time.\n\n","type":"content","url":"/euclid-intro-mer-images#data-volume","position":7},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Imports"},"type":"lvl2","url":"/euclid-intro-mer-images#imports","position":8},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Imports"},"content":"Important\n\nWe rely on astroquery and sep features that have been recently added, so please make sure you have the respective version v0.4.10 and v1.4 or newer installed.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install numpy 'astropy>=5.3' matplotlib 'astroquery>=0.4.10' 'sep>=1.4' fsspec\n\n\n\n\n\nimport re\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.nddata import Cutout2D\nfrom astropy.utils.data import download_file\nfrom astropy.visualization import ImageNormalize, PercentileInterval, AsinhStretch,  ZScaleInterval, SquaredStretch\nfrom astropy.wcs import WCS\nfrom astropy import units as u\n\nfrom astroquery.ipac.irsa import Irsa\nimport sep\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#imports","position":9},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"1. Search for multiwavelength Euclid Q1 MER mosaics that cover the star HD 168151"},"type":"lvl2","url":"/euclid-intro-mer-images#id-1-search-for-multiwavelength-euclid-q1-mer-mosaics-that-cover-the-star-hd-168151","position":10},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"1. Search for multiwavelength Euclid Q1 MER mosaics that cover the star HD 168151"},"content":"Below we set a search radius of 10 arcsec and convert the name “HD 168151” into coordinates.\n\nsearch_radius = 10 * u.arcsec\ncoord = SkyCoord.from_name('HD 168151')\n\n\n\nUse IRSA’s Simple Image Access (SIA) API to search for all Euclid MER mosaics that overlap with the search region you have specified. We specify the euclid_DpdMerBksMosaic “collection” because it lists all of the multiwavelength MER mosaics, along with their associated catalogs.\n\nTip\n\nThe IRSA SIA collections can be listed using using the list_collections method, we can filter on the ones containing “euclid” in the collection name:Irsa.list_collections(filter='euclid')\n\nThere are currently four collections available:\n\n'euclid_ero' -- the Early Release Observations, for more information see \n\nEuclid ERO documentation at IRSA.\n\n'euclid_DpdMerBksMosaic' -- This is the collection of interest for us in this notebook, it contains all the Q1 MER mosaic data. For more information on MER mosaics, see the “Introduction” above.\n\n'euclid_DpdVisCalibratedQuadFrame' -- This collection contains the VIS instrument Calibrated images. Please note these data are not stacked, so there are four dither positions for every observation ID and two calibration frames.\n\n'euclid_DpdNirCalibratedFrame' -- This collection contains the NIR instrument Calibrated images. Please note these data are not stacked, so there are four dither positions for every observation ID.\n\nFor more information on the VIS and NIR calibrated frames, please read the \n\nIRSA User Guide\n\nIn this notebook, we will focus on downloading and visualizing the MER mosaic data.\n\nimage_table = Irsa.query_sia(pos=(coord, search_radius), collection='euclid_DpdMerBksMosaic')\n\n\n\nThis table lists all MER mosaic images available in this search position. These mosaics include the Euclid VIS, Y, J, H images, as well as ground-based telescopes which have been put on the same pixel scale. For more information, see the \n\nEuclid documentation at IPAC.\n\nNote that there are various image types are returned as well, we filter out the science images from these:\n\nscience_images = image_table[image_table['dataproduct_subtype'] == 'science']\nscience_images\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#id-1-search-for-multiwavelength-euclid-q1-mer-mosaics-that-cover-the-star-hd-168151","position":11},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"2. Retrieve a Euclid Q1 MER mosaic image in the VIS bandpass"},"type":"lvl2","url":"/euclid-intro-mer-images#id-2-retrieve-a-euclid-q1-mer-mosaic-image-in-the-vis-bandpass","position":12},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"2. Retrieve a Euclid Q1 MER mosaic image in the VIS bandpass"},"content":"\n\n","type":"content","url":"/euclid-intro-mer-images#id-2-retrieve-a-euclid-q1-mer-mosaic-image-in-the-vis-bandpass","position":13},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Let’s first look at one example full image, the VIS image","lvl2":"2. Retrieve a Euclid Q1 MER mosaic image in the VIS bandpass"},"type":"lvl3","url":"/euclid-intro-mer-images#lets-first-look-at-one-example-full-image-the-vis-image","position":14},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Let’s first look at one example full image, the VIS image","lvl2":"2. Retrieve a Euclid Q1 MER mosaic image in the VIS bandpass"},"content":"Note that ‘access_estsize’ is in units of kb\n\nfilename = science_images[science_images['energy_bandpassname'] == 'VIS']['access_url'][0]\nfilesize = science_images[science_images['energy_bandpassname'] == 'VIS']['access_estsize'][0] / 1000000\nprint(filename)\n\nprint(f'Please note this image is {filesize} GB. With 230 Mbps internet download speed, it takes about 1 minute to download.')\n\n\n\n\n\nscience_images\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#lets-first-look-at-one-example-full-image-the-vis-image","position":15},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Extract the tileID of this image from the filename","lvl2":"2. Retrieve a Euclid Q1 MER mosaic image in the VIS bandpass"},"type":"lvl3","url":"/euclid-intro-mer-images#extract-the-tileid-of-this-image-from-the-filename","position":16},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Extract the tileID of this image from the filename","lvl2":"2. Retrieve a Euclid Q1 MER mosaic image in the VIS bandpass"},"content":"\n\ntileID = science_images[science_images['energy_bandpassname'] == 'VIS']['obs_id'][0][:9]\n\nprint(f'The MER tile ID for this object is : {tileID}')\n\n\n\nRetrieve the MER image -- note this file is about 1.46 GB\n\nfname = download_file(filename, cache=True)\nhdu_mer_irsa = fits.open(fname)\nprint(hdu_mer_irsa.info())\n\nheader_mer_irsa = hdu_mer_irsa[0].header\n\n\n\n\n\nIf you would like to save the MER mosaic to disk, uncomment the following cell.\nPlease also define a suitable download directory; by default it will be data at the same location as your notebook.\n\n# download_path = 'data'\n# hdu_mer_irsa.writeto(os.path.join(download_path, 'MER_image_VIS.fits'), overwrite=True)\n\n\n\nHave a look at the header information for this image.\n\nheader_mer_irsa\n\n\n\nLets extract just the primary image.\n\nim_mer_irsa = hdu_mer_irsa[0].data\n\nprint(im_mer_irsa.shape)\n\n\n\nDue to the large field of view of the MER mosaic, let’s cut out a smaller section (2\"x2\")of the MER mosaic to inspect the image\n\nplt.imshow(im_mer_irsa[0:1200,0:1200], cmap='gray', origin='lower',\n           norm=ImageNormalize(im_mer_irsa[0:1200,0:1200], interval=PercentileInterval(99.9), stretch=AsinhStretch()))\ncolorbar = plt.colorbar()\n\n\n\nUncomment the code below to plot an image of the entire field of view of the MER mosaic.\n\n# # Full MER mosaic, may take a minute for python to create this image\n# plt.imshow(im_mer_irsa, cmap='gray', origin='lower', norm=ImageNormalize(im_mer_irsa, interval=PercentileInterval(99.9), stretch=AsinhStretch()))\n# colorbar = plt.colorbar()\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#extract-the-tileid-of-this-image-from-the-filename","position":17},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"3. Create multiwavelength Euclid Q1 MER cutouts of a region of interest"},"type":"lvl2","url":"/euclid-intro-mer-images#id-3-create-multiwavelength-euclid-q1-mer-cutouts-of-a-region-of-interest","position":18},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"3. Create multiwavelength Euclid Q1 MER cutouts of a region of interest"},"content":"\n\nNote\n\nWe’d like to take a look at the multiwavelength images of our object, but the full MER mosaics are very large, so we will inspect the multiwavelength cutouts.\n\nurls = science_images['access_url']\n\nurls\n\n\n\nCreate an array with the instrument and filter name so we can add this to the plots.\n\nscience_images['filters'] = science_images['instrument_name'] + \"_\" + science_images['energy_bandpassname']\n\n# VIS_VIS appears in the filters, so update that filter to just say VIS\nscience_images['filters'][science_images['filters']== 'VIS_VIS'] = \"VIS\"\n\nscience_images['filters']\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#id-3-create-multiwavelength-euclid-q1-mer-cutouts-of-a-region-of-interest","position":19},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"The image above is very large, so let’s cut out a smaller image to inspect these data."},"type":"lvl2","url":"/euclid-intro-mer-images#the-image-above-is-very-large-so-lets-cut-out-a-smaller-image-to-inspect-these-data","position":20},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"The image above is very large, so let’s cut out a smaller image to inspect these data."},"content":"\n\n######################## User defined section ############################\n## How large do you want the image cutout to be?\nim_cutout = 1.0 * u.arcmin\n\n## What is the center of the cutout?\n## For now choosing a random location on the image\n## because the star itself is saturated\nra = 273.8667\ndec =  64.525\n\n## Bright star position\n# ra = 273.474451\n# dec = 64.397273\n\ncoords_cutout = SkyCoord(ra, dec, unit='deg', frame='icrs')\n\n##########################################################################\n\n## Iterate through each filter\n\ncutout_list = []\n\nfor url in urls:\n    ## Use fsspec to interact with the fits file without downloading the full file\n    hdu = fits.open(url, use_fsspec=True)\n    print(f\"Opened {url}\")\n\n    ## Store the header\n    header = hdu[0].header\n\n    ## Read in the cutout of the image that you want\n    cutout_data = Cutout2D(hdu[0].section, position=coords_cutout, size=im_cutout, wcs=WCS(hdu[0].header))\n\n    ## Close the file\n    # hdu.close()\n\n    ## Define a new fits file based on this smaller cutout, with accurate WCS based on the cutout size\n    new_hdu = fits.PrimaryHDU(data=cutout_data.data, header=header)\n    new_hdu.header.update(cutout_data.wcs.to_header())\n\n    ## Append the cutout to the list\n    cutout_list.append(new_hdu)\n\n## Combine all cutouts into a single HDUList and display information\nfinal_hdulist = fits.HDUList(cutout_list)\nfinal_hdulist.info()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#the-image-above-is-very-large-so-lets-cut-out-a-smaller-image-to-inspect-these-data","position":21},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"3. Visualize multiwavelength Euclid Q1 MER cutouts"},"type":"lvl2","url":"/euclid-intro-mer-images#id-3-visualize-multiwavelength-euclid-q1-mer-cutouts","position":22},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"3. Visualize multiwavelength Euclid Q1 MER cutouts"},"content":"We need to determine the number of images for the grid layout, and then plot each cutout.\n\nnum_images = len(final_hdulist)\ncolumns = 4\nrows = -(-num_images // columns)\n\nfig, axes = plt.subplots(rows, columns, figsize=(4 * columns, 4 * rows), subplot_kw={'projection': WCS(final_hdulist[0].header)})\naxes = axes.flatten()\n\nfor idx, (ax, filt) in enumerate(zip(axes, science_images['filters'])):\n    image_data = final_hdulist[idx].data\n    norm = ImageNormalize(image_data, interval=PercentileInterval(99.9), stretch=AsinhStretch())\n    ax.imshow(image_data, cmap='gray', origin='lower', norm=norm)\n    ax.set_xlabel('RA')\n    ax.set_ylabel('Dec')\n    ax.text(0.05, 0.05, filt, color='white', fontsize=14, transform=ax.transAxes, va='bottom', ha='left')\n\n## Remove empty subplots if any\nfor ax in axes[num_images:]:\n    fig.delaxes(ax)\n\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#id-3-visualize-multiwavelength-euclid-q1-mer-cutouts","position":23},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"type":"lvl2","url":"/euclid-intro-mer-images#id-4-use-the-python-package-sep-to-identify-and-measure-sources-in-the-euclid-q1-mer-cutouts","position":24},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"content":"First we list all the filters so you can choose which cutout you want to extract sources on. We will choose VIS.\n\nfilt_index = np.where(science_images['filters'] == 'VIS')[0][0]\n\nimg1 = final_hdulist[filt_index].data\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#id-4-use-the-python-package-sep-to-identify-and-measure-sources-in-the-euclid-q1-mer-cutouts","position":25},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Extract some sources from the cutout using sep (python package based on source extractor)","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"type":"lvl3","url":"/euclid-intro-mer-images#extract-some-sources-from-the-cutout-using-sep-python-package-based-on-source-extractor","position":26},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Extract some sources from the cutout using sep (python package based on source extractor)","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"content":"Following the sep tutorial, first create a background for the cutout\n\n\nhttps://​sep​.readthedocs​.io​/en​/stable​/tutorial​.html\n\nNeed to do some initial steps (swap byte order) with the cutout to prevent sep from crashing. Then create a background model with sep.\n\nimg2 = img1.byteswap().view(img1.dtype.newbyteorder())\nc_contiguous_data = np.array(img2, dtype=np.float32)\n\nbkg = sep.Background(c_contiguous_data)\n\nbkg_image = bkg.back()\n\nplt.imshow(bkg_image, interpolation='nearest', cmap='gray', origin='lower')\nplt.colorbar()\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#extract-some-sources-from-the-cutout-using-sep-python-package-based-on-source-extractor","position":27},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Inspect the background rms as well","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"type":"lvl3","url":"/euclid-intro-mer-images#inspect-the-background-rms-as-well","position":28},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Inspect the background rms as well","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"content":"\n\nbkg_rms = bkg.rms()\n\nplt.imshow(bkg_rms, interpolation='nearest', cmap='gray', origin='lower')\nplt.colorbar();\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#inspect-the-background-rms-as-well","position":29},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Subtract the background","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"type":"lvl3","url":"/euclid-intro-mer-images#subtract-the-background","position":30},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Subtract the background","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"content":"\n\ndata_sub = img2 - bkg\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#subtract-the-background","position":31},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Source extraction via sep","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"type":"lvl3","url":"/euclid-intro-mer-images#source-extraction-via-sep","position":32},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl3":"Source extraction via sep","lvl2":"4. Use the Python package sep to identify and measure sources in the Euclid Q1 MER cutouts"},"content":"\n\n######################## User defined section ############################\n\n## Sigma threshold to consider this a detection above the global RMS\nthreshold= 3\n\n## Minimum number of pixels required for an object. Default is 5.\nminarea_0=2\n\n## Minimum contrast ratio used for object deblending. Default is 0.005. To entirely disable deblending, set to 1.0.\ndeblend_cont_0= 0.005\n\nflux_threshold= 0.01\n##########################################################################\n\n\nsources = sep.extract(data_sub, threshold, err=bkg.globalrms, minarea=minarea_0, deblend_cont=deblend_cont_0)\nsources_thr = sources[sources['flux'] > flux_threshold]\nprint(\"Found\", len(sources_thr), \"objects above flux threshold\")\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#source-extraction-via-sep","position":33},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Lets have a look at the objects that were detected with sep in the cutout"},"type":"lvl2","url":"/euclid-intro-mer-images#lets-have-a-look-at-the-objects-that-were-detected-with-sep-in-the-cutout","position":34},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"Lets have a look at the objects that were detected with sep in the cutout"},"content":"We plot the VIS cutout with the sources detected overplotted with a red ellipse\n\nfig, ax = plt.subplots()\nm, s = np.mean(data_sub), np.std(data_sub)\nim = ax.imshow(data_sub, cmap='gray', origin='lower', norm=ImageNormalize(img2, interval=ZScaleInterval(), stretch=SquaredStretch()))\n\n## Plot an ellipse for each object detected with sep\n\nfor i in range(len(sources_thr)):\n    e = Ellipse(xy=(sources_thr['x'][i], sources_thr['y'][i]),\n                width=6*sources_thr['a'][i],\n                height=6*sources_thr['b'][i],\n                angle=sources_thr['theta'][i] * 180. / np.pi)\n    e.set_facecolor('none')\n    e.set_edgecolor('red')\n    ax.add_artist(e)\n\n\n\n","type":"content","url":"/euclid-intro-mer-images#lets-have-a-look-at-the-objects-that-were-detected-with-sep-in-the-cutout","position":35},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-intro-mer-images#about-this-notebook","position":36},{"hierarchy":{"lvl1":"Euclid Q1: MER Mosaics","lvl2":"About this Notebook"},"content":"Author: Tiffany Meshkat, Anahita Alavi, Anastasia Laity, Andreas Faisst, Brigitta Sipőcz, Dan Masters, Harry Teplitz, Jaladh Singhal, Shoubaneh Hemmati, Vandana Desai\n\nUpdated: 2025-03-31\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/euclid-intro-mer-images#about-this-notebook","position":37},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs"},"type":"lvl1","url":"/euclid-intro-mer-catalog","position":0},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs"},"content":"","type":"content","url":"/euclid-intro-mer-catalog","position":1},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-intro-mer-catalog#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"Learning Goals"},"content":"\n\nBy the end of this tutorial, you will:\n\nUnderstand the basic characteristics of Euclid Q1 MER catalogs.\n\nWhat columns are available in the MER catalog.\n\nHow to query with ADQL in the MER catalog.\n\nHow to make a simple color-magnitude diagram with the data.\n\n","type":"content","url":"/euclid-intro-mer-catalog#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"Introduction"},"type":"lvl2","url":"/euclid-intro-mer-catalog#introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"Introduction"},"content":"\n\nEuclid launched in July 2023 as a European Space Agency (ESA) mission with involvement by NASA.\nThe primary science goals of Euclid are to better understand the composition and evolution of the dark Universe.\nThe Euclid mission is providing space-based imaging and spectroscopy as well as supporting ground-based imaging to achieve these primary goals.\nThese data will be archived by multiple global repositories, including IRSA, where they will support transformational work in many areas of astrophysics.\n\nEuclid Quick Release 1 (Q1) consists of consists of ~30 TB of imaging, spectroscopy, and catalogs covering four non-contiguous fields:\nEuclid Deep Field North (22.9 sq deg), Euclid Deep Field Fornax (12.1 sq deg), Euclid Deep Field South (28.1 sq deg), and LDN1641.\n\nAmong the data products included in the Q1 release are the three MER catalogs: the final catalog, the morphology catalog, and the cutouts catalog.\nThis notebook provides an introduction to the MER final catalog.\nEach entry is a single source with associated photometry from the multiwavelength MER Mosaics (VIS, Y, J, H, and any accompanying external ground-based measurements), along with other basic measurements, like size and shape.\nIf you have questions about this notebook, please contact the \n\nIRSA helpdesk.\n\n","type":"content","url":"/euclid-intro-mer-catalog#introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"Imports"},"type":"lvl2","url":"/euclid-intro-mer-catalog#imports","position":6},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"Imports"},"content":"Important\n\nWe rely on astroquery features that have been recently added, so please make sure you have version v0.4.10 or newer installed.\n\n# Uncomment the next line to install dependencies if needed\n# !pip install numpy matplotlib 'astroquery>=0.4.10'\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom astroquery.ipac.irsa import Irsa\n\n\n\n","type":"content","url":"/euclid-intro-mer-catalog#imports","position":7},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"type":"lvl2","url":"/euclid-intro-mer-catalog#id-1-download-mer-catalog-from-irsa-directly-to-this-notebook","position":8},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"content":"\n\nFirst, have a look at what Euclid catalogs are available. With the list_catalogs functionality, we’ll receive a list of the name of the catalogs as well as their brief desciption.\n\nIrsa.list_catalogs(filter='euclid')\n\n\n\n","type":"content","url":"/euclid-intro-mer-catalog#id-1-download-mer-catalog-from-irsa-directly-to-this-notebook","position":9},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl3":"Choose the Euclid MER table","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"type":"lvl3","url":"/euclid-intro-mer-catalog#choose-the-euclid-mer-table","position":10},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl3":"Choose the Euclid MER table","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"content":"\n\ntable_mer = 'euclid_q1_mer_catalogue'\n\n\n\n","type":"content","url":"/euclid-intro-mer-catalog#choose-the-euclid-mer-table","position":11},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl3":"Learn some information about the MER catalog:","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"type":"lvl3","url":"/euclid-intro-mer-catalog#learn-some-information-about-the-mer-catalog","position":12},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl3":"Learn some information about the MER catalog:","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"content":"How many columns are there?\n\nList the column names\n\ncolumns_info = Irsa.list_columns(catalog=table_mer)\nprint(len(columns_info))\n\n\n\n\n\ncolumns_info\n\n\n\nTip\n\nThe MER catalog contains 476 columns, below are a few highlights:\n\nobject_id\n\nflux_vis_psf, mer.flux_y_templfit,flux_j_templfit, mer.flux_h_templfit\n\nfwhm\n\n","type":"content","url":"/euclid-intro-mer-catalog#learn-some-information-about-the-mer-catalog","position":13},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl3":"Define the following ADQL query to find the first 10k stars in the MER catalog","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"type":"lvl3","url":"/euclid-intro-mer-catalog#define-the-following-adql-query-to-find-the-first-10k-stars-in-the-mer-catalog","position":14},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl3":"Define the following ADQL query to find the first 10k stars in the MER catalog","lvl2":"1. Download MER catalog from IRSA directly to this notebook"},"content":"Since we are just using the MER catalog alone, it does not have a column for classification.\nWe can use the point_like_flag = 1 or point_like_prob > 0.99 for stars.\n\nSet all the fluxes to be greater than 0 so the object is detected in all four Euclid MER mosaic images.\n\nadql_stars = (\"SELECT TOP 10000 mer.object_id, mer.ra, mer.dec, mer.flux_vis_psf, mer.fluxerr_vis_psf, \"\n              \"mer.flux_y_templfit,mer.fluxerr_y_templfit, mer.flux_j_templfit, mer.fluxerr_j_templfit, \"\n              \"mer.flux_h_templfit, mer.fluxerr_h_templfit, mer.point_like_prob, mer.extended_prob \"\n              f\"FROM {table_mer} AS mer \"\n              \"WHERE  mer.flux_vis_psf > 0 \"\n              \"AND mer.flux_y_templfit > 0 \"\n              \"AND mer.flux_j_templfit > 0 \"\n              \"AND mer.flux_h_templfit > 0 \"\n              \"AND mer.point_like_flag = 1 \")\n\n\n\nWe can run the query with the TAP service, and then look at some of the results.\n\nresult_stars = Irsa.query_tap(adql_stars).to_table()\nresult_stars[:5]\n\n\n\n","type":"content","url":"/euclid-intro-mer-catalog#define-the-following-adql-query-to-find-the-first-10k-stars-in-the-mer-catalog","position":15},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"2. Make a color-magnitude diagram using the catalogs pulled from IRSA"},"type":"lvl2","url":"/euclid-intro-mer-catalog#id-2-make-a-color-magnitude-diagram-using-the-catalogs-pulled-from-irsa","position":16},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"2. Make a color-magnitude diagram using the catalogs pulled from IRSA"},"content":"Convert from flux in uJy to magnitudes using the zero point correction\n\nConvert the error bars to magnitudes as well\n\nPlot the color-magnitude diagram\n\nmag_y = -2.5 * np.log10(result_stars[\"flux_y_templfit\"]) + 23.9\nmag_h = -2.5 * np.log10(result_stars[\"flux_h_templfit\"]) + 23.9\n\nx = mag_y - mag_h  # Y - H\ny = mag_y\n\nxerr = (2.5 / np.log(10) * np.sqrt((result_stars[\"fluxerr_y_templfit\"] / result_stars[\"flux_y_templfit\"])**2\n                                   + (result_stars[\"fluxerr_h_templfit\"] / result_stars[\"flux_h_templfit\"])**2))\nyerr = (2.5 / np.log(10) * (result_stars[\"fluxerr_y_templfit\"] / result_stars[\"flux_y_templfit\"]))\n\nplt.errorbar(x, y, xerr=xerr, yerr=yerr,\n             fmt='o', markersize=1.5, ecolor='lightgrey', elinewidth=0.5, capsize=2)\n\nplt.xlabel('Y-H')\nplt.ylabel('Y')\n# Note that these limits exclude a handful of points with large error bars.\nplt.xlim(-2, 2)\nplt.ylim(24, 16)\nplt.title('10k Stars in MER catalog -- IRSA')\n\n\n\n","type":"content","url":"/euclid-intro-mer-catalog#id-2-make-a-color-magnitude-diagram-using-the-catalogs-pulled-from-irsa","position":17},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-intro-mer-catalog#about-this-notebook","position":18},{"hierarchy":{"lvl1":"Euclid Q1: MER Catalogs","lvl2":"About this Notebook"},"content":"Author: Tiffany Meshkat, Anahita Alavi, Anastasia Laity, Andreas Faisst, Brigitta Sipőcz, Dan Masters, Harry Teplitz, Jaladh Singhal, Shoubaneh Hemmati, Vandana Desai\n\nUpdated: 2025-04-09\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/euclid-intro-mer-catalog#about-this-notebook","position":19},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra"},"type":"lvl1","url":"/euclid-intro-1d-spectra","position":0},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra"},"content":"","type":"content","url":"/euclid-intro-1d-spectra","position":1},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-intro-1d-spectra#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"Learning Goals"},"content":"\n\nBy the end of this tutorial, you will:\n\nUnderstand the basic characteristics of Euclid Q1 SIR 1D spectra.\n\nExamine the provided boolean masks\n\nMake a simple plot of a Euclid spectrum.\n\n","type":"content","url":"/euclid-intro-1d-spectra#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"Introduction"},"type":"lvl2","url":"/euclid-intro-1d-spectra#introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"Introduction"},"content":"Euclid launched in July 2023 as a European Space Agency (ESA) mission with involvement by NASA.\nThe primary science goals of Euclid are to better understand the composition and evolution of the dark Universe.\nThe Euclid mission is providing space-based imaging and spectroscopy as well as supporting ground-based imaging to achieve these primary goals.\nThese data will be archived by multiple global repositories, including IRSA, where they will support transformational work in many areas of astrophysics.\n\nEuclid Quick Release 1 (Q1) consists of consists of ~30 TB of imaging, spectroscopy, and catalogs covering four non-contiguous fields:\nEuclid Deep Field North (22.9 sq deg), Euclid Deep Field Fornax (12.1 sq deg), Euclid Deep Field South (28.1 sq deg), and LDN1641.\n\nAmong the data products included in the Q1 release are the 1D spectra created by the SIR Processing Function.\nThis notebook provides an introduction to these SIR 1D spectra.\nIf you have questions about it, please contact the \n\nIRSA helpdesk.\n\n","type":"content","url":"/euclid-intro-1d-spectra#introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"Imports"},"type":"lvl2","url":"/euclid-intro-1d-spectra#imports","position":6},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"Imports"},"content":"Important\n\nWe rely on astroquery features that have been recently added, so please make sure you have version v0.4.10 or newer installed.\n\n# Uncomment the next line to install dependencies if needed\n# !pip install matplotlib astropy 'astroquery>=0.4.10'\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom astropy.io import fits\nfrom astropy.table import QTable\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.visualization import quantity_support\n\nfrom astroquery.ipac.irsa import Irsa\n\n#suppress warnings about deprecated units\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"The unit 'Angstrom' has been deprecated\",\n    category=u.UnitsWarning,\n)\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"The unit 'erg' has been deprecated\",\n    category=u.UnitsWarning,\n)\n\n\n\n","type":"content","url":"/euclid-intro-1d-spectra#imports","position":7},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"1. Search for the spectrum of a specific galaxy"},"type":"lvl2","url":"/euclid-intro-1d-spectra#id-1-search-for-the-spectrum-of-a-specific-galaxy","position":8},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"1. Search for the spectrum of a specific galaxy"},"content":"First, explore what Euclid catalogs are available. Note that we need to use the object ID for our targets to be able to download their spectrum.\n\nSearch for spectra collections in IRSA.\n\nIrsa.list_collections(servicetype=\"ssa\")\n\n\n\n\n\neuclid_ssa_collection = 'euclid_DpdSirCombinedSpectra'\n\n\n\n","type":"content","url":"/euclid-intro-1d-spectra#id-1-search-for-the-spectrum-of-a-specific-galaxy","position":9},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"2. Search for the spectrum of a specific galaxy in the 1D spectra table"},"type":"lvl2","url":"/euclid-intro-1d-spectra#id-2-search-for-the-spectrum-of-a-specific-galaxy-in-the-1d-spectra-table","position":10},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"2. Search for the spectrum of a specific galaxy in the 1D spectra table"},"content":"\n\ncoord = SkyCoord(\n    ra=269.7 * u.deg,\n    dec=66.0 * u.deg\n)\n\nsearch_radius = 5.0 * u.arcsec\n\n\n\nQuery the IRSA SSA service for spectra near this position\n\nssa_result = Irsa.query_ssa(\n    pos=coord,\n    radius=search_radius,\n    collection=euclid_ssa_collection,\n)\n\n#check whether any spectra were found\nlen(ssa_result)\n\n\n\nPull out the file name from the result table:\n\n# Extract the single SSA result row\nrow = ssa_result[0]  #in case there is more than one spectra in the search radius\n\n\n# Each SSA row provides a direct URL to the spectrum file\nspectrum_path = row[\"access_url\"]\nspectrum_path\n\n\n\n","type":"content","url":"/euclid-intro-1d-spectra#id-2-search-for-the-spectrum-of-a-specific-galaxy-in-the-1d-spectra-table","position":11},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"3. Read in the spectrum for only our specific object"},"type":"lvl2","url":"/euclid-intro-1d-spectra#id-3-read-in-the-spectrum-for-only-our-specific-object","position":12},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"3. Read in the spectrum for only our specific object"},"content":"spectrum_path is a url that will return a VOTable containing the spectrum of our object.\n\nspectrum = QTable.read(spectrum_path)\n\n\n\n\n\nspectrum\n\n\n\n","type":"content","url":"/euclid-intro-1d-spectra#id-3-read-in-the-spectrum-for-only-our-specific-object","position":13},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"4. Plot the image of the extracted spectrum"},"type":"lvl2","url":"/euclid-intro-1d-spectra#id-4-plot-the-image-of-the-extracted-spectrum","position":14},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"4. Plot the image of the extracted spectrum"},"content":"Tip\n\nAs we use astropy.visualization’s quantity_support, matplotlib automatically picks up the axis units from the quantities we plot.\n\nquantity_support()\n\n\n\nNote\n\nThe 1D combined spectra table contains 6 columns, below are a few highlights:\n\nWAVELENGTH is in Angstroms by default.\n\nSIGNAL is the flux. The values are scaled and the scaling factor is included in the column’s units. This value corresponds to the 'FSCALE' entry in the HDU header of the original FITS file.\n\nMASK values can be used to determine which flux bins to discard. MASK = odd and MASK >=64 means the flux bins not be used.\n\nWe investigate the MASK column to see which flux bins are recommended to keep vs “Do Not Use”\n\nplt.plot(spectrum['WAVELENGTH'].to(u.micron), spectrum['MASK'])\nplt.ylabel('Mask value')\nplt.title('Values of MASK by flux bin')\n\n\n\nWe use the MASK column to create a boolean mask for values to ignore. We use the inverse of this mask to mark the flux bins to use.\n\nbad_mask = (spectrum['MASK'].value % 2 == 1) | (spectrum['MASK'].value >= 64)\n\nplt.plot(spectrum['WAVELENGTH'].to(u.micron), np.ma.masked_where(bad_mask, spectrum['SIGNAL']), color='black', label='Spectrum')\nplt.plot(spectrum['WAVELENGTH'], np.ma.masked_where(~bad_mask, spectrum['SIGNAL']), color='red', label='Do not use')\nplt.plot(spectrum['WAVELENGTH'], np.sqrt(spectrum['VAR']), color='grey', label='Error')\n\nplt.legend(loc='upper right')\nplt.ylim(-0.15, 0.25)\nplt.title(f\"Euclid SIR 1D Spectrum at RA={coord.ra.deg:.4f}°, Dec={coord.dec.deg:.4f}°\")\n\n\n\n","type":"content","url":"/euclid-intro-1d-spectra#id-4-plot-the-image-of-the-extracted-spectrum","position":15},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-intro-1d-spectra#about-this-notebook","position":16},{"hierarchy":{"lvl1":"Euclid Q1: SIR 1D Spectra","lvl2":"About this Notebook"},"content":"Author: Tiffany Meshkat, Anahita Alavi, Anastasia Laity, Andreas Faisst, Brigitta Sipőcz, Dan Masters, Harry Teplitz, Jaladh Singhal, Shoubaneh Hemmati, Vandana Desai, Troy Raen, Jessica Krick\n\nUpdated: 2026-01-13\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/euclid-intro-1d-spectra#about-this-notebook","position":17},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs"},"type":"lvl1","url":"/euclid-intro-phz-catalog","position":0},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs"},"content":"","type":"content","url":"/euclid-intro-phz-catalog","position":1},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-intro-phz-catalog#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"Learning Goals"},"content":"\n\nBy the end of this tutorial, you will:\n\nUnderstand the basic characteristics of Euclid Q1 photo-z catalog and how to match it with MER mosaics.\n\nUnderstand what PHZ catalogs are available and how to view the columns in those catalogs.\n\nHow to query with ADQL in the PHZ catalog to find galaxies between a redshift of 1.4 and 1.6.\n\nPull and plot a spectrum of one of the galaxies in that catalog.\n\nCutout an image of the galaxy to view it close up.\n\nLearn how to upload images and catalogs to Firefly to inspect individual sources in greater detail.\n\n","type":"content","url":"/euclid-intro-phz-catalog#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"Introduction"},"type":"lvl2","url":"/euclid-intro-phz-catalog#introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"Introduction"},"content":"\n\nEuclid launched in July 2023 as a European Space Agency (ESA) mission with involvement by NASA.\nThe primary science goals of Euclid are to better understand the composition and evolution of the dark Universe.\nThe Euclid mission is providing space-based imaging and spectroscopy as well as supporting ground-based imaging to achieve these primary goals.\nThese data will be archived by multiple global repositories, including IRSA, where they will support transformational work in many areas of astrophysics.\n\nEuclid Quick Release 1 (Q1) consists of consists of ~30 TB of imaging, spectroscopy, and catalogs covering four non-contiguous fields:\nEuclid Deep Field North (22.9 sq deg), Euclid Deep Field Fornax (12.1 sq deg), Euclid Deep Field South (28.1 sq deg), and LDN1641.\n\nAmong the data products included in the Q1 release are multiple catalogs created by the PHZ Processing Function.\nThis notebook provides an introduction to the main PHZ catalog, which contains 61 columns describing the photometric redshift probability distribution, fluxes, and classification for each source.\nIf you have questions about this notebook, please contact the \n\nIRSA helpdesk.\n\n","type":"content","url":"/euclid-intro-phz-catalog#introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"Imports"},"type":"lvl2","url":"/euclid-intro-phz-catalog#imports","position":6},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"Imports"},"content":"Important\n\nWe rely on astroquery features that have been recently added, so please make sure you have version v0.4.10 or newer installed.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install matplotlib 'astropy>=5.3' 'astroquery>=0.4.10' fsspec firefly_client\n\n\n\n\n\nimport os\nimport re\nimport urllib\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.nddata import Cutout2D\nfrom astropy.table import QTable\nfrom astropy import units as u\nfrom astropy.utils.data import download_file\nfrom astropy.visualization import ImageNormalize, PercentileInterval, AsinhStretch, LogStretch, quantity_support\nfrom astropy.wcs import WCS\n\nfrom firefly_client import FireflyClient\nfrom astroquery.ipac.irsa import Irsa\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#imports","position":7},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"type":"lvl2","url":"/euclid-intro-phz-catalog#id-1-find-the-mer-tile-id-that-corresponds-to-a-given-ra-and-dec","position":8},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"content":"In this case, choose random coordinates to show a different MER mosaic image. Search a radius around these coordinates.\n\nra = 268\ndec = 66\nsearch_radius= 10 * u.arcsec\n\ncoord = SkyCoord(ra, dec, unit='deg', frame='icrs')\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#id-1-find-the-mer-tile-id-that-corresponds-to-a-given-ra-and-dec","position":9},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Use IRSA to search for all Euclid data on this target","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"type":"lvl3","url":"/euclid-intro-phz-catalog#use-irsa-to-search-for-all-euclid-data-on-this-target","position":10},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Use IRSA to search for all Euclid data on this target","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"content":"This searches specifically in the euclid_DpdMerBksMosaic “collection” which is the MER images and catalogs.\n\nNote\n\nThis table lists all MER mosaic images available in this position. These mosaics include the Euclid VIS, Y, J, H images, as well as ground-based telescopes which have been put on the same pixel scale. For more information, see the \n\nEuclid documentation at IPAC. We use the facility argument below to query for Euclid images only.\n\nimage_table = Irsa.query_sia(pos=(coord, search_radius), collection='euclid_DpdMerBksMosaic', facility='Euclid')\n\n\n\nNote that there are various image types are returned as well, we filter out the science images from these:\n\nscience_images = image_table[image_table['dataproduct_subtype'] == 'science']\nscience_images\n\n\n\nChoose the VIS image and pull the filename and tileID\n\nfilename = science_images[science_images['energy_bandpassname'] == 'VIS']['access_url'][0]\ntileID = science_images[science_images['energy_bandpassname'] == 'VIS']['obs_id'][0][:9]\n\nprint(f'The MER tile ID for this object is : {tileID}')\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#use-irsa-to-search-for-all-euclid-data-on-this-target","position":11},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"2. Download PHZ catalog from IRSA"},"type":"lvl2","url":"/euclid-intro-phz-catalog#id-2-download-phz-catalog-from-irsa","position":12},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"2. Download PHZ catalog from IRSA"},"content":"Use IRSA’s TAP to search catalogs\n\nIrsa.list_catalogs(filter='euclid')\n\n\n\n\n\ntable_mer = 'euclid_q1_mer_catalogue'\ntable_phz = 'euclid_q1_phz_photo_z'\ntable_1dspectra = 'euclid.objectid_spectrafile_association_q1'\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#id-2-download-phz-catalog-from-irsa","position":13},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Learn some information about the photo-z catalog:","lvl2":"2. Download PHZ catalog from IRSA"},"type":"lvl3","url":"/euclid-intro-phz-catalog#learn-some-information-about-the-photo-z-catalog","position":14},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Learn some information about the photo-z catalog:","lvl2":"2. Download PHZ catalog from IRSA"},"content":"How many columns are there?\n\nList the column names\n\ncolumns_info = Irsa.list_columns(catalog=table_phz)\nprint(len(columns_info))\n\n\n\nTip\n\nThe PHZ catalog contains 67 columns, below are a few highlights:\n\nobject_id\n\nflux_vis_unif, flux_y_unif, flux_j_unif, flux_h_unif\n\nmedian redshift (phz_median)\n\nphz_classification\n\nphz_90_int1,  phz_90_int2 (The phz PDF interval containing 90% of the probability, upper and lower values)\n\n# Full list of columns and their description\ncolumns_info\n\n\n\nNote\n\nThe phz_catalog on IRSA has more columns than it does on the ESA archive.\nThis is because the ESA catalog stores some information in one column (for example, phz_90_int is stored as [lower, upper], rather than in two separate columns).\n\nThe fluxes are different from the fluxes derived in the MER catalog.\nThe _unif fluxes are: “Unified flux recomputed after correction from galactic extinction and filter shifts”.\n\n","type":"content","url":"/euclid-intro-phz-catalog#learn-some-information-about-the-photo-z-catalog","position":15},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Find some galaxies between 1.4 and 1.6 at a selected RA and Dec","lvl2":"2. Download PHZ catalog from IRSA"},"type":"lvl3","url":"/euclid-intro-phz-catalog#find-some-galaxies-between-1-4-and-1-6-at-a-selected-ra-and-dec","position":16},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Find some galaxies between 1.4 and 1.6 at a selected RA and Dec","lvl2":"2. Download PHZ catalog from IRSA"},"content":"We specify the following conditions on our search:\n\nWe select just the galaxies where the flux is greater than zero, to ensure the appear in all four of the Euclid MER images.\n\nSelect only objects in a circle (search radius selected below) around our selected RA and Dec\n\nphz_classification = 2 means we select only galaxies\n\nUsing the phz_90_int1 and phz_90_int2, we select just the galaxies where the error on the photometric redshift is less than 20%\n\nSelect just the galaxies between a median redshift of 1.4 and 1.6\n\nWe search just a 5 arcminute box around an RA and Dec\n\nSearch based on tileID:\n\n######################## User defined section ############################\n## How large do you want the image cutout to be?\nim_cutout= 5 * u.arcmin\n\n## What is the center of the cutout?\nra_cutout = 267.8\ndec_cutout =  66\n\ncoords_cutout = SkyCoord(ra_cutout, dec_cutout, unit='deg', frame='icrs')\nsize_cutout = im_cutout.to(u.deg).value\n\n\n\n\n\nadql = (\"SELECT DISTINCT mer.object_id, mer.ra, mer.dec, \"\n        \"phz.flux_vis_unif, phz.flux_y_unif, phz.flux_j_unif, phz.flux_h_unif, \"\n        \"phz.phz_classification, phz.phz_median, phz.phz_90_int1, phz.phz_90_int2 \"\n        f\"FROM {table_mer} AS mer \"\n        f\"JOIN {table_phz} as phz \"\n        \"ON mer.object_id = phz.object_id \"\n        \"WHERE 1 = CONTAINS(POINT('ICRS', mer.ra, mer.dec), \"\n                            f\"BOX('ICRS', {ra_cutout}, {dec_cutout}, {size_cutout/np.cos(coords_cutout.dec)}, {size_cutout})) \"\n        \"AND phz.flux_vis_unif> 0 \"\n        \"AND  phz.flux_y_unif > 0 \"\n        \"AND phz.flux_j_unif > 0 \"\n        \"AND phz.flux_h_unif > 0 \"\n        \"AND phz.phz_classification = 2 \"\n        \"AND ((phz.phz_90_int2 - phz.phz_90_int1) / (1 + phz.phz_median)) < 0.20 \"\n        \"AND phz.phz_median BETWEEN 1.4 AND 1.6\")\n\n\n## Use TAP with this ADQL string\nresult_galaxies = Irsa.query_tap(adql).to_table()\nresult_galaxies[:5]\n\n\n\nWarning\n\nNote that we use to_table above rather than to_qtable. While astropy’s QTable is more powerful than its Table, as it e.g. handles the column units properly, we cannot use it here due to a known bug; it mishandles the large integer numbers in the object_id column and recast them as float during which process some precision is being lost.\n\nOnce the bug is fixed, we plan to update the code in this notebook and simplify some of the approaches below.\n\n","type":"content","url":"/euclid-intro-phz-catalog#find-some-galaxies-between-1-4-and-1-6-at-a-selected-ra-and-dec","position":17},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"3. Read in a cutout of the MER image from IRSA directly"},"type":"lvl2","url":"/euclid-intro-phz-catalog#id-3-read-in-a-cutout-of-the-mer-image-from-irsa-directly","position":18},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"3. Read in a cutout of the MER image from IRSA directly"},"content":"\n\nDue to the large field of view of the MER mosaic, let’s cut out a smaller section (5’x5’) of the MER mosaic to inspect the image.\n\n## Use fsspec to interact with the fits file without downloading the full file\nhdu = fits.open(filename, use_fsspec=True)\n\n## Store the header\nheader = hdu[0].header\n\n## Read in the cutout of the image that you want\ncutout_image = Cutout2D(hdu[0].section, position=coords_cutout, size=im_cutout, wcs=WCS(header))\n\n\n\n\n\ncutout_image.data.shape\n\n\n\n\n\nnorm = ImageNormalize(cutout_image.data, interval=PercentileInterval(99.9), stretch=AsinhStretch())\n_ = plt.imshow(cutout_image.data, cmap='gray', origin='lower', norm=norm)\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#id-3-read-in-a-cutout-of-the-mer-image-from-irsa-directly","position":19},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"4. Overplot the catalog on the MER mosaic image"},"type":"lvl2","url":"/euclid-intro-phz-catalog#id-4-overplot-the-catalog-on-the-mer-mosaic-image","position":20},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"4. Overplot the catalog on the MER mosaic image"},"content":"\n\nTip\n\nWe can rely on astropy’s WCSAxes framework for making plots of Astronomical data in Matplotlib. Please note the usage of projection and transform arguments in the code example below.\n\nFor more info, please visit the \n\nWCSAxes documentation.\n\nax = plt.subplot(projection=cutout_image.wcs)\n\nax.imshow(cutout_image.data, cmap='gray', origin='lower',\n          norm=ImageNormalize(cutout_image.data, interval=PercentileInterval(99.9), stretch=LogStretch()))\nplt.scatter(result_galaxies['ra'], result_galaxies['dec'], s=36, facecolors='none', edgecolors='red',\n            transform=ax.get_transform('world'))\n\n_ = plt.title('Galaxies between z = 1.4 and 1.6')\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#id-4-overplot-the-catalog-on-the-mer-mosaic-image","position":21},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"5. Pull the spectra on the top brightest source based on object ID"},"type":"lvl2","url":"/euclid-intro-phz-catalog#id-5-pull-the-spectra-on-the-top-brightest-source-based-on-object-id","position":22},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"5. Pull the spectra on the top brightest source based on object ID"},"content":"\n\nresult_galaxies.sort(keys='flux_h_unif', reverse=True)\n\n\n\n\n\nresult_galaxies[:3]\n\n\n\nLet’s pick one of these galaxies. Note that the table has been sorted above, we can use the same index here and below to access the data for this particular galaxy.\n\nindex = 2\n\nobj_id = result_galaxies['object_id'][index]\nredshift = result_galaxies['phz_median'][index]\n\n\n\nWe will use TAP and an ASQL query to find the spectral data for this particular galaxy.\n\nadql_object = f\"SELECT * FROM {table_1dspectra} WHERE objectid = {obj_id}\"\n\n## Pull the data on this particular galaxy\nresult_spectra = Irsa.query_tap(adql_object).to_table()\nresult_spectra\n\n\n\nPull out the file name from the result_spectra table:\n\nspectrum_path = f\"https://irsa.ipac.caltech.edu/{result_spectra['path'][0]}\"\nspectrum_path\n\n\n\n\n\nspectrum = QTable.read(spectrum_path)\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#id-5-pull-the-spectra-on-the-top-brightest-source-based-on-object-id","position":23},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Now the data are read in, plot the spectrum","lvl2":"5. Pull the spectra on the top brightest source based on object ID"},"type":"lvl3","url":"/euclid-intro-phz-catalog#now-the-data-are-read-in-plot-the-spectrum","position":24},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Now the data are read in, plot the spectrum","lvl2":"5. Pull the spectra on the top brightest source based on object ID"},"content":"Tip\n\nAs we use astropy.visualization’s quantity_support, matplotlib automatically picks up the axis units from the quantities we plot.\n\nquantity_support()\n\n\n\n\n\nplt.plot(spectrum['WAVELENGTH'].to(u.micron), spectrum['SIGNAL'])\n\nplt.xlim(1.25, 1.85)\nplt.ylim(-0.5, 0.5)\n_ = plt.title(f\"Object {obj_id} with phz_median={redshift}\")\n\n\n\nLet’s cut out a very small patch of the MER image to see what this galaxy looks like. Remember that we sorted the table above, so can reuse the same index to pick up the coordinates for the galaxy. Otherwise we could filter on the object ID.\n\nresult_galaxies[index]\n\n\n\n\n\n## How large do you want the image cutout to be?\nsize_galaxy_cutout = 2.0 * u.arcsec\n\n\n\nUse the ra and dec columns for the galaxy to create a SkyCoord.\n\ncoords_galaxy = SkyCoord(result_galaxies['ra'][index], result_galaxies['dec'][index], unit='deg')\n\n\n\n\n\ncoords_galaxy\n\n\n\nWe haven’t closed the image file above, so use Cutout2D again to cut out a section around the galaxy.\n\ncutout_galaxy = Cutout2D(hdu[0].section, position=coords_galaxy, size=size_galaxy_cutout, wcs=WCS(header))\n\n\n\nPlot to show the cutout on the galaxy\n\nax = plt.subplot(projection=cutout_galaxy.wcs)\n\nax.imshow(cutout_galaxy.data, cmap='gray', origin='lower',\n          norm=ImageNormalize(cutout_galaxy.data, interval=PercentileInterval(99.9), stretch=AsinhStretch()))\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#now-the-data-are-read-in-plot-the-spectrum","position":25},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"type":"lvl2","url":"/euclid-intro-phz-catalog#id-6-load-the-image-on-firefly-to-be-able-to-interact-with-the-data-directly","position":26},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"content":"\n\nSave the data locally if you have not already done so, in order to upload to IRSA viewer.\n\ndownload_path = \"data\"\nif os.path.exists(download_path):\n    print(\"Output directory already created.\")\nelse:\n    print(\"Creating data directory.\")\n    os.mkdir(download_path)\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#id-6-load-the-image-on-firefly-to-be-able-to-interact-with-the-data-directly","position":27},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Vizualize the image with Firefly","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"type":"lvl3","url":"/euclid-intro-phz-catalog#vizualize-the-image-with-firefly","position":28},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Vizualize the image with Firefly","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"content":"First initialize the client, then set the path to the image, upload it to firefly, load it and align with WCS.\n\nNote this can take a while to upload the full MER image.\n\nfc = FireflyClient.make_client('https://irsa.ipac.caltech.edu/irsaviewer')\n\nfc.show_fits(url=filename)\n\nfc.align_images(lock_match=True)\n\n\n\n\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#vizualize-the-image-with-firefly","position":29},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Save the table as a CSV for Firefly upload","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"type":"lvl3","url":"/euclid-intro-phz-catalog#save-the-table-as-a-csv-for-firefly-upload","position":30},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Save the table as a CSV for Firefly upload","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"content":"\n\ncsv_path = os.path.join(download_path, \"mer_df.csv\")\nresult_galaxies.write(csv_path, format=\"csv\")\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#save-the-table-as-a-csv-for-firefly-upload","position":31},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Upload the CSV table to Firefly and display as an overlay on the FITS image","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"type":"lvl3","url":"/euclid-intro-phz-catalog#upload-the-csv-table-to-firefly-and-display-as-an-overlay-on-the-fits-image","position":32},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl3":"Upload the CSV table to Firefly and display as an overlay on the FITS image","lvl2":"6. Load the image on Firefly to be able to interact with the data directly"},"content":"\n\nuploaded_table = fc.upload_file(csv_path)\nprint(f\"Uploaded Table URL: {uploaded_table}\")\n\nfc.show_table(uploaded_table)\n\n\n\n\n\n","type":"content","url":"/euclid-intro-phz-catalog#upload-the-csv-table-to-firefly-and-display-as-an-overlay-on-the-fits-image","position":33},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-intro-phz-catalog#about-this-notebook","position":34},{"hierarchy":{"lvl1":"Euclid Q1: PHZ Catalogs","lvl2":"About this Notebook"},"content":"Author: Tiffany Meshkat, Anahita Alavi, Anastasia Laity, Andreas Faisst, Brigitta Sipőcz, Dan Masters, Harry Teplitz, Jaladh Singhal, Shoubaneh Hemmati, Vandana Desai, Troy Raen\n\nUpdated: 2025-09-24\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/euclid-intro-phz-catalog#about-this-notebook","position":35},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs"},"type":"lvl1","url":"/euclid-intro-spe-catalog","position":0},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs"},"content":"","type":"content","url":"/euclid-intro-spe-catalog","position":1},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-intro-spe-catalog#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Learning Goals"},"content":"\n\nBy the end of this tutorial, you will:\n\nUnderstand the basic characteristics of Euclid Q1 SPE catalogs.\n\nUnderstand what SPE catalogs are available and how to view the columns in those catalogs.\n\nHow to query with ADQL in the SPE lines catalog to find strong H-alpha detections.\n\nHow to make a plot the detected line features over the 1D spectra.\n\n","type":"content","url":"/euclid-intro-spe-catalog#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Introduction"},"type":"lvl2","url":"/euclid-intro-spe-catalog#introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Introduction"},"content":"\n\nEuclid launched in July 2023 as a European Space Agency (ESA) mission with involvement by NASA.\nThe primary science goals of Euclid are to better understand the composition and evolution of the dark Universe.\nThe Euclid mission is providing space-based imaging and spectroscopy as well as supporting ground-based imaging to achieve these primary goals.\nThese data will be archived by multiple global repositories, including IRSA, where they will support transformational work in many areas of astrophysics.\n\nEuclid Quick Release 1 (Q1) consists of consists of ~30 TB of imaging, spectroscopy, and catalogs covering four non-contiguous fields:\nEuclid Deep Field North (22.9 sq deg), Euclid Deep Field Fornax (12.1 sq deg), Euclid Deep Field South (28.1 sq deg), and LDN1641.\n\nAmong the data products included in the Q1 release are multiple catalogs created by the SPE Processing Function.\nThis notebook provides an introduction to these SPE catalogs.\nIf you have questions about this notebook, please contact the \n\nIRSA helpdesk.\n\n","type":"content","url":"/euclid-intro-spe-catalog#introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Imports"},"type":"lvl2","url":"/euclid-intro-spe-catalog#imports","position":6},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Imports"},"content":"Important\n\nWe rely on astroquery features that have been recently added, so please make sure you have version v0.4.10 or newer installed.\n\n# Uncomment the next line to install dependencies if needed\n# %pip install matplotlib astropy 'astroquery>=0.4.10'\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.table import QTable\nfrom astropy import units as u\nfrom astropy.utils.data import download_file\nfrom astropy.utils import data\nfrom astropy.visualization import ImageNormalize, PercentileInterval, AsinhStretch, quantity_support\n\nfrom astroquery.ipac.irsa import Irsa\n\n# Increase Astropy’s default network timeout (in seconds) for remote name resolution and data access\ndata.conf.remote_timeout = 60\n\n#suppress warnings about deprecated units and cache\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"The unit 'Angstrom' has been deprecated\",\n    category=u.UnitsWarning,\n)\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"The unit 'erg' has been deprecated\",\n    category=u.UnitsWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"XDG_CACHE_HOME is set\",\n)\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#imports","position":7},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"type":"lvl2","url":"/euclid-intro-spe-catalog#id-1-find-the-mer-tile-id-that-corresponds-to-a-given-ra-and-dec","position":8},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"content":"In this case, choose the coordinates from the first notebook to save time downloading the MER mosaic. Search a radius of 1.5 arcminutes around these coordinates.\n\nsearch_radius = 10 * u.arcsec\ncoord = SkyCoord.from_name('HD 168151')\n\n\n\nTip\n\nThe IRSA SIA collections can be listed using using the list_collections method, we can filter on the ones containing “euclid” in the collection name:Irsa.list_collections(filter='euclid')\n\n","type":"content","url":"/euclid-intro-spe-catalog#id-1-find-the-mer-tile-id-that-corresponds-to-a-given-ra-and-dec","position":9},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Use IRSA to search for all Euclid data on this target","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"type":"lvl3","url":"/euclid-intro-spe-catalog#use-irsa-to-search-for-all-euclid-data-on-this-target","position":10},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Use IRSA to search for all Euclid data on this target","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"content":"This searches specifically in the euclid_DpdMerBksMosaic collection which is the MER images and catalogs.\n\nimage_table = Irsa.query_sia(pos=(coord, search_radius), collection='euclid_DpdMerBksMosaic')\n\n\n\nThis table lists all MER mosaic images available in this search position. These mosaics include the Euclid VIS, Y, J, H images, as well as ground-based telescopes which have been put on the same pixel scale. For more information, see the \n\nEuclid documentation at IPAC.\n\nNote that there are various image types are returned as well, we filter out the science images from these:\n\nscience_images = image_table[image_table['dataproduct_subtype'] == 'science']\nscience_images\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#use-irsa-to-search-for-all-euclid-data-on-this-target","position":11},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Choose the VIS image and pull the Tile ID","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"type":"lvl3","url":"/euclid-intro-spe-catalog#choose-the-vis-image-and-pull-the-tile-id","position":12},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Choose the VIS image and pull the Tile ID","lvl2":"1. Find the MER Tile ID that corresponds to a given RA and Dec"},"content":"\n\nExtract the tile ID from the obs_id column. The values in this column are made a combination of the 9 digit tile ID and the abbreviation of the instrument.\n\ntileID = science_images[science_images['energy_bandpassname'] == 'VIS']['obs_id'][0][:9]\n\nprint(f'The MER tile ID for this object is : {tileID}')\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#choose-the-vis-image-and-pull-the-tile-id","position":13},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"2. Download SPE catalog from IRSA directly to this notebook"},"type":"lvl2","url":"/euclid-intro-spe-catalog#id-2-download-spe-catalog-from-irsa-directly-to-this-notebook","position":14},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"2. Download SPE catalog from IRSA directly to this notebook"},"content":"Search for all tables in IRSA labeled as euclid\n\nIrsa.list_catalogs(filter='euclid')\n\n\n\n\n\ntable_mer = 'euclid_q1_mer_catalogue'\ntable_galaxy_candidates = 'euclid_q1_spectro_zcatalog_spe_galaxy_candidates'\ntable_1dspectra = 'euclid.objectid_spectrafile_association_q1'\ntable_lines = 'euclid_q1_spe_lines_line_features'\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#id-2-download-spe-catalog-from-irsa-directly-to-this-notebook","position":15},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Learn some information about the table:","lvl2":"2. Download SPE catalog from IRSA directly to this notebook"},"type":"lvl3","url":"/euclid-intro-spe-catalog#learn-some-information-about-the-table","position":16},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Learn some information about the table:","lvl2":"2. Download SPE catalog from IRSA directly to this notebook"},"content":"How many columns are there?\n\nList the column names\n\ncolumns_info = Irsa.list_columns(catalog=table_mer)\nprint(len(columns_info))\n\n\n\n\n\nIrsa.list_columns(catalog=table_1dspectra, full=True)\n\n\n\n\n\n# Full list of columns and their description\ncolumns_info\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#learn-some-information-about-the-table","position":17},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Find some objects with spectra in our tileID"},"type":"lvl2","url":"/euclid-intro-spe-catalog#find-some-objects-with-spectra-in-our-tileid","position":18},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"Find some objects with spectra in our tileID"},"content":"We specify the following conditions on our search:\n\nSignal to noise ratio column (_gf = gaussian fit) should be greater than 5\n\nWe want to detect H-alpha.\n\nWe choose in which tileID to search, usign the tileID from the first notebook.\n\nChoose spectroscopic redshift (spe_z) between 1.4 and 1.6 and spe_z_prob greater than 0.999\n\nH-alpha line flux should be more than 2x10^16 erg s^-1 cm^-2\n\nJoin the lines and galaxy candidates tables on object_id and spe_rank\n\nFinally we sort the data by descending spe_line_snr_gf to have the largest SNR H-alpha lines detected at the top.\n\nadql_query = (\"SELECT DISTINCT mer.object_id,mer.ra, mer.dec, mer.tileid, mer.flux_y_templfit, \"\n    \"lines.spe_line_snr_gf,lines.spe_line_snr_di, lines.spe_line_name, lines.spe_line_central_wl_gf, \"\n    \"lines.spe_line_ew_gf, galaxy.spe_z_err, galaxy.spe_z,galaxy.spe_z_prob, \"\n    \"lines.spe_line_flux_gf, lines.spe_line_flux_err_gf \"\n    f\"FROM {table_mer} AS mer \"\n    f\"JOIN {table_lines} AS lines \"\n    \"ON mer.object_id = lines.object_id \"\n    f\"JOIN {table_galaxy_candidates} AS galaxy \"\n    \"ON lines.object_id = galaxy.object_id AND lines.spe_rank = galaxy.spe_rank \"\n    \"WHERE lines.spe_line_snr_gf >5 \"\n    \"AND lines.spe_line_name = 'Halpha' \"\n    f\"AND mer.tileid = {tileID} \"\n    \"AND galaxy.spe_z_prob > 0.99 \"\n    \"AND galaxy.spe_z BETWEEN 1.4 AND 1.6 \"\n    \"AND lines.spe_line_flux_gf > 2E-16 \"\n    \"ORDER BY lines.spe_line_snr_gf DESC \")\n\n# Use TAP with this ADQL string\nresult_table = Irsa.query_tap(adql_query).to_table()\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#find-some-objects-with-spectra-in-our-tileid","position":19},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Choose an object of interest, lets look at an object with a strong Halpha line detected with high SNR.","lvl2":"Find some objects with spectra in our tileID"},"type":"lvl3","url":"/euclid-intro-spe-catalog#choose-an-object-of-interest-lets-look-at-an-object-with-a-strong-halpha-line-detected-with-high-snr","position":20},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Choose an object of interest, lets look at an object with a strong Halpha line detected with high SNR.","lvl2":"Find some objects with spectra in our tileID"},"content":"\n\nobj_id = 2737659721646729968\n\nobj_row = result_table[(result_table['object_id'] == obj_id)]\n\nobj_row\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#choose-an-object-of-interest-lets-look-at-an-object-with-a-strong-halpha-line-detected-with-high-snr","position":21},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Pull the spectrum of this object","lvl2":"Find some objects with spectra in our tileID"},"type":"lvl3","url":"/euclid-intro-spe-catalog#pull-the-spectrum-of-this-object","position":22},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Pull the spectrum of this object","lvl2":"Find some objects with spectra in our tileID"},"content":"\n\n# Query SSA for the 1D spectrum near this object's sky position\neuclid_ssa_collection = \"euclid_DpdSirCombinedSpectra\"\n\n# Use the object's MER coordinates from obj_row\ncoord_obj = SkyCoord(obj_row[\"ra\"][0], obj_row[\"dec\"][0], unit=u.deg)\n\n#complete the query\nssa_result = Irsa.query_ssa(\n    pos=coord_obj,\n    radius=2.0 * u.arcsec,\n    collection=euclid_ssa_collection,\n)\n\nssa_result\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#pull-the-spectrum-of-this-object","position":23},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"The following steps to read in the spectrum follows the 3_Euclid_intro_1D_spectra notebook.","lvl2":"Find some objects with spectra in our tileID"},"type":"lvl3","url":"/euclid-intro-spe-catalog#the-following-steps-to-read-in-the-spectrum-follows-the-3-euclid-intro-1d-spectra-notebook","position":24},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"The following steps to read in the spectrum follows the 3_Euclid_intro_1D_spectra notebook.","lvl2":"Find some objects with spectra in our tileID"},"content":"\n\n# Read in the spectrum for this object from the SSA access URL\nspectrum_path = ssa_result[\"access_url\"][0]\nspectrum_path\n\n\n\n\n\nspectrum = QTable.read(spectrum_path)\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#the-following-steps-to-read-in-the-spectrum-follows-the-3-euclid-intro-1d-spectra-notebook","position":25},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Now the data are read in, plot the spectrum with the H-alpha line labeled","lvl2":"Find some objects with spectra in our tileID"},"type":"lvl3","url":"/euclid-intro-spe-catalog#now-the-data-are-read-in-plot-the-spectrum-with-the-h-alpha-line-labeled","position":26},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl3":"Now the data are read in, plot the spectrum with the H-alpha line labeled","lvl2":"Find some objects with spectra in our tileID"},"content":"Tip\n\nAs we use astropy.visualization’s quantity_support, matplotlib automatically picks up the axis units from the quantities we plot.\n\nquantity_support()\n\n\n\n\n\n# Note that the units are missing from the lines table, we manually add Angstrom\nline_wavelengths = obj_row['spe_line_central_wl_gf'] * u.angstrom\nline_names = obj_row['spe_line_name']\nsnr_gf = obj_row['spe_line_snr_gf']\n\nplt.plot(spectrum['WAVELENGTH'].to(u.micron), spectrum['SIGNAL'])\n\nfor wl, name, snr in zip(np.atleast_1d(line_wavelengths), np.atleast_1d(line_names), np.atleast_1d(snr_gf)):\n    plt.axvline(wl, color='b', linestyle='--', alpha=0.3)\n    plt.text(wl, .2, name+' SNR='+str(round(snr)), rotation=90, ha='center', va='bottom', fontsize=10)\n\nplt.title(f'Object ID {obj_id}')\n\n\n\n","type":"content","url":"/euclid-intro-spe-catalog#now-the-data-are-read-in-plot-the-spectrum-with-the-h-alpha-line-labeled","position":27},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-intro-spe-catalog#about-this-notebook","position":28},{"hierarchy":{"lvl1":"Euclid Q1: SPE Catalogs","lvl2":"About this Notebook"},"content":"Author: Tiffany Meshkat, Anahita Alavi, Anastasia Laity, Andreas Faisst, Brigitta Sipőcz, Dan Masters, Harry Teplitz, Jaladh Singhal, Shoubaneh Hemmati, Vandana Desai, Troy Raen, Jessica Krick\n\nUpdated: 2026-01-13\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.\n\nRuntime: As of the date above, this notebook takes about 90 seconds to run to completion on a machine with 8GB RAM and 4 CPU.","type":"content","url":"/euclid-intro-spe-catalog#about-this-notebook","position":29},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data"},"type":"lvl1","url":"/euclid-ero","position":0},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data"},"content":"","type":"content","url":"/euclid-ero","position":1},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-ero#learning-goals","position":2},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\n• Access the Euclid ERO images using astroquery\n\n• Create cutouts on the large Euclid ERO images directly\n\n• Extract sources on the Euclid image and run photometry tools\n\n• Compare photometry to the Gaia catalog\n\n• Visualize the Euclid ERO image and the Gaia catalog in Firefly","type":"content","url":"/euclid-ero#learning-goals","position":3},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Introduction"},"type":"lvl2","url":"/euclid-ero#introduction","position":4},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Introduction"},"content":"Euclid is a European Space Agency (ESA) space mission with NASA participation, to study the geometry and nature of the dark Universe. As part of its first observations, Euclid publicly released so-called Early Release Observations (ERO) targeting some press-worthy targets on the sky such as star clusters or local galaxies.\n\nIn this notebook, we will focus on the ERO data of NGC 6397, a globular cluster 78,000 light years away. (Note that there is another globular cluster, NGC 6254 that can also be used for this analysis - in fact the user can choose which one to use) The goal of this analysis is to extract the Euclid photometry of the stars belonging to the cluster and compare them to the photometry of Gaia. Due to the different pixel scales of the visible (VIS, 0.1^{\\prime\\prime}) and near-IR (NISP, 0.3^{\\prime\\prime}) wavelengths, we will first detect and extract the stars in the VIS filter and use their position to subsequently extract the photometry in the NISP (Y, J, H) filters (a method also referred to as forced photometry).\n\nOne challenge with Euclid data is their size due to the large sky coverage and small pixel size.\nThis notebook demonstrates how to download only a cutout of the large Euclid ERO observation image (namely focussing only on the position of the globular cluster. Furthermore, this notebook demonstrates how to extract sources on a large astronomical image and how to measure their photometry across different pixel scales using a very simple implementation of the forced photometry method with positional priors.\nFinally, we also demonstrate how to visualize the Euclid images and catalog in Firefly, an open-source web-based UI library for astronomical data archive access and visualization developed at Caltech (\n\nhttps://​github​.com​/Caltech​-IPAC​/firefly). Firefly is a convenient tool to visualize images similar to DS9 on your local computer, but it can run on a cloud-based science platform.\n\nThis notebook is written to be used in Fornax which is a cloud based computing platform using AWS. The advantage of this is that the user does not need to download actuall data, hence the analysis of large datasets is not limited by local computing resources. It also allows to access data across all archives fast and easy.","type":"content","url":"/euclid-ero#introduction","position":5},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Data Volume"},"type":"lvl2","url":"/euclid-ero#data-volume","position":6},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Data Volume"},"content":"The total data volume required for running this notebook is less than 20 MB.\n\n","type":"content","url":"/euclid-ero#data-volume","position":7},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Imports"},"type":"lvl2","url":"/euclid-ero#imports","position":8},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Imports"},"content":"Important\n\nWe rely on astroquery, firefly_client, photutils, and sep features that have been recently added, so please make sure you have the respective versions v0.4.10, v3.2, v2.0, and v1.4 or newer installed.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install tqdm numpy matplotlib astropy 'photutils>=2.0' 'astroquery>=0.4.10' 'sep>=1.4' 'firefly_client>=3.2'\n\n\n\nFirst, we import all necessary packages.\n\n# General imports\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\n# Astroquery\nfrom astroquery.ipac.irsa import Irsa\nfrom astroquery.gaia import Gaia\n\n# Astropy\nfrom astropy.coordinates import SkyCoord\nfrom astropy import units as u\nfrom astropy.io import fits\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nfrom astropy.table import Table\nfrom astropy.stats import sigma_clipped_stats\n\n# Photometry tools\nimport sep\nfrom photutils.detection import DAOStarFinder\nfrom photutils.psf import PSFPhotometry, IterativePSFPhotometry, CircularGaussianSigmaPRF, make_psf_model_image\nfrom photutils.background import LocalBackground, MMMBackground\n\n# Firefly\nfrom firefly_client import FireflyClient\n\n# Matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n\n\n\n\nNext, we define some parameters for Matplotlib plotting.\n\n## Plotting stuff\nmpl.rcParams['font.size'] = 14\nmpl.rcParams['axes.labelpad'] = 7\nmpl.rcParams['xtick.major.pad'] = 7\nmpl.rcParams['ytick.major.pad'] = 7\nmpl.rcParams['xtick.minor.visible'] = True\nmpl.rcParams['ytick.minor.visible'] = True\nmpl.rcParams['xtick.minor.top'] = True\nmpl.rcParams['xtick.minor.bottom'] = True\nmpl.rcParams['ytick.minor.left'] = True\nmpl.rcParams['ytick.minor.right'] = True\nmpl.rcParams['xtick.major.size'] = 5\nmpl.rcParams['ytick.major.size'] = 5\nmpl.rcParams['xtick.minor.size'] = 3\nmpl.rcParams['ytick.minor.size'] = 3\nmpl.rcParams['xtick.direction'] = 'in'\nmpl.rcParams['ytick.direction'] = 'in'\n#mpl.rc('text', usetex=True)\nmpl.rc('font', family='serif')\nmpl.rcParams['xtick.top'] = True\nmpl.rcParams['ytick.right'] = True\nmpl.rcParams['hatch.linewidth'] = 1\n\ndef_cols = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\n\n\n","type":"content","url":"/euclid-ero#imports","position":9},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Setting up the Environment"},"type":"lvl2","url":"/euclid-ero#setting-up-the-environment","position":10},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Setting up the Environment"},"content":"Next, we set up the environment. This includes\n\nsetting up an output data directory (will be created if it does not exist)\n\ndefine the search radius around the target of interest to pull data from the Gaia catalog\n\ndefine the cutout size that will be used to download a certain part of the Euclid ERO images\n\nFinally, we also define the target of interest here. We can choose between two globular clusters, NGC 6254 and NGC6397, both covered by the Euclid ERO data.\n\nNote that astropy units can be attached to the search_radius and cutout_size.\n\n# create output directory\nif os.path.exists(\"./data/\"):\n    print(\"Output directory already created.\")\nelse:\n    print(\"Creating data directory.\")\n    os.mkdir(\"./data/\")\n\nsearch_radius = 1.5 * u.arcmin # search radius\ncutout_size = 1.5 * u.arcmin # cutout size\n\ncoord = SkyCoord.from_name('NGC 6397')\n\n\n\n","type":"content","url":"/euclid-ero#setting-up-the-environment","position":11},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Search Euclid ERO Images"},"type":"lvl2","url":"/euclid-ero#search-euclid-ero-images","position":12},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Search Euclid ERO Images"},"content":"Now, we search for the Euclid ERO images using the astroquery package.\nNote that the Euclid ERO images are no in the cloud currently, but we access them directly from IRSA using IRSA’s Simple Image Access (SIA) methods.\n\nNote\n\nThe following only works for combined images (either extended or point source stacks). This would not work if there are multiple, let’s say, H-band images of Euclid at a given position. Therefore, no time domain studies here (which is anyway not one of the main goals of Euclid).The IRSA SIA products can be listed via\n ```\n Irsa.list_collections(servicetype='SIA')\n ```\n\nHere we use the collection euclid_ero, containing the Euclid ERO images. We first create a SkyCoord object and then query the SIA.\n\nimage_tab = Irsa.query_sia(pos=(coord, search_radius), collection='euclid_ero')\nprint(\"Number of images available: {}\".format(len(image_tab)))\n\n\n\nSort the queried table by wavelength (column em_min). This allows us later when we plot the images to keep them sorted by wavelength (VIS, Y, J, H).\n\nimage_tab.sort('em_min')\n\n\n\nLet’s inspec the table that was downloaded.\n\nimage_tab[0:3]\n\n\n\nNext, we add additional restrictions to narrow down the images.\n\nThe Euclid ERO images come in two different flavors:\n\nFlattened images: idealized for compact sources (for example stars)\n\nLSB images: idealized for low surface brightness objects (for example galaxies)\n\nMaybe counter-intuitively, we select the LSB images here by checking if the file name given in the URL (access_url column) contains that key word. We found that the Flattened images contain many masked pixels.\n\n#sel_basic = np.where( [\"Flattened\" in tt[\"access_url\"] for tt in image_tab] )[0]\nsel_basic = np.where( [\"LSB\" in tt[\"access_url\"] for tt in image_tab] )[0]\nimage_tab = image_tab[sel_basic]\n\n\n\nNext, we want to check what filteres are available. This can be done by np.unique(), however, in that case it would sort the filters alphabetically. We want to keep the sorting based on the wavelength, therefore we opt for a more complicated way.\n\nidxs = np.unique(image_tab[\"energy_bandpassname\"], return_index=True)[1]\nfilters = [image_tab[\"energy_bandpassname\"][idx] for idx in sorted(idxs)]\nprint(\"Filters: {}\".format(filters))\n\n\n\nWe can now loop throught the filters and collect the images to create a handy summary table with all the data we have. This way we will have an easier time to later access the data.\n\n# Create a dictionary with all the necessary information (science, weight, noise, mask)\nsummary_table = Table(names=[\"filter\",\"products\",\"facility_name\",\"instrument_name\"] , dtype=[str,str,str,str])\nfor filt in filters:\n    sel = np.where(image_tab['energy_bandpassname'] == filt)[0]\n    products = list( np.unique(image_tab[\"dataproduct_subtype\"][sel].value) )\n    if \"science\" in products: # sort so that science is the first in the list. This is the order we create the hdu extensions\n        products.remove(\"science\")\n        products.insert(0,\"science\")\n    else:\n        print(\"WARNING: no science image found!\")\n    print(products)\n\n    summary_table.add_row( [filt , \";\".join(products), str(np.unique(image_tab[\"facility_name\"][sel].value)[0]), str(np.unique(image_tab[\"instrument_name\"][sel].value)[0])] )\n\n\n\nLet’s check out the summary table that we have created. We see that we have all the 4 Euclid bands and what data products are available for each of them.\n\nsummary_table\n\n\n\n","type":"content","url":"/euclid-ero#search-euclid-ero-images","position":13},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Create Cutout Images"},"type":"lvl2","url":"/euclid-ero#create-cutout-images","position":14},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Create Cutout Images"},"content":"Now that we have a list of data products, we can create the cutouts. This is important as the full Euclid ERO images would be too large to run extraction and photometry software on them (they would simply fail due to memory issues).\n\nFor each image, we create a cutout around the target of interest, using the cutout_size defined above. The cutouts are then collected into HDUs. That way we can easily access the different data products. Note that we only use the science products as the ancillary products are not needed.\n\nWe save the HDU to disk as it will be later used when we visualize the Euclid ERO FITS images in Firefly.\n\nNote\n\nYou will notice that Cutout2D can be applied to an URL. That way, it we do not need to download the full image to create a cutout. This is a useful trick to keep in mind when analyzing large images. This makes creating cutout images very fast.\n\nfor ii,filt in tqdm(enumerate(filters)):\n    products = summary_table[summary_table[\"filter\"] == filt][\"products\"][0].split(\";\")\n\n    for product in products:\n        sel = np.where( (image_tab[\"energy_bandpassname\"] == filt) & (image_tab[\"dataproduct_subtype\"] == product) )[0]\n\n        with fits.open(image_tab['access_url'][sel[0]], use_fsspec=True) as hdul:\n            tmp = Cutout2D(hdul[0].section, position=coord, size=cutout_size, wcs=WCS(hdul[0].header)) # create cutout\n\n\n            if (product == \"science\") & (ii == 0): # if science image, then create a new hdu.\n                hdu0 = fits.PrimaryHDU(data = tmp.data, header=hdul[0].header)\n                hdu0.header.update(tmp.wcs.to_header()) # update header with WCS info\n                hdu0.header[\"EXTNAME\"] = \"{}_{}\".format(filt,product.upper())\n                hdu0.header[\"PRODUCT\"] = product.upper()\n                hdu0.header[\"FILTER\"] = filt.upper()\n                hdulcutout = fits.HDUList([hdu0])\n            elif (product == \"science\") & (ii > 0):\n                hdu = fits.ImageHDU(data = tmp.data, header=hdul[0].header)\n                hdu.header.update(tmp.wcs.to_header()) # update header with WCS info\n                hdu.header[\"EXTNAME\"] = \"{}_{}\".format(filt,product.upper())\n                hdu.header[\"PRODUCT\"] = product.upper()\n                hdu.header[\"FILTER\"] = filt.upper()\n                hdulcutout.append(hdu)\n\n## Save the HDUL cube:\nhdulcutout.writeto(\"./data/euclid_images_test.fits\", overwrite=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s look at the HDU that we created to check what information we have. You see that all filters are collected in different extensions. Also note the different dimensions of the FITS layers as the pixel scale of VIS and NISP are different.\n\nhdulcutout.info()\n\n\n\nWe can now plot the image cutouts that we generated. The globular cluster is clearly visible.\n\nnimages = len(filters) # number of images\nncols = int(4) # number of columns\nnrows = int( nimages // ncols ) # number of rows\n\nfig = plt.figure(figsize = (5*ncols,5*nrows) )\naxs = [fig.add_subplot(nrows,ncols,ii+1) for ii in range(nimages)]\n\nfor ii,filt in enumerate(filters):\n    img = hdulcutout[\"{}_SCIENCE\".format(filt)].data\n    axs[ii].imshow(img , origin=\"lower\")\n    axs[ii].text(0.05,0.05 , \"{} ({}/{})\".format(summary_table[\"facility_name\"][ii],summary_table[\"instrument_name\"][ii],filt) , fontsize=14 , color=\"white\",\n                 va=\"bottom\", ha=\"left\" , transform=axs[ii].transAxes)\n\nplt.show()\n\n\n\n","type":"content","url":"/euclid-ero#create-cutout-images","position":15},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Extract Sources and Measure their Photometry on the VIS Image"},"type":"lvl2","url":"/euclid-ero#extract-sources-and-measure-their-photometry-on-the-vis-image","position":16},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Extract Sources and Measure their Photometry on the VIS Image"},"content":"Now that we have the images in memory (and on disk - but we do not need them, yet), we can measure the fluxes of the individual stars.\nOur simple photometry pipeline has different parts:\n\nFirst, we are using the Python package sep (similar to SExtractor) to extract the position of the sources. We do that on the VIS image, which provides the highest resolution.\n\nSecond, we use sep to perform aperture measurements of the photometry. We will use that to compare the obtained fluxes to our forced photometry method\n\nThird, we apply a PSF fitting technique (using the photutils Python package) to improve the photometry measurement\n\nWe start by extracting the sources using sep. We first isolate the data that we want to look at (the VIS image only).\n\n## Get Data (this will be replaced later)\nimg = hdulcutout[\"VIS_SCIENCE\"].data\nhdr = hdulcutout[\"VIS_SCIENCE\"].header\nimg[img == 0] = np.nan\n\n\n\nThere are some NaN value on the image that we need to mask out. For this we create a mask image that we later feed to sep.\n\nmask = np.isnan(img)\n\n\n\nNext, we compute the background statistics what will be used by sep to extract the sources above a certain threshold.\n\nmean, median, std = sigma_clipped_stats(img, sigma=3.0)\nprint(np.array((mean, median, std)))\n\n\n\n\n\nFinally, we perform object detection with sep. There are also modules in photutils to do that, however, we found that sep works best here. We output the number of objects found on the image.\n\nobjects = sep.extract(img-median, thresh=1.2, err=std, minarea=3, mask=mask, deblend_cont=0.0002, deblend_nthresh=64 )\nprint(\"Number of sources extracted: \", len(objects))\n\n\n\nNext, we perform simple aperture photometry on the detected sources. Again, we use the sep package for this. We will use these aperture photometry later to compare to the PSF photometry.\n\nflux, fluxerr, flag = sep.sum_circle(img-median, objects['x'], objects['y'],r=3.0, err=std, gain=1.0)\n\n\n\nNow, we use the photutils Python package to perform PSF fitting. Here we assume a simple Gaussian with a FWHM given by psf_fwhm as PSF.\n\nNote\n\nWe use a Gaussian PSF here for simplicity. The photometry can be improved by using a pixelated PSF measured directly from the Euclid images (for example by stacking stars).\n\npsf_fwhm = 0.16 # PSF FWHM in arcsec\npixscale = 0.1 # VIS pixel scale in arcsec/px\n\ninit_params = Table([objects[\"x\"],objects[\"y\"]] , names=[\"x\",\"y\"]) # initial positions\npsf_model = CircularGaussianSigmaPRF(flux=1, sigma=psf_fwhm/pixscale / 2.35)\npsf_model.x_0.fixed = True\npsf_model.y_0.fixed = True\npsf_model.sigma.fixed = False\nfit_shape = (5, 5)\npsfphot = PSFPhotometry(psf_model,\n                        fit_shape,\n                        finder = DAOStarFinder(fwhm=0.1, threshold=3.*std, exclude_border=True), # not needed because we are using fixed initial positions.\n                        aperture_radius = 4,\n                        progress_bar = True)\n\n\n\nAfter initiating the PSFPhotometry object, we can run the PSF photometry measurement. This can take a while (typically between 1 and 2 minutes).\n\nphot = psfphot(img-median, error=None, mask=mask, init_params=init_params)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce this is done, we can create a residual image.\n\nresimage = psfphot.make_residual_image(data = img-median, psf_shape = (9, 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now want to add the best-fit coordinates (R.A. and Decl.) to the VIS photometry catalog. For this, we have to convert the image coordinates into sky coordinates using the WCS information. We will need these coordinates because we want to use them as positional priors for the photometry measurement on the NISP images.\n\n## Add coordinates to catalog\nwcs1 = WCS(hdr) # VIS\nradec = wcs1.all_pix2world(phot[\"x_fit\"],phot[\"y_fit\"],0)\nphot[\"ra_fit\"] = radec[0]\nphot[\"dec_fit\"] = radec[1]\n\n\n\nFinally, we plot the VIS image and the residual with the extracted sources overlaid.\n\nfig = plt.figure(figsize=(20,10))\nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\nax1.imshow(np.log10(img), cmap=\"Greys\", origin=\"lower\")\nax1.plot(phot[\"x_fit\"], phot[\"y_fit\"] , \"o\", markersize=8 , markeredgecolor=\"red\", fillstyle=\"none\")\n\nax2.imshow(resimage,vmin=-5*std, vmax=5*std, cmap=\"RdBu\", origin=\"lower\")\nax2.plot(phot[\"x_fit\"], phot[\"y_fit\"] , \"o\", markersize=8 , markeredgecolor=\"red\", fillstyle=\"none\")\n\nplt.show()\n\n\n\nAs an additional check, we can compare the aperture photometry and the PSF photometry. We should find a good agreement between those two measurement methods. However, note that the PSF photometry should do a better job in deblending the fluxes of sources that are close by.\n\nx = objects[\"flux\"]\ny = phot[\"flux_fit\"]\n\nfig = plt.figure(figsize=(6,5))\nax1 = fig.add_subplot(1,1,1)\n\nax1.plot(x , y , \"o\", markersize=2, alpha=0.5)\nminlim = np.nanmin(np.concatenate((x,y)))\nmaxlim = np.nanmax(np.concatenate((x,y)))\n\nax1.fill_between(np.asarray([minlim,maxlim]),np.asarray([minlim,maxlim])/1.5,np.asarray([minlim,maxlim])*1.5, color=\"gray\", alpha=0.2, linewidth=0)\nax1.fill_between(np.asarray([minlim,maxlim]),np.asarray([minlim,maxlim])/1.2,np.asarray([minlim,maxlim])*1.2, color=\"gray\", alpha=0.4, linewidth=0)\nax1.plot(np.asarray([minlim,maxlim]),np.asarray([minlim,maxlim]), \":\", color=\"gray\")\n\nax1.set_xlabel(\"Aperture Photometry [flux]\")\nax1.set_ylabel(\"PSF forced-photometry [flux]\")\nax1.set_xscale('log')\nax1.set_yscale('log')\nplt.show()\n\n\n\n","type":"content","url":"/euclid-ero#extract-sources-and-measure-their-photometry-on-the-vis-image","position":17},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Measure the Photometry on the NISP Images"},"type":"lvl2","url":"/euclid-ero#measure-the-photometry-on-the-nisp-images","position":18},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Measure the Photometry on the NISP Images"},"content":"We now have the photometry and the position of sources on the VIS image. We can now proceed with similar steps on the NISP images. Because the NISP PSF and pixel scale are larger that those of the VIS images, we utilize the advantage of position prior-based forced photometry.\nFor this, we use the positions of the VIS measurements and perform PSF fitting on the NISP image using these priors.\n\nThe steps below are almost identical to the ones applied to the VIS images.\n\nWe first isolate the data, which is in this case the NISP H-band filter. Note that this is an arbitrary choice and you should be encouraged to try other filters as well!\n\nimg2 = hdulcutout[\"H_SCIENCE\"].data\nhdr2 = hdulcutout[\"H_SCIENCE\"].header\nimg2[img2 == 0] = np.nan\n\n\n\nWe again need to create a mask that will be fed to the PSF fitting module.\n\nmask2 = np.isnan(img2)\n\n\n\n... and we also get some statistics on the sky background.\n\nmean2, median2, std2 = sigma_clipped_stats(img2, sigma=3.0)\nprint(np.array((mean2, median2, std2)))\n\n\n\nNow, we need to obtain the (x,y) image coordinates on the NISP image that correspond to the extracted sources on the VIS image. We use the WCS information from the NISP image for this case and apply it to the sky coordinates obtained on the VIS image.\n\nwcs = WCS(hdr) # VIS\nwcs2 = WCS(hdr2) # NISP\nradec = wcs.all_pix2world(objects[\"x\"],objects[\"y\"], 0)\nxy = wcs2.all_world2pix(radec[0],radec[1],0)\n\n\n\nHaving all this set up, we can now again perform the PSF photometry using PSFPhotometry() from the photutils package. This again can take a while, typically 1 minute.\n\npsf_fwhm = 0.3 # arcsec\npixscale = 0.3 # arcsec/px\n\ninit_params = Table([xy[0],xy[1]] , names=[\"x\",\"y\"]) # initial positions\npsf_model = CircularGaussianSigmaPRF(flux=1, sigma=psf_fwhm/pixscale / 2.35)\npsf_model.x_0.fixed = True\npsf_model.y_0.fixed = True\npsf_model.sigma.fixed = False\nfit_shape = (3, 3)\npsfphot2 = PSFPhotometry(psf_model,\n                        fit_shape,\n                        finder = DAOStarFinder(fwhm=0.1, threshold=3.*std2, exclude_border=True), # not needed because we are using fixed initial positions.\n                        aperture_radius = 4,\n                        progress_bar = True)\nphot2 = psfphot2(img2-median2, error=None, mask=mask2, init_params=init_params)\nresimage2 = psfphot2.make_residual_image(data = img2-median2, psf_shape = (3, 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, we convert the pixel coordinates to sky coordinates and add them to the catalog.\n\nwcs2 = WCS(hdr2) # NISP\nradec = wcs2.all_pix2world(phot2[\"x_fit\"],phot2[\"y_fit\"],0)\nphot2[\"ra_fit\"] = radec[0]\nphot2[\"dec_fit\"] = radec[1]\n\n\n\nFinally, we create the same figure as above, showing the NISP image and the residual with the (VIS-extracted) sources overlaid.\n\nfig = plt.figure(figsize=(20,10))\nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\nax1.imshow(np.log10(img2), cmap=\"Greys\", origin=\"lower\")\nax1.plot(phot2[\"x_fit\"], phot2[\"y_fit\"] , \"o\", markersize=8 , markeredgecolor=\"red\", fillstyle=\"none\")\n\nax2.imshow(resimage2,vmin=-20*std2, vmax=20*std2, cmap=\"RdBu\", origin=\"lower\")\nax2.plot(phot2[\"x_fit\"], phot2[\"y_fit\"] , \"o\", markersize=8 , markeredgecolor=\"red\", fillstyle=\"none\")\n\nplt.show()\n\n\n\n","type":"content","url":"/euclid-ero#measure-the-photometry-on-the-nisp-images","position":19},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Load Gaia Catalog"},"type":"lvl2","url":"/euclid-ero#load-gaia-catalog","position":20},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Load Gaia Catalog"},"content":"We now load the Gaia sources at the location of the globular clusters. The goal is to compare the photometry of Gaia to the one derived above for the Euclid VIS and NISP images. This is scientifically useful, for example we can compute the colors of the stars in the Gaia optical bands and the Euclid near-IR bands.\nTo search for Gaia sources, we use astroquery again.\n\nWe first have to elimiate the row limit for the Gaia query by setting\n\nGaia.ROW_LIMIT = -1\n\n\n\nNext, we request the Gaia catalog around the position of the globular cluster. We use the same size as the cutout size.\n\ngaia_objects = Gaia.query_object_async(coordinate=coord, radius = cutout_size/2)\nprint(\"Number of Gaia stars found: {}\".format(len(gaia_objects)))\n\n\n\nWe then convert the sky coordinates of the Gaia stars to (x,y) image coordinates for VIS and NISP images using the corresponding WCS. This makes it more easy to plot the Gaia sources later on the images.\n\nwcs = WCS(hdr) # VIS\nwcs2 = WCS(hdr2) # NISP\nxy = wcs.all_world2pix(gaia_objects[\"ra\"],gaia_objects[\"dec\"],0)\nxy2 = wcs2.all_world2pix(gaia_objects[\"ra\"],gaia_objects[\"dec\"],0)\n\ngaia_objects[\"x_vis\"] = xy[0]\ngaia_objects[\"y_vis\"] = xy[1]\ngaia_objects[\"x_nisp\"] = xy2[0]\ngaia_objects[\"y_nisp\"] = xy2[1]\n\n\n\nWe save the Gaia table to disk as we will later use it for the visualization in Firefly.\n\ngaia_objects.write(\"./data/gaiatable.csv\", format=\"csv\", overwrite=True)\n\n\n\nNow we can overlay the Gaia sources on the VIS and NISP images (here the x/y coordinates become handy).\n\nfig = plt.figure(figsize=(20,10))\nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\nax1.imshow(np.log10(img), cmap=\"Greys\", origin=\"lower\")\nax1.plot(gaia_objects[\"x_vis\"], gaia_objects[\"y_vis\"] , \"o\", markersize=8 , markeredgecolor=\"red\", fillstyle=\"none\")\nax1.set_title(\"VIS\")\n\nax2.imshow(np.log10(img2), cmap=\"Greys\", origin=\"lower\")\nax2.plot(gaia_objects[\"x_nisp\"], gaia_objects[\"y_nisp\"] , \"o\", markersize=8 , markeredgecolor=\"red\", fillstyle=\"none\")\nax2.set_title(\"NISP\")\n\nplt.show()\n\n\n\n","type":"content","url":"/euclid-ero#load-gaia-catalog","position":21},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Match the Gaia Catalog to the VIS and NISP Catalogs"},"type":"lvl2","url":"/euclid-ero#match-the-gaia-catalog-to-the-vis-and-nisp-catalogs","position":22},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Match the Gaia Catalog to the VIS and NISP Catalogs"},"content":"Now, we match the Gaia source positions to the extracted sources in the VIS and NISP images.\n\nWe first define which Gaia columns to copy to the matched catalog as well as the matching distance.\n\ngaia_keys = [\"source_id\", \"phot_g_mean_mag\", \"phot_bp_mean_mag\", \"phot_rp_mean_mag\",\"ra\",\"dec\",\"pmra\",\"pmdec\"]\nmatching_distance = 0.6*u.arcsecond\n\n\n\nFirst match to the VIS image. We use the astropy SkyCoord() function for matching in sky coordinates.\n\nc = SkyCoord(ra=phot[\"ra_fit\"]*u.degree, dec=phot[\"dec_fit\"]*u.degree )\ncatalog = SkyCoord(ra=gaia_objects[\"ra\"].data*u.degree, dec=gaia_objects[\"dec\"].data*u.degree)\nidx, d2d, d3d = c.match_to_catalog_sky(catalog)\n\nsel_matched = np.where(d2d.to(u.arcsecond) < (matching_distance))[0]\nprint(\"Gaia Sources matched to VIS: {}\".format( len(sel_matched) ) )\nphot[\"gaia_distance\"] = d2d.to(u.arcsecond)\n\nfor gaia_key in gaia_keys:\n    phot[\"gaia_{}\".format(gaia_key)] = 0.0\n    phot[\"gaia_{}\".format(gaia_key)][sel_matched] = gaia_objects[gaia_key][idx[sel_matched]]\n\n\n\nAnd then we add the NISP sources. Note that we do not have to perform matching here because by design the VIS and NISP sources are matched (spatial prior forced photometry).\n\nphot2[\"gaia_distance\"] = d2d.to(u.arcsecond)\n\nfor gaia_key in gaia_keys:\n    phot2[\"gaia_{}\".format(gaia_key)] = 0.0\n    phot2[\"gaia_{}\".format(gaia_key)][sel_matched] = gaia_objects[gaia_key][idx[sel_matched]]\n\n\n\nOnce matched, we can now compare the Gaia and Euclid/NISP magnitudes of the stars.\n\n# Data\nx = phot[\"gaia_phot_rp_mean_mag\"]\ny = -2.5*np.log10(phot[\"flux_fit\"]) + hdr[\"ZP_STACK\"]\n\n# selection\nsel_good = np.where(phot[\"gaia_source_id\"] > 0)[0]\nx = x[sel_good]\ny = y[sel_good]\n\nfig = plt.figure(figsize=(6,5))\nax1 = fig.add_subplot(1,1,1)\n\nax1.plot(x , y , \"o\", markersize=2)\nminlim = np.nanmin(np.concatenate((x,y)))\nmaxlim = np.nanmax(np.concatenate((x,y)))\n\nax1.fill_between(np.asarray([minlim,maxlim]),np.asarray([minlim,maxlim])/1.5,np.asarray([minlim,maxlim])*1.5, color=\"gray\", alpha=0.2, linewidth=0)\nax1.fill_between(np.asarray([minlim,maxlim]),np.asarray([minlim,maxlim])/1.2,np.asarray([minlim,maxlim])*1.2, color=\"gray\", alpha=0.4, linewidth=0)\nax1.plot(np.asarray([minlim,maxlim]),np.asarray([minlim,maxlim]), \":\", color=\"gray\")\n\nax1.set_xlabel(\"Gaia Rp [mag]\")\nax1.set_ylabel(\"I$_E$ [mag]\")\nplt.show()\n\n\n\n","type":"content","url":"/euclid-ero#match-the-gaia-catalog-to-the-vis-and-nisp-catalogs","position":23},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Visualization with Firefly"},"type":"lvl2","url":"/euclid-ero#visualization-with-firefly","position":24},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"Visualization with Firefly"},"content":"At the end of this Notebook, we demonstrate how we can visualize the images and catalogs created above in Firefly.\n\nWe start by initializing the Firefly client.\nThe following line will open a new Firefly GUI in a separate tab inside the Jupyter Notebook environment. The user can drag the tab onto the currently open tab to create a “split tab”. This the user to see the code and images side-by-side.\n\n# Uncomment when using within Jupyter Lab with jupyter_firefly_extensions installed\n# fc = FireflyClient.make_lab_client()\n\n# Uncomment for contexts other than the above\nfc = FireflyClient.make_client(url=\"https://irsa.ipac.caltech.edu/irsaviewer\")\n\n\n\nIn order to display in image or catalog in Firefly, it needs to be uploaded to the Firefly server. We do this here using the upload_file() function.\nWe first upload the FITS image that we created above.\n\nfval = fc.upload_file('./data/euclid_images_test.fits')\n\n\n\nOnce the image is uploaded we can use the show_fits() function to display it.\nNote that our FITS image has multiple extensions (VIS, and NISP bands). We can open them separately in new Firefly tabs by looping over the HDUs and specifying the plot ID by the extension’s name.\n\nfor hh,hdu in enumerate(hdulcutout):\n    fc.show_fits(fval, MultiImageIdx=hh, plot_id=hdu.header[\"EXTNAME\"] )\n\n\n\nWe can lock the WCS between the images (allowing the user to pan and zoom the images simultaneously) by running:\n\nfc.align_images(lock_match=True)\n\n\n\nIn the same way, we can upload a table, in this case our Gaia table. We again use upload_file() but in this case we use show_table() to show it in Firefly.\n\ntval = fc.upload_file('./data/gaiatable.csv')\nfc.show_table(tval, tbl_id = \"gaiatable\")\n\n\n\nNow, check out the Firefly GUI. You can zoom the images, click on sources, filter the table, display different selection, and much more!\n\n","type":"content","url":"/euclid-ero#visualization-with-firefly","position":25},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-ero#about-this-notebook","position":26},{"hierarchy":{"lvl1":"Exploring Star Clusters in the Euclid ERO Data","lvl2":"About this Notebook"},"content":"Author: Andreas Faisst (IPAC Scientist)\n\nUpdated: 2025-03-17\n\nContact: the \n\nIRSA Helpdesk with questions or reporting problems.","type":"content","url":"/euclid-ero#about-this-notebook","position":27},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access"},"type":"lvl1","url":"/euclid-cloud-access","position":0},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access"},"content":"","type":"content","url":"/euclid-cloud-access","position":1},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-cloud-access#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nLearn where Euclid Q1 data are stored in the cloud.\n\nRetrieve an image cutout from the cloud.\n\nRetrieve a spectrum from the cloud.\n\n","type":"content","url":"/euclid-cloud-access#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"1. Introduction"},"type":"lvl2","url":"/euclid-cloud-access#id-1-introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"1. Introduction"},"content":"Euclid launched in July 2023 as a European Space Agency (ESA) mission with involvement by NASA.\nThe primary science goals of Euclid are to better understand the composition and evolution of the dark Universe.\nThe Euclid mission is providing space-based imaging and spectroscopy as well as supporting ground-based imaging to achieve these primary goals.\nThese data will be archived by multiple global repositories, including IRSA, where they will support transformational work in many areas of astrophysics.\n\nEuclid Quick Release 1 (Q1) consists of consists of ~30 TB of imaging, spectroscopy, and catalogs covering four non-contiguous fields:\nEuclid Deep Field North (22.9 sq deg), Euclid Deep Field Fornax (12.1 sq deg), Euclid Deep Field South (28.1 sq deg), and LDN1641.\n\nIRSA maintains copies of the Euclid Q1 data products both on premises at IPAC and on the cloud via Amazon Web Services (AWS).\nThis notebook provides an introduction to accessing Euclid Q1 data from the cloud.\nIf you have questions, please contact the \n\nIRSA helpdesk.\n\n","type":"content","url":"/euclid-cloud-access#id-1-introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"2. Imports"},"type":"lvl2","url":"/euclid-cloud-access#id-2-imports","position":6},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"2. Imports"},"content":"s3fs for browsing S3 buckets\n\nastropy for handling coordinates, units, FITS I/O, tables, images, etc.\n\nastroquery>=0.4.10 for querying Euclid data products from IRSA\n\nmatplotlib for visualization\n\njson for decoding JSON strings\n\nImportant\n\nWe rely on astroquery features that have been recently added, so please make sure you have version v0.4.10 or newer installed.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install s3fs astropy 'astroquery>=0.4.10' matplotlib\n\n\n\n\n\nimport s3fs\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nfrom astropy.visualization import ImageNormalize, PercentileInterval, AsinhStretch\nfrom astropy.io import fits\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nfrom astropy.table import Table\nfrom astroquery.ipac.irsa import Irsa\nfrom matplotlib import pyplot as plt\nimport json\n\n\n\n","type":"content","url":"/euclid-cloud-access#id-2-imports","position":7},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"3. Browse Euclid Q1 cloud-hosted data"},"type":"lvl2","url":"/euclid-cloud-access#id-3-browse-euclid-q1-cloud-hosted-data","position":8},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"3. Browse Euclid Q1 cloud-hosted data"},"content":"\n\nBUCKET_NAME = 'nasa-irsa-euclid-q1'\n\n\n\ns3fs provides a filesystem-like python interface for AWS S3 buckets. First we create a s3 client:\n\ns3 = s3fs.S3FileSystem(anon=True)\n\n\n\nThen we list the q1 directory that contains Euclid Q1 data products:\n\ns3.ls(f'{BUCKET_NAME}/q1')\n\n\n\nLet’s navigate to MER images (available as FITS files):\n\ns3.ls(f'{BUCKET_NAME}/q1/MER')[:10] # ls only top 10 to limit the long output\n\n\n\n\n\ns3.ls(f'{BUCKET_NAME}/q1/MER/102018211') # pick any tile ID from above\n\n\n\n\n\ns3.ls(f'{BUCKET_NAME}/q1/MER/102018211/VIS') # pick any instrument from above\n\n\n\nAs per “Browsable Directories” section in \n\nuser guide, we need MER/{tile_id}/{instrument}/EUC_MER_BGSUB-MOSAIC*.fits for displaying background-subtracted mosiac images. But these images are stored under TILE IDs so first we need to find TILE ID for a coordinate search we are interested in. We will use astroquery (in next section) to retrieve FITS file paths for our coordinates by doing spatial search.\n\n","type":"content","url":"/euclid-cloud-access#id-3-browse-euclid-q1-cloud-hosted-data","position":9},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"4. Do a spatial search for MER mosaics"},"type":"lvl2","url":"/euclid-cloud-access#id-4-do-a-spatial-search-for-mer-mosaics","position":10},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"4. Do a spatial search for MER mosaics"},"content":"Pick a target and search radius:\n\ntarget_name = 'TYC 4429-1677-1'\ncoord = SkyCoord.from_name(target_name)\nsearch_radius = 10 * u.arcsec\n\n\n\nList all Simple Image Access (SIA) collections for IRSA with names containing \"euclid\":\n\n ```{code-cell} ipython3\n collections = Irsa.list_collections(servicetype='SIA', filter='euclid')\n collections\n ```\n\nAs per “Data Products Overview” in \n\nuser guide and above table, we identify that MER Mosiacs are available as the following collection:\n\nimg_collection = 'euclid_DpdMerBksMosaic'\n\n\n\nNow query this collection for our target’s coordinates and search radius:\n\nimg_tbl = Irsa.query_sia(pos=(coord, search_radius), collection=img_collection)\nimg_tbl\n\n\n\nLet’s narrow it down to the images with science dataproduct subtype and Euclid facility:\n\neuclid_sci_img_tbl = img_tbl[[row['facility_name']=='Euclid'\n                              and row['dataproduct_subtype']=='science'\n                              for row in img_tbl]]\neuclid_sci_img_tbl\n\n\n\nWe can see there’s a cloud_access column that gives us the location info of the image files we are interested in. So let’s extract the S3 bucket file path from it:\n\ndef get_s3_fpath(cloud_access):\n    cloud_info = json.loads(cloud_access) # converts str to dict\n    bucket_name = cloud_info['aws']['bucket_name']\n    key = cloud_info['aws']['key']\n\n    return f'{bucket_name}/{key}'\n\n\n\n\n\n[get_s3_fpath(row['cloud_access']) for row in euclid_sci_img_tbl]\n\n\n\nLet’s also extract filter names to use when displaying the images:\n\ndef get_filter_name(instrument, bandpass):\n    return f'{instrument}_{bandpass}' if instrument!=bandpass else instrument\n\n\n\n\n\n[get_filter_name(row['instrument_name'], row['energy_bandpassname']) for row in euclid_sci_img_tbl]\n\n\n\n","type":"content","url":"/euclid-cloud-access#id-4-do-a-spatial-search-for-mer-mosaics","position":11},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"5. Efficiently retrieve mosaic cutouts"},"type":"lvl2","url":"/euclid-cloud-access#id-5-efficiently-retrieve-mosaic-cutouts","position":12},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"5. Efficiently retrieve mosaic cutouts"},"content":"These image files are very big (~1.4GB), so we use astropy’s lazy-loading capability of FITS for better performance. (See \n\nObtaining subsets from cloud-hosted FITS files.)\n\ncutout_size = 1 * u.arcmin\n\n\n\n\n\ncutouts = []\nfilters = []\n\nfor row in euclid_sci_img_tbl:\n    s3_fpath = get_s3_fpath(row['cloud_access'])\n    filter_name = get_filter_name(row['instrument_name'], row['energy_bandpassname'])\n\n    with fits.open(f's3://{s3_fpath}', fsspec_kwargs={\"anon\": True}) as hdul:\n        print(f'Retrieving cutout for {filter_name} ...')\n        cutout = Cutout2D(hdul[0].section,\n                          position=coord,\n                          size=cutout_size,\n                          wcs=WCS(hdul[0].header))\n        cutouts.append(cutout)\n        filters.append(filter_name)\n\n\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(4 * 2, 4 * 2), subplot_kw={'projection': cutouts[0].wcs})\n\nfor idx, ax in enumerate(axes.flat):\n    norm = ImageNormalize(cutouts[idx].data, interval=PercentileInterval(99), stretch=AsinhStretch())\n    ax.imshow(cutouts[idx].data, cmap='gray', origin='lower', norm=norm)\n    ax.set_xlabel('RA')\n    ax.set_ylabel('Dec')\n    ax.text(0.95, 0.05, filters[idx], color='white', fontsize=14, transform=ax.transAxes, va='bottom', ha='right')\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/euclid-cloud-access#id-5-efficiently-retrieve-mosaic-cutouts","position":13},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"6. Find the MER catalog for a given tile"},"type":"lvl2","url":"/euclid-cloud-access#id-6-find-the-mer-catalog-for-a-given-tile","position":14},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"6. Find the MER catalog for a given tile"},"content":"Let’s navigate to MER catalog in the Euclid Q1 bucket:\n\ns3.ls(f'{BUCKET_NAME}/q1/catalogs')\n\n\n\n\n\ns3.ls(f'{BUCKET_NAME}/q1/catalogs/MER_FINAL_CATALOG')[:10] # ls only top 10 to limit the long output\n\n\n\n\n\nmer_tile_id = 102160339 # from the image paths for the target we picked\ns3.ls(f'{BUCKET_NAME}/q1/catalogs/MER_FINAL_CATALOG/{mer_tile_id}')\n\n\n\nAs per “Browsable Directiories” section in \n\nuser guide, we can use catalogs/MER_FINAL_CATALOG/{tile_id}/EUC_MER_FINAL-CAT*.fits for listing the objects catalogued. We can read the identified FITS file as table and do filtering on ra, dec columns to find object ID(s) only for the target we picked. But it will be an expensive operation so we will instead use astroquery (in next section) to do a spatial search in the MER catalog provided by IRSA.\n\nNote\n\nOnce the catalogs are available as Parquet files in the cloud, we can efficiently do spatial filtering directly on the cloud-hosted file to identify object ID(s) for our target. But for the time being, we can use catalog VO services through astroquery to do the same.\n\n","type":"content","url":"/euclid-cloud-access#id-6-find-the-mer-catalog-for-a-given-tile","position":15},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"7. Find the MER Object ID for our target"},"type":"lvl2","url":"/euclid-cloud-access#id-7-find-the-mer-object-id-for-our-target","position":16},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"7. Find the MER Object ID for our target"},"content":"First, list the Euclid catalogs provided by IRSA:\n\ncatalogs = Irsa.list_catalogs(full=True, filter='euclid')\ncatalogs\n\n\n\nFrom this table, we can extract the MER catalog name. We also see several other interesting catalogs, let’s also extract spectral file association catalog for retrieving spectra later.\n\neuclid_mer_catalog = 'euclid_q1_mer_catalogue'\neuclid_spec_association_catalog = 'euclid.objectid_spectrafile_association_q1'\n\n\n\nNow, we do a region search within a cone of 5 arcsec around our target to pinpoint its object ID in Euclid catalog:\n\nsearch_radius = 5 * u.arcsec\n\nmer_catalog_tbl = Irsa.query_region(coordinates=coord, spatial='Cone',\n                                    catalog=euclid_mer_catalog, radius=search_radius)\nmer_catalog_tbl\n\n\n\n\n\nobject_id = int(mer_catalog_tbl['object_id'][0])\nobject_id\n\n\n\n","type":"content","url":"/euclid-cloud-access#id-7-find-the-mer-object-id-for-our-target","position":17},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"8. Find the spectrum of an object in the MER catalog"},"type":"lvl2","url":"/euclid-cloud-access#id-8-find-the-spectrum-of-an-object-in-the-mer-catalog","position":18},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"8. Find the spectrum of an object in the MER catalog"},"content":"Using the object ID(s) we extracted above, we can narrow down the spectral file association catalog to identify spectra file path(s). So we do the following TAP search:\n\nadql_query = f\"SELECT * FROM {euclid_spec_association_catalog} \\\n    WHERE objectid = {object_id}\"\n\nspec_association_tbl = Irsa.query_tap(adql_query).to_table()\nspec_association_tbl\n\n\n\nWarning\n\nIf you picked a target other than what this notebook uses, it’s possible that there is no spectrum associated for your target’s object ID. In that case, spec_association_tbl will contain 0 rows.\n\nIn above table, we can see that the 'path' column gives us a url that can be used to call an IRSA service to get the spectrum of our object as SpectrumDM VOTable. We can map it to an S3 bucket key to retrieve a spectra file from the cloud. This is a very big FITS spectra file with multiple extensions where each extension contains spectrum of one object. The 'hdu' column gives us the extension number for our object. So let’s extract both of these.\n\nspec_fpath_key = spec_association_tbl['path'][0].replace('api/spectrumdm/convert/euclid/', '').split('?')[0]\nspec_fpath_key\n\n\n\n\n\nobject_hdu_idx = int(spec_association_tbl['hdu'][0])\nobject_hdu_idx\n\n\n\nAgain, we use astropy’s lazy-loading capability of FITS to only retrieve the spectrum table of our object from the S3 bucket.\n\nwith fits.open(f's3://{BUCKET_NAME}/{spec_fpath_key}', fsspec_kwargs={'anon': True}) as hdul:\n    spec_hdu = hdul[object_hdu_idx]\n    spec_tbl = Table.read(spec_hdu)\n    spec_header = spec_hdu.header\n\n\n\n\n\nspec_tbl\n\n\n\n\n\n# The signal needs to be multiplied by the scale factor in the header.\nplt.plot(spec_tbl['WAVELENGTH'], spec_header['FSCALE'] * spec_tbl['SIGNAL'])\nplt.xlabel(spec_tbl['WAVELENGTH'].unit.to_string('latex_inline'))\nplt.ylabel(spec_tbl['SIGNAL'].unit.to_string('latex_inline'))\n\nplt.title(f'Spectrum of Target: {target_name}\\n(Euclid Object ID: {object_id})');\n\n\n\n","type":"content","url":"/euclid-cloud-access#id-8-find-the-spectrum-of-an-object-in-the-mer-catalog","position":19},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"About this Notebook"},"type":"lvl2","url":"/euclid-cloud-access#about-this-notebook","position":20},{"hierarchy":{"lvl1":"Euclid Q1: Cloud Access","lvl2":"About this Notebook"},"content":"Author: Jaladh Singhal (IRSA Developer) in conjunction with Vandana Desai, Brigitta Sipőcz, Tiffany Meshkat, Troy Raen, and the IRSA Data Science Team\n\nUpdated: 2025-09-23\n\nContact: the \n\nIRSA Helpdesk with questions or reporting problems.","type":"content","url":"/euclid-cloud-access#about-this-notebook","position":21},{"hierarchy":{"lvl1":"Euclid Tutorial Notebooks"},"type":"lvl1","url":"/euclid","position":0},{"hierarchy":{"lvl1":"Euclid Tutorial Notebooks"},"content":"Euclid launched in July 2023 with the primary science goals of better understanding the composition and evolution of the dark Universe.\nIt carries two instruments: the VISible instrument (VIS) and the Near-Infrared Spectrometer and Photometer (NISP).\n\nQuick Release 1 (Q1) was released in March 2025 and consists of approximately 35 TB of imaging, spectroscopy, and catalogs, covering four non-contiguous fields totaling 63 square degrees.\nData products include MERged mosaics of calibrated and stacked frames; combined infrared spectra (SIR); and catalogs of MER objects, photometric redshifts and classifications (PHZ), and spectroscopic redshifts and line measurements (SPE).\n\nData products include:\n\nMER (merged) mosaic images of calibrated and stacked frames;\n\nSIR combined infrared spectra;\n\nCatalogs of MER objects, photometric redshifts and classifications (PHZ), and spectroscopic redshifts and line measurements (SPE);\n\nMerged Objects Catalog (created by IRSA) containing the MER, PHZ, and SPE catalogs in a single HATS Catalog.\n\nMER Image Mosaics →\n\nRetrieve both a full MER mosaic image and multi-wavelength cutouts, then subtract the background from the cutouts and extract sources.\n\nSIR 1D Spectra →\n\nLoad a galaxy spectrum and plot it. Understand the wavelength, flux, and mask values.\n\nMER Catalogs →\n\nExplore the columns in the MER final catalog, query for stars, and create a color-magnitude diagram.\n\nPHZ Catalogs →\n\nJoin the PHZ and MER catalogs to query galaxies with quality redshifts in a box region, create MER mosaic cutouts with catalog overlays, and plot the brightest galaxy’s SIR spectrum.\n\nSPE Catalogs →\n\nJoin the SPE and MER catalogs and query for galaxies with H-alpha line detections, then plot the SIR spectrum of a galaxy with a high SNR H-alpha line measurement.\n\nMerged Objects Catalog →\n\nIntroduction: Understand the content and format of the Euclid Q1 Merged Objects HATS Catalog, then perform a basic query.\n\nMerged Objects Catalog →\n\nMagnitudes: Review the types of flux measurements available, load template-fit and aperture magnitudes, and plot distributions and comparisons for different object types.\n\nCloud Access →\n\nBrowse the on-cloud copy of Q1, then efficiently retrieve a MER mosaic cutout and a SIR spectrum.\n\nERO Star Clusters →\n\nCreate multi-wavelength ERO image cutouts of a globular cluster, extract sources, and measure photometry. Match Gaia sources with Euclid ERO catalogs, then visualize with Firefly.","type":"content","url":"/euclid","position":1},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction"},"type":"lvl1","url":"/euclid-q1-hats-intro","position":0},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction"},"content":"This tutorial is an introduction to the content and format of the Euclid Q1 Merged Objects HATS Catalog.\nLater tutorials in this series will show how to load quality samples.\n\n","type":"content","url":"/euclid-q1-hats-intro","position":1},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-q1-hats-intro#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"Learning Goals"},"content":"In this tutorial, we will:\n\nLearn about the Euclid Merged Objects catalog that IRSA created by combining information from multiple Euclid Quick Release 1 (Q1) catalogs.\n\nFind columns of interest.\n\nPerform a basic query using the Python library PyArrow.\n\n","type":"content","url":"/euclid-q1-hats-intro#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"1. Introduction"},"type":"lvl2","url":"/euclid-q1-hats-intro#id-1-introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"1. Introduction"},"content":"The \n\nEuclid Q1 catalogs were derived from Euclid photometry and spectroscopy, taken by the Visible Camera (VIS) and the Near-Infrared Spectrometer and Photometer (NISP), and from photometry taken by other ground-based instruments.\nThe data include several flux measurements per band, several redshift estimates, several morphology parameters, etc.\nEach was derived for different science goals using different algorithms or configurations.\n\nThe Euclid Q1 Merged Objects HATS Catalog was produced by IRSA by joining 14 of the original catalogs on object ID (column: object_id).\nFollowing the Hierarchical Adaptive Tiling Scheme \n\nHATS framework, the data were then partitioned spatially (by right ascension and declination) and written as an Apache Parquet dataset.\n\nColumns: 1,594\n\nRows: 29,953,430 (one per Euclid Q1 object)\n\nSize: 400 GB\n\nThe catalog is served from an AWS S3 cloud storage bucket.\nAccess is free and no credentials are required.\n\n","type":"content","url":"/euclid-q1-hats-intro#id-1-introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"2. Imports"},"type":"lvl2","url":"/euclid-q1-hats-intro#id-2-imports","position":6},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"2. Imports"},"content":"\n\n# # Uncomment the next line to install dependencies if needed.\n# %pip install hpgeom pandas pyarrow\n\n\n\n\n\nimport hpgeom  # Find HEALPix indexes from RA and Dec\nimport pyarrow.compute as pc  # Filter the catalog\nimport pyarrow.dataset  # Load the catalog\nimport pyarrow.fs  # Simple S3 filesystem pointer\nimport pyarrow.parquet  # Load the schema\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-2-imports","position":7},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"3. Load Parquet Metadata"},"type":"lvl2","url":"/euclid-q1-hats-intro#id-3-load-parquet-metadata","position":8},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"3. Load Parquet Metadata"},"content":"First we’ll load the Parquet schema (column information) of the Merged Objects catalog so we can use it in later sections.\nThe Parquet schema is accessible from a few locations, all of which include the column names and types.\nHere, we load it from the _common_metadata file because it also includes the column units and descriptions.\n\n# AWS S3 paths.\ns3_bucket = \"nasa-irsa-euclid-q1\"\ndataset_prefix = \"contributed/q1/merged_objects/hats/euclid_q1_merged_objects-hats/dataset\"\n\ndataset_path = f\"{s3_bucket}/{dataset_prefix}\"\nschema_path = f\"{dataset_path}/_common_metadata\"\n\n# S3 pointer. Use `anonymous=True` to access without credentials.\ns3 = pyarrow.fs.S3FileSystem(anonymous=True)\n\n\n\n\n\n# Load the Parquet schema.\nschema = pyarrow.parquet.read_schema(schema_path, filesystem=s3)\n\n# There are almost 1600 columns in this dataset.\nprint(f\"{len(schema)} columns in the Euclid Q1 Merged Objects catalog\")\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-3-load-parquet-metadata","position":9},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"4. Merged Objects Catalog Contents"},"type":"lvl2","url":"/euclid-q1-hats-intro#id-4-merged-objects-catalog-contents","position":10},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"4. Merged Objects Catalog Contents"},"content":"\n\nThe Merged Objects catalog contains data from 14 Euclid Q1 tables, joined on the column object_id.\nThe tables were produced by three Euclid processing functions: MER (multi-wavelength mosaics on common spatial and pixel scales), PHZ (photometric redshifts), and SPE (spectroscopy).\nThe subsections below include the table names, links to reference papers, URLs to the original table schemas, and examples of how the column names were transformed for the Merged Objects catalog.\n\nThe original tables’ column names are mostly in all caps.\nIn the Merged Objects catalog and the catalogs available through IRSA’s TAP service, all column names have been lower-cased.\nIn addition, all non-alphanumeric characters have been replaced with an underscore for compatibility with various libraries and services.\nFinally, the original table name has been prepended to column names in the Merged Objects catalog, both for provenance and to avoid duplicates.\nAn example that includes all of these transformations is: E(B-V) -> physparamqso_e_b_v_.\n\nThree columns have special names that differ from the standard naming convention described above:\n\nobject_id : Euclid MER Object ID. Unique identifier of a row in this dataset. No table name prepended.\n\nra : ‘RIGHT_ASCENSION’ from the ‘mer’ table. Name shortened to match other IRSA services. No table name prepended.\n\ndec : ‘DECLINATION’ from the ‘mer’ table. Name shortened to match other IRSA services. No table name prepended.\n\nSeven additional columns have been added to the Merged Objects catalog that are not in the original Euclid tables.\nThey are described below, after the Euclid tables.","type":"content","url":"/euclid-q1-hats-intro#id-4-merged-objects-catalog-contents","position":11},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.1 MER tables","lvl2":"4. Merged Objects Catalog Contents"},"type":"lvl3","url":"/euclid-q1-hats-intro#id-4-1-mer-tables","position":12},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.1 MER tables","lvl2":"4. Merged Objects Catalog Contents"},"content":"The Euclid MER processing function produced three tables.\nThe reference paper is \n\nEuclid Collaboration: Romelli et al., 2025 (hereafter, Romelli).\nThe tables are:\n\nMain table (mer)\n\nDescribed in Romelli sections 6 & 8 (“EUC_MER_FINAL-CAT”)\n\nOriginal schema: \n\nMain catalog FITS file\n\nExample column name transform: FLUX_DETECTION_TOTAL --> mer_flux_detection_total\n\nMorphology (morph)\n\nDescribed in Romelli sections 7 & 8 (“EUC_MER_FINAL-MORPH-CAT”)\n\nOriginal schema: \n\nMorphology catalog FITS file\n\nExample column name transform: CONCENTRATION --> morph_concentration\n\nCutouts (cutouts)\n\nDescribed in Romelli section 8 (“EUC_MER_FINAL-CUTOUTS-CAT”)\n\nOriginal schema: \n\nCutouts catalog FITS file\n\nExample column name transform: CORNER_0_RA --> cutouts_corner_0_ra\n\nFind all columns from these tables in the Parquet schema:\n\nmer_prefixes = [\"mer_\", \"morph_\", \"cutouts_\"]\nmer_col_counts = {p: len([n for n in schema.names if n.startswith(p)]) for p in mer_prefixes}\n\nprint(f\"MER tables: {sum(mer_col_counts.values())} columns total\")\nfor prefix, count in mer_col_counts.items():\n    print(f\"  {prefix}: {count}\")\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-4-1-mer-tables","position":13},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.2 PHZ tables","lvl2":"4. Merged Objects Catalog Contents"},"type":"lvl3","url":"/euclid-q1-hats-intro#id-4-2-phz-tables","position":14},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.2 PHZ tables","lvl2":"4. Merged Objects Catalog Contents"},"content":"The Euclid PHZ processing function produced eight tables.\nThe reference paper is \n\nEuclid Collaboration: Tucci et al., 2025 (hereafter, Tucci).\nThe tables are:\n\nPhotometric Redshifts (phz)\n\nDescribed in Tucci section 5 (“phz_photo_z”)\n\nOriginal schema: \n\nPhoto Z catalog\n\nExample column name transform: PHZ_PDF --> phz_phz_pdf\n\nClassifications (class)\n\nDescribed in Tucci section 4 (“phz_classification”)\n\nOriginal schema: \n\nClassification catalog\n\nExample column name transform: PHZ_CLASSIFICATION --> class_phz_classification\n\nGalaxy Physical Parameters (physparam)\n\nDescribed in Tucci section 6 (6.1; “phz_physical_parameters”)\n\nOriginal schema: \n\nPhysical Parameters catalog\n\nExample column name transform: PHZ_PP_MEDIAN_REDSHIFT --> physparam_phz_pp_median_redshift\n\nGalaxy SEDs (galaxysed)\n\nDescribed in Tucci appendix B (B.1 “phz_galaxy_sed”)\n\nOriginal schema: \n\nGalaxy SED catalog\n\nExample column name transform: FLUX_4900_5000 --> galaxysed_flux_4900_5000\n\nQSO Physical Parameters (physparamqso)\n\nDescribed in Tucci section 6 (6.2; “phz_qso_physical_parameters”)\n\nOriginal schema: \n\nQSO Physical Parameters catalog\n\nExample column name transform: E(B-V) --> physparamqso_e_b_v_\n\nStar Parameters (starclass)\n\nDescribed in Tucci section 6 (6.3; “phz_star_template”)\n\nOriginal schema: \n\nStar Template\n\nExample column name transform: FLUX_VIS_Total_Corrected --> starclass_flux_vis_total_corrected\n\nStar SEDs (starsed)\n\nDescribed in Tucci appendix B (B.1 “phz_star_sed”)\n\nOriginal schema: \n\nStar SED catalog\n\nExample column name transform: FLUX_4900_5000 --> starsed_flux_4900_5000\n\nNIR Physical Parameters (physparamnir)\n\nDescribed in Tucci section 6 (6.4; “phz_nir_physical_parameters”)\n\nOriginal schema: \n\nNIR Physical Parameters catalog\n\nExample column name transform: E(B-V) --> physparamnir_e_b_v_\n\nFind all columns from these tables in the Parquet schema:\n\nphz_prefixes = [\"phz_\", \"class_\", \"physparam_\", \"galaxysed_\", \"physparamqso_\",\n                \"starclass_\", \"starsed_\", \"physparamnir_\"]\nphz_col_counts = {p: len([n for n in schema.names if n.startswith(p)]) for p in phz_prefixes}\n\nprint(f\"PHZ tables: {sum(phz_col_counts.values())} columns total\")\nfor prefix, count in phz_col_counts.items():\n    print(f\"  {prefix}: {count}\")\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-4-2-phz-tables","position":15},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.3 SPE tables","lvl2":"4. Merged Objects Catalog Contents"},"type":"lvl3","url":"/euclid-q1-hats-intro#id-4-3-spe-tables","position":16},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.3 SPE tables","lvl2":"4. Merged Objects Catalog Contents"},"content":"The Euclid SPE processing function produced three tables from which data are included in the Merged Objects catalog.\nThe reference paper is \n\nEuclid Collaboration: Le Brun et al., 2025 (hereafter, Le Brun).\n\nThese tables required special handling because they contain multiple rows per object (identified by column object_id).\nThe tables were pivoted before being joined so that the Merged Objects catalog contains one row per object.\nThe pivoted columns were named by combining at least the table name, the original column name, and the rank of the redshift estimate (i.e., the value in the original ‘SPE_RANK’ column).\n\nThe tables are:\n\nSpectroscopic Redshifts (z)\n\nDescribed in Le Brun section 2 (“spectro_zcatalog_spe_quality”, “spectro_zcatalog_spe_classification”, “spectro_zcatalog_spe_galaxy_candidates”, “spectro_zcatalog_spe_star_candidates”, and “spectro_zcatalog_spe_qso_candidates”)\n\nOriginal schema: \n\nRedshift catalog\n\nPivot and naming: For each object, the original table contains up to 5 redshift estimates (ranked by confidence) produced by assuming the object is a galaxy, star, and QSO -- for a total of up to 15 rows per object.\nThe table was pivoted to one row per object and the resulting columns were named by prepending the table name (z) and the assumed type (galaxy_candidates, star_candidates, and qso_candidates), and appending the rank (rank[0-4]).\n\nExample column name transform: SPE_PDF --> z_galaxy_candidates_spe_pdf_rank0 (top-ranked redshift PDF, assuming galaxy)\n\nSpectral Line Measurements (lines)\n\nDescribed in Le Brun section 5 (“spectro_line_features_catalog_spe_line_features_cat”).\nNotice that lines were identified by assuming the object is a galaxy.\n\nOriginal schema: \n\nLines catalog (HDU#1 only)\n\nPivot and naming: For each object and line, the original table contains up to 5 rows, one per galaxy redshift estimate from the z table.\nThe Merged Objects catalog only contains the Halpha line measurements.\nThe table was pivoted to one row per object and the resulting columns were named by prepending the table name (lines) and appending both the redshift rank (rank[0-4]) and the name of the line (halpha).\n\nExample column name transform: SPE_LINE_FLUX_GF --> lines_spe_line_flux_gf_rank0_halpha (Halpha line flux of the top-ranked redshift, assuming galaxy)\n\nModels (models)\n\nDescribed in Le Brun section 5 (“spectro_model_catalog_spe_lines_catalog”)\n\nOriginal schema: \n\nModels catalog (HDU#2 only)\n\nPivot and naming: The original table has the same structure as the z table.\nThe Merged Objects catalog only contains the galaxy solutions.\nThe table was pivoted to one row per object and the resulting columns were named by prepending the table name (models) and the assumed type (galaxy), and appending the redshift rank (rank[0-4]).\n\nExample column name transform: SPE_VEL_DISP_E --> models_galaxy_spe_vel_disp_e_rank0 (velocity dispersion of the top-ranked redshift, assuming galaxy)\n\nFind all columns from these tables in the Parquet schema:\n\nspe_prefixes = [\"z_\", \"lines_\", \"models_\"]\nspe_col_counts = {p: len([n for n in schema.names if n.startswith(p)]) for p in spe_prefixes}\n\nprint(f\"SPE tables: {sum(spe_col_counts.values())} columns total\")\nfor prefix, count in spe_col_counts.items():\n    print(f\"  {prefix[:-1]}: {count}\")\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-4-3-spe-tables","position":17},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.4 Additional columns","lvl2":"4. Merged Objects Catalog Contents"},"type":"lvl3","url":"/euclid-q1-hats-intro#id-4-4-additional-columns","position":18},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.4 Additional columns","lvl2":"4. Merged Objects Catalog Contents"},"content":"The following columns were added to the Merged Objects catalog but do not appear in the original Euclid tables.\n\nEuclid columns:\n\ntileid : ID of the Euclid Tile the object was detected in.\nThe Euclid tiling is described in Romelli section 3.1.\n\nHEALPix columns:\n\nThese HEALPix indexes correspond to the object’s RA and Dec coordinates.\nThey are useful for spatial queries, as demonstrated in the Euclid Deep Fields section below.\n\n_healpix_9 : HEALPix order 9 pixel index.\nOrder 9 pixels have a resolution (square root of area) of ~400 arcseconds or ~0.1 degrees.\n\n_healpix_19 : HEALPix order 19 pixel index.\nOrder 19 pixels have a resolution of ~0.4 arcseconds.\n\n_healpix_29 : HEALPix order 29 pixel index.\nOrder 29 pixels have a resolution of ~4e-4 arcseconds.\n\nThe HEALPix, Euclid object ID, and Euclid tile ID columns appear first:\n\nschema.names[:5]\n\n\n\nHATS columns:\n\nThese are the HATS partitioning columns.\nThey appear in the Parquet file names but are not included inside the files.\nHowever, PyArrow automatically makes them available as regular columns when the dataset is loaded as demonstrated in these tutorials.\n\nNorder : (hats column) HEALPix order at which the data is partitioned.\n\nNpix : (hats column) HEALPix pixel index at order Norder.\n\nDir : (hats column) Integer equal to 10_000 * floor[Npix / 10_000].\n\nThe HATS columns appear at the end:\n\nschema.names[-3:]\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-4-4-additional-columns","position":19},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.5 Find columns of interest","lvl2":"4. Merged Objects Catalog Contents"},"type":"lvl3","url":"/euclid-q1-hats-intro#id-4-5-find-columns-of-interest","position":20},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl3":"4.5 Find columns of interest","lvl2":"4. Merged Objects Catalog Contents"},"content":"The subsections above show how to find all columns from a given Euclid table as well as the additional columns.\nHere we show some additional techniques for finding columns.\n\n# Access the data type using the `field` method.\nschema.field(\"mer_flux_y_2fwhm_aper\")\n\n\n\n\n\n# The column metadata includes unit and description.\n# Parquet metadata is always stored as bytestrings, which are denoted by a leading 'b'.\nschema.field(\"mer_flux_y_2fwhm_aper\").metadata\n\n\n\nEuclid Q1 offers many flux measurements, both from Euclid detections and from external ground-based surveys.\nThey are given in microjanskys, so all flux columns can be found by searching the metadata for this unit.\n\n# Find all flux columns.\nflux_columns = [field.name for field in schema if field.metadata[b\"unit\"] == b\"uJy\"]\n\nprint(f\"{len(flux_columns)} flux columns. First four are:\")\nflux_columns[:4]\n\n\n\n\n\nColumns associated with external surveys are identified by the inclusion of “ext” in the name.\n\nexternal_flux_columns = [name for name in flux_columns if \"ext\" in name]\nprint(f\"{len(external_flux_columns)} flux columns from external surveys. First four are:\")\nexternal_flux_columns[:4]\n\n\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-4-5-find-columns-of-interest","position":21},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"5. Euclid Deep Fields"},"type":"lvl2","url":"/euclid-q1-hats-intro#id-5-euclid-deep-fields","position":22},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"5. Euclid Deep Fields"},"content":"\n\nEuclid Q1 includes data from three Euclid Deep Fields: EDF-N (North), EDF-S (South), EDF-F (Fornax; also in the southern hemisphere).\nThere is also a small amount of data from a fourth field: LDN1641 (Lynds’ Dark Nebula 1641), which was observed for technical reasons during Euclid’s verification phase.\nThe fields are described in \n\nEuclid Collaboration: Aussel et al., 2025 and can be seen on this \n\nskymap.\n\nThe regions are well separated, so we can distinguish them using a simple cone search without having to be too picky about the radius.\nWe can load data more efficiently using the HEALPix order 9 pixels that cover each area rather than using RA and Dec values directly.\nThese will be used in later tutorials.\n\n# EDF-N (Euclid Deep Field - North)\nra, dec, radius = 269.733, 66.018, 4  # 20 sq deg\nedfn_k9_pixels = hpgeom.query_circle(hpgeom.order_to_nside(9), ra, dec, radius, inclusive=True)\n\n# EDF-S (Euclid Deep Field - South)\nra, dec, radius = 61.241, -48.423, 5  # 23 sq deg\nedfs_k9_pixels = hpgeom.query_circle(hpgeom.order_to_nside(9), ra, dec, radius, inclusive=True)\n\n# EDF-F (Euclid Deep Field - Fornax)\nra, dec, radius = 52.932, -28.088, 3  # 10 sq deg\nedff_k9_pixels = hpgeom.query_circle(hpgeom.order_to_nside(9), ra, dec, radius, inclusive=True)\n\n# LDN1641 (Lynds' Dark Nebula 1641)\nra, dec, radius = 85.74, -8.39, 1.5  # 6 sq deg\nldn_k9_pixels = hpgeom.query_circle(hpgeom.order_to_nside(9), ra, dec, radius, inclusive=True)\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-5-euclid-deep-fields","position":23},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"6. Basic Query"},"type":"lvl2","url":"/euclid-q1-hats-intro#id-6-basic-query","position":24},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"6. Basic Query"},"content":"To demonstrate a basic query, we’ll search for objects with a galaxy photometric redshift estimate of 6.0 (largest possible).\nOther tutorials in this series will show more complex queries, and describe the redshifts and other data in more detail.\nPyArrow dataset filters are described at \n\nFiltering by Expressions, and the list of available functions is at \n\nCompute Functions.\n\ndataset = pyarrow.dataset.dataset(dataset_path, partitioning=\"hive\", filesystem=s3, schema=schema)\n\nhighz_objects = dataset.to_table(\n    columns=[\"object_id\", \"phz_phz_median\"], filter=pc.field(\"phz_phz_median\") == 6\n).to_pandas()\nhighz_objects\n\n\n\n","type":"content","url":"/euclid-q1-hats-intro#id-6-basic-query","position":25},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"About this notebook"},"type":"lvl2","url":"/euclid-q1-hats-intro#about-this-notebook","position":26},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Introduction","lvl2":"About this notebook"},"content":"Authors: Troy Raen, Vandana Desai, Andreas Faisst, Shoubaneh Hemmati, Jaladh Singhal, Brigitta Sipőcz, Jessica Krick, the IRSA Data Science Team, and the Euclid NASA Science Center at IPAC (ENSCI).\n\nUpdated: 2025-12-23\n\nContact: \n\nIRSA Helpdesk","type":"content","url":"/euclid-q1-hats-intro#about-this-notebook","position":27},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes"},"type":"lvl1","url":"/euclid-q1-hats-magnitudes","position":0},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes"},"content":"This tutorial explores Euclid photometry measurements.\nIt assumes you are familiar with the \n\nfirst tutorial in this series, which covers the Euclid Q1 Merged Objects HATS Catalog content, format, and basic access.\n\n","type":"content","url":"/euclid-q1-hats-magnitudes","position":1},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"Learning Goals"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#learning-goals","position":2},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\nUnderstand the different Euclid Q1 flux measurements and their intended use cases.\n\nLoad aperture and template-fit magnitudes for Euclid I, Y, J, and H bands from the Euclid Q1 Merged Objects HATS Catalog.\n\nVisualize and understand the template-fit magnitude distributions as a function of object classification.\n\nCompare aperture and template-fit magnitudes to understand their differences.\n\n","type":"content","url":"/euclid-q1-hats-magnitudes#learning-goals","position":3},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"1. Introduction"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#id-1-introduction","position":4},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"1. Introduction"},"content":"The \n\nEuclid Q1 data release contains photometry from Euclid as well as from external surveys.\nThere are several flux measurements per band.\nThe measurements are described in \n\nEuclid Collaboration: Romelli et al., 2025 (hereafter, Romelli), especially sections 6 and 8.\n\nIn this tutorial, we will look at aperture and template-fit photometry measurements in the four Euclid bands: I (from the VIS instrument), Y, J, and H (from the NISP instrument).\nAperture fluxes are generally more accurate for point-like sources, especially bright stars in the NIR bands, likely due to better handling of PSF-related effects.\nTemplate-fit fluxes are expected to be more accurate for extended sources because the templates do a better job of excluding contamination from nearby sources.\nAdditional photometry measurements that we won’t cover here include: Sérsic-fit fluxes (computed for parametric morphology), PSF-fit fluxes (VIS only), and class-corrected fluxes that were corrected based on the PHZ (photometric) classifications.\n\nThe best-estimate flux in the detection band is given by the column mer_flux_detection_total.\nThis can also be used to color-correct flux measurements in non-detection bands, as we will demonstrate.\nThe object detection process is described in Romelli.\nThe \n\nMER Photometry Cookbook describes the color corrections and how to convert from flux to magnitude.\nIn this tutorial, we will restrict to objects detected in VIS (I band) because it simplifies the calculations.\n\n","type":"content","url":"/euclid-q1-hats-magnitudes#id-1-introduction","position":5},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"2. Imports and Paths"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#id-2-imports-and-paths","position":6},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"2. Imports and Paths"},"content":"\n\n# # Uncomment the next line to install dependencies if needed.\n# %pip install hpgeom matplotlib pandas pyarrow\n\n\n\n\n\nimport hpgeom  # Find HEALPix indexes from RA and Dec\nimport matplotlib.pyplot as plt  # Create figures\nimport pyarrow.compute as pc  # Filter dataset\nimport pyarrow.dataset  # Load the dataset\nimport pyarrow.fs  # Simple S3 filesystem pointer\nimport pyarrow.parquet  # Load the schema\n\n# Increase font size in figures.\nplt.rcParams.update({\"font.size\": 14})\n\n\n\n\n\n# AWS S3 paths.\ns3_bucket = \"nasa-irsa-euclid-q1\"\ndataset_prefix = \"contributed/q1/merged_objects/hats/euclid_q1_merged_objects-hats/dataset\"\ndataset_path = f\"{s3_bucket}/{dataset_prefix}\"\n\n# S3 pointer. Use `anonymous=True` to access without credentials.\ns3 = pyarrow.fs.S3FileSystem(anonymous=True)\n\n\n\n","type":"content","url":"/euclid-q1-hats-magnitudes#id-2-imports-and-paths","position":7},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"3. Load Template-fit and Aperture Magnitudes"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#id-3-load-template-fit-and-aperture-magnitudes","position":8},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"3. Load Template-fit and Aperture Magnitudes"},"content":"The following columns will be important.\nDescriptions come from Romelli.\n\n# Whether the source was detected in VIS mosaic (1) or only in NIR-stack mosaic (0).\nVIS_DET = \"mer_vis_det\"\n\n# Best estimate of the total flux in the detection band.\n# From aperture photometry within a Kron radius.\n# Detection band is VIS if `VIS_DET == 1`. Otherwise, this is a\n# non-physical NIR-stack flux and there was no VIS detection (aka, NIR-only).\n# We will only deal with VIS-detected objects in this notebook.\nFLUX_DET_TOTAL = \"mer_flux_detection_total\"\n\n# Peak surface brightness minus the magnitude used for `mer_point_like_prob`.\n# This is a measure of compactness.\nMUMAX_MINUS_MAG = \"mer_mumax_minus_mag\"\n\n# Whether the detection has a >50% probability of being spurious (1=Yes, 0=No).\nSPURIOUS_FLAG = \"mer_spurious_flag\"\n\n# PHZ classification: 1=Star, 2=Galaxy, 4=QSO.\n# Combinations (3, 5, 6, and 7) indicate multiple probability thresholds were exceeded.\nPHZ_CLASS = \"phz_phz_classification\"\n\n\n\nWe’ll convert the catalog fluxes to magnitudes following the \n\nMER Photometry Cookbook.\nPyArrow can do the conversion during the read operation and return only the magnitudes.\nTo do this, we’ll use the following function to define the magnitudes as pyarrow.compute (pc) functions, which are described at \n\nCompute Functions.\n\ndef flux_to_magnitude(flux_col_name: str) -> pc.Expression:\n    \"\"\"Convert catalog fluxes to magnitudes following the MER Photometry Cookbook.\n\n    Parameters\n    ----------\n    flux_col_name : str\n        The name of the flux column to convert to magnitude.\n\n    Returns\n    -------\n    pyarrow.compute.Expression\n        An expression for the magnitude. It can be used in the `filter` and `columns`\n        keyword arguments when loading data from a PyArrow dataset.\n    \"\"\"\n    # We expect to be dealing with VIS_DET == 1 objects, so FLUX_DET_TOTAL == VIS flux.\n    vis_flux = pc.field(FLUX_DET_TOTAL)\n    band_flux = pc.field(flux_col_name)\n\n    if flux_col_name == FLUX_DET_TOTAL:\n        # Best-estimate flux in VIS is FLUX_DET_TOTAL.\n        best_flux = vis_flux\n    elif flux_col_name.endswith(\"_templfit\"):\n        # Best-estimate template-fit flux is the band flux scaled by a color correction.\n        band = flux_col_name.split(\"_\")[-2]  # y, j, or h\n        color_scale = pc.divide(vis_flux, pc.field(f\"mer_flux_vis_to_{band}_templfit\"))\n        best_flux = pc.multiply(band_flux, color_scale)\n    elif flux_col_name.endswith(\"aper\"):\n        # Best-estimate aperture flux is the band flux scaled by a color correction.\n        nfwhm = flux_col_name.split(\"_\")[-2]  # e.g., 2fwhm\n        color_scale = pc.divide(vis_flux, pc.field(f\"mer_flux_vis_{nfwhm}_aper\"))\n        best_flux = pc.multiply(band_flux, color_scale)\n\n    # magnitude = -2.5 * log10(flux) + 23.9.\n    scale = pc.scalar(-2.5)\n    log10_flux = pc.log10(best_flux)\n    zeropoint = pc.scalar(23.9)\n    mag_expression = pc.add(pc.multiply(scale, log10_flux), zeropoint)\n    return mag_expression\n\n\n\nDefine the columns we want to load.\nThis needs to be a dictionary (rather than a simple list of column names) because we’re asking PyArrow to compute the magnitudes dynamically from the catalog fluxes.\nThe dictionary keys will be the column names in the resultant table.\nThe values must be pyarrow.compute expressions (described above).\n\nI_MAG = \"I (mag)\"\ncolumns = {\n    PHZ_CLASS: pc.field(PHZ_CLASS),\n    I_MAG: flux_to_magnitude(FLUX_DET_TOTAL),\n    \"Y aperture (mag)\": flux_to_magnitude(\"mer_flux_y_2fwhm_aper\"),\n    \"J aperture (mag)\": flux_to_magnitude(\"mer_flux_j_2fwhm_aper\"),\n    \"H aperture (mag)\": flux_to_magnitude(\"mer_flux_h_2fwhm_aper\"),\n    \"Y templfit (mag)\": flux_to_magnitude(\"mer_flux_y_templfit\"),\n    \"J templfit (mag)\": flux_to_magnitude(\"mer_flux_j_templfit\"),\n    \"H templfit (mag)\": flux_to_magnitude(\"mer_flux_h_templfit\"),\n    MUMAX_MINUS_MAG: pc.field(MUMAX_MINUS_MAG),\n}\n# Let's see what one of these looks like.\ncolumns[\"Y aperture (mag)\"]\n\n\n\nWe’ll restrict to the Euclid Deep Field - Fornax (EDF-F) to reduce the amount of data loaded.\nCompute the HEALPix order 9 pixel indexes, following the \n\nintroductory tutorial.\n\nra, dec, radius = 52.932, -28.088, 3  # 10 sq deg\nedff_k9_pixels = hpgeom.query_circle(hpgeom.order_to_nside(9), ra, dec, radius, inclusive=True)\n\n\n\nConstruct the row filter.\n\nrow_filter = (\n    # Stars, Galaxies, QSOs, and mixed classes.\n    pc.field(PHZ_CLASS).isin([1, 2, 3, 4, 5, 6, 7])\n    # Basic quality cut.\n    & (pc.field(SPURIOUS_FLAG) == 0)\n    # VIS-detected objects. (If you want to include NIR-only objects, alter flux_to_magnitude()\n    # following MER Photometry Cookbook and also comment out the next line.)\n    & (pc.field(VIS_DET) == 1)\n    # EDF-F region. (Comment out the next line to do an all-sky search.)\n    & pc.field(\"_healpix_9\").isin(edff_k9_pixels)\n)\n\n\n\nLoad the data.\n\n# Load the catalog as a PyArrow dataset. Include partitioning=\"hive\"\n# so PyArrow understands the file naming scheme and can navigate the partitions.\nschema = pyarrow.parquet.read_schema(f\"{dataset_path}/_common_metadata\", filesystem=s3)\ndataset = pyarrow.dataset.dataset(dataset_path, partitioning=\"hive\", filesystem=s3, schema=schema)\n\nmags_df = dataset.to_table(columns=columns, filter=row_filter).to_pandas()\nmags_df\n\n\n\n","type":"content","url":"/euclid-q1-hats-magnitudes#id-3-load-template-fit-and-aperture-magnitudes","position":9},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"4. Magnitude Distributions of Galaxies, Stars, and QSOs"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#id-4-magnitude-distributions-of-galaxies-stars-and-qsos","position":10},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"4. Magnitude Distributions of Galaxies, Stars, and QSOs"},"content":"\n\nLet’s visualize the template-fit magnitude distributions as a function of PHZ classification.\nSince the template-fit photometry is recommended for extended objects, we’ll separate the point-like objects.\n\n\nEuclid Collaboration: Tucci et al., 2025 defines point-like objects as having MUMAX_MINUS_MAG < -2.5.\n\n# Galaxy + any. Star + galaxy. QSO + galaxy.\nclasses = {\"Galaxy\": (2, 3, 6, 7), \"Star\": (1, 3), \"QSO\": (4, 6)}\nclass_colors = [\"tab:green\", \"tab:blue\", \"tab:orange\"]\n\nbands = [I_MAG, \"Y templfit (mag)\", \"J templfit (mag)\", \"H templfit (mag)\"]\nmag_limits = (14, 28)  # Excluding all magnitudes outside this range.\nhist_kwargs = dict(bins=20, range=mag_limits, histtype=\"step\")\n\nfig, axes = plt.subplots(3, 4, figsize=(18, 12), sharey=\"row\", sharex=True)\nfor (class_name, class_ids), class_color in zip(classes.items(), class_colors):\n    hist_kwargs[\"color\"] = class_color\n\n    # Get the objects that are in this class only.\n    class_df = mags_df.loc[mags_df[PHZ_CLASS] == class_ids[0]]\n    # Plot histograms for each band. Galaxies on top row, then stars, then QSOs.\n    axs = axes[0] if class_name == \"Galaxy\" else (axes[1] if class_name == \"Star\" else axes[2])\n    for ax, band in zip(axs, bands):\n        ax.hist(class_df[band], label=class_name, **hist_kwargs)\n\n    # Get the objects that were accepted as multiple classes.\n    class_df = mags_df.loc[mags_df[PHZ_CLASS].isin(class_ids)]\n    label = \"+Galaxy\" if class_name != \"Galaxy\" else \"+any\"\n    # Of those objects, restrict to the ones that are point-like.\n    classpt_df = class_df.loc[class_df[MUMAX_MINUS_MAG] < -2.5]\n    pt_label = f\"{label} (point-like)\"\n    # Plot histograms for both sets of objects.\n    for ax, band in zip(axs, bands):\n        ax.hist(class_df[band], label=label, linestyle=\":\", **hist_kwargs)\n        ax.hist(classpt_df[band], linestyle=\"-.\", label=pt_label, **hist_kwargs)\n\n# Add axis labels, etc.\nfor ax, loc in zip(axes[:, 0], [2, 3, 2]):\n    ax.legend(loc=loc)\n    ax.set_ylabel(\"Counts\")\nfor axs, band in zip(axes.transpose(), bands):\n    axs[0].set_title(band.split()[0])\n    axs[-1].set_xlabel(band)\nfig.suptitle(\"Magnitude Distributions by Object Type\")\nplt.tight_layout()\n\n\n\nThe Euclid instruments are tuned to detect galaxies for cosmology studies, so it’s no surprise that there are many more galaxies than other object types.\n\nThe green lines (top row) show the magnitude distributions of objects classified as galaxy only (solid) and those classified as galaxy plus possibly other types (dot and dash-dot).\nThe dash-dot line highlights the population of point-like “galaxies”, which are likely misclassified stars or QSOs and mostly appear at faint magnitudes.\n\nThe star distributions (middle row, blue) are broader and peak at brighter magnitudes than the galaxy distributions, as expected.\nAdding objects classified as both star and galaxy (dotted line) adds significant numbers, especially near the peak and toward the faint end where confusion is more likely.\nRestricting these to point-like objects (dash-dot line) shows that many bright objects surpassing both probability thresholds are likely to be stars, not galaxies.\nHowever, this doesn’t hold at the faint end where even some star-only classified objects fail the point-like cut.\n\nThe bottom row (orange) is the same as the middle row but for QSOs instead of stars.\nThere are very few point-like QSOs, reminding us that most QSO classifications in Q1 should be treated with skepticism (as discussed in the Classifications tutorial).\nBy default, this figure only includes objects in the EDF-F region.\nHigh-confidence QSOs are more concentrated in the EDF-N region where advantageous external photometry (particularly u-band from UNIONS) was available.\n\n","type":"content","url":"/euclid-q1-hats-magnitudes#id-4-magnitude-distributions-of-galaxies-stars-and-qsos","position":11},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"5. Template-fit vs. Aperture Magnitudes"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#id-5-template-fit-vs-aperture-magnitudes","position":12},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"5. Template-fit vs. Aperture Magnitudes"},"content":"\n\nNow let’s compare template-fit and aperture magnitudes by plotting their differences.\nThis comparison reveals systematic offsets that depend on factors including morphology (extended vs. point-like) and brightness.\n\nThis figure is inspired by Romelli Fig. 6 (top panel).\n\n# Only consider objects within these mag and mag difference limits.\nmag_limits, mag_diff_limits = (16, 24), (-1, 1)\nmag_limited_df = mags_df.loc[(mags_df[I_MAG] > mag_limits[0]) & (mags_df[I_MAG] < mag_limits[1])]\n\nbands = [\n    (\"Y templfit (mag)\", \"Y aperture (mag)\"),\n    (\"J templfit (mag)\", \"J aperture (mag)\"),\n    (\"H templfit (mag)\", \"H aperture (mag)\"),\n]\nhexbin_kwargs = dict(\n    cmap=\"YlGnBu\", bins=\"log\", extent=(*mag_limits, *mag_diff_limits), gridsize=25\n)\nannotate_kwargs = dict(\n    xycoords=\"axes fraction\", ha=\"left\", fontweight=\"bold\", bbox=dict(facecolor=\"white\", alpha=0.8)\n)\n\n# Plot\nfig, axes = plt.subplots(2, 3, figsize=(18, 9), sharey=True, sharex=True)\nfor axs, (ref_band, aper_band) in zip(axes.transpose(), bands):\n    # Extended objects, top row.\n    ax = axs[0]\n    extended = mags_df.loc[mags_df[MUMAX_MINUS_MAG] >= -2.5, [I_MAG, ref_band, aper_band]]\n    extended[\"mag_diff\"] = extended[ref_band] - extended[aper_band]\n    extended = extended.dropna(subset=\"mag_diff\")\n    cb = ax.hexbin(extended[I_MAG], extended[\"mag_diff\"], **hexbin_kwargs)\n    plt.colorbar(cb)\n    ax.set_ylabel(f\"{ref_band} - {aper_band}\")\n    # Annotate top (bottom) with the fraction of objects having a magnitude difference greater (less) than 0.\n    frac_tmpl_greater = len(extended.loc[extended[\"mag_diff\"] > 0]) / len(extended)\n    ax.annotate(f\"{frac_tmpl_greater:.3f}\", xy=(0.01, 0.99), va=\"top\", **annotate_kwargs)\n    frac_tmpl_less = len(extended.loc[extended[\"mag_diff\"] < 0]) / len(extended)\n    ax.annotate(f\"{frac_tmpl_less:.3f}\", xy=(0.01, 0.01), va=\"bottom\", **annotate_kwargs)\n\n    # Point-like objects, bottom row.\n    ax = axs[1]\n    pointlike = mags_df.loc[mags_df[MUMAX_MINUS_MAG] < -2.5, [I_MAG, ref_band, aper_band]]\n    pointlike[\"mag_diff\"] = pointlike[ref_band] - pointlike[aper_band]\n    pointlike = pointlike.dropna(subset=\"mag_diff\")\n    cb = ax.hexbin(pointlike[I_MAG], pointlike[\"mag_diff\"], **hexbin_kwargs)\n    plt.colorbar(cb)\n    ax.set_ylabel(f\"{ref_band} - {aper_band}\")\n    # Annotate top (bottom) with the fraction of objects having a magnitude difference greater (less) than 0.\n    frac_tmpl_greater = len(pointlike.loc[pointlike[\"mag_diff\"] > 0]) / len(pointlike)\n    ax.annotate(f\"{frac_tmpl_greater:.3f}\", xy=(0.01, 0.99), va=\"top\", **annotate_kwargs)\n    frac_tmpl_less = len(pointlike.loc[pointlike[\"mag_diff\"] < 0]) / len(pointlike)\n    ax.annotate(f\"{frac_tmpl_less:.3f}\", xy=(0.01, 0.01), va=\"bottom\", **annotate_kwargs)\n\n# Add axis labels, etc.\nfor i, ax in enumerate(axes.flatten()):\n    ax.axhline(0, color=\"gray\", linewidth=1)\n    if i == 1:\n        ax.set_title(\"Extended objects\")\n    if i == 4:\n        ax.set_title(\"Point-like objects\")\n    if i > 2:\n        ax.set_xlabel(I_MAG)\nfig.suptitle(\"Magnitude Offsets (Template fit - Aperture)\")\nplt.tight_layout()\n\n\n\nThe panel annotations give the fraction of objects with magnitude differences that are positive (top number) and negative (bottom number).\nThe magnitude difference is fairly tightly clustered around 0 for extended objects (top row), but with asymmetric outliers.\nThere is a positive offset, indicating fainter template-fit magnitudes, as expected: templates better exclude contaminating light from nearby sources.\nThe offset is more pronounced for point-like objects (bottom row), likely due to the PSF handling mentioned above, and we are reminded that aperture magnitudes are more reliable here.\n\n","type":"content","url":"/euclid-q1-hats-magnitudes#id-5-template-fit-vs-aperture-magnitudes","position":13},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"About this notebook"},"type":"lvl2","url":"/euclid-q1-hats-magnitudes#about-this-notebook","position":14},{"hierarchy":{"lvl1":"Euclid Q1 Merged Objects HATS Catalog: Magnitudes","lvl2":"About this notebook"},"content":"Authors: Troy Raen, Vandana Desai, Andreas Faisst, Shoubaneh Hemmati, Jaladh Singhal, Brigitta Sipőcz, Jessica Krick, the IRSA Data Science Team, and the Euclid NASA Science Center at IPAC (ENSCI).\n\nUpdated: 2025-12-23\n\nContact: \n\nIRSA Helpdesk","type":"content","url":"/euclid-q1-hats-magnitudes#about-this-notebook","position":15},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images"},"type":"lvl1","url":"/openuniverse2024preview-firefly","position":0},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images"},"content":"","type":"content","url":"/openuniverse2024preview-firefly","position":1},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Learning Goals"},"type":"lvl2","url":"/openuniverse2024preview-firefly#learning-goals","position":2},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nLearn how to access cloud-hosted Roman and Rubin simulated images.\n\nLearn how to launch an interactive Firefly instance inside JupyterLab.\n\nLearn how to use the Firefly Jupyterlab extension to visualize cloud-hosted simulated images, overplot ds9 regions, overplot catalogs in Parquet format, and create 3 color images.\n\n","type":"content","url":"/openuniverse2024preview-firefly#learning-goals","position":3},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Introduction"},"type":"lvl2","url":"/openuniverse2024preview-firefly#introduction","position":4},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Introduction"},"content":"The purpose of this tutorial is to become familiar with simulated Roman and Rubin simulated data published through the OpenUniverse 2024 data preview, and to become familiar with the Firefly JupyterLab Extension for visualizing astronomical data products.\n\nOpenUniverse2024 is a project to simulate spatially overlapping imaging surveys to be carried out by the Nancy Grace Roman Telescope and the Vera C. Rubin Observatory. The simulations were carried out on Argonne’s Theta cluster and consist of:\n\nThe LSST ELAIS-S1 Deep Drilling Field (DDF)\n\nThe Roman Time-Domain Survey (TDS) shifted to overlap the ELAIS region and LSST DDF\n\nOverlapping LSST Wide-Fast-Deep (WFD) survey (with rolling cadence)\n\nOverlapping Roman Wide-Area Survey (WAS) in the same region\n\nA deep-field calibration region of the Roman WAS in the same region\n\nThis data preview release consists of a subset of data from each of the five categories above. More information about data preview can be found at \n\nIRSA holding of this dataset.\n\nFirefly is an open-source web-based UI library for astronomical data archive access and visualization developed at Caltech and used by multiple space- and ground-based astrophysics archives. More information on Firefly can be found \n\nhere.\n\nIn addition to being used to make web applications, Firefly can be used from Python. More information on Firefly Python client can be found \n\nhere.\n\nThe Firefly JupyterLab Extension makes it particularly easy to use Firefly to efficiently visualize cloud-hosted astronomical data using JupyterLab instances running locally or on cloud. More information on Firefly JupyterLab Extension can be found \n\nhere.\n\n","type":"content","url":"/openuniverse2024preview-firefly#introduction","position":5},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Imports"},"type":"lvl2","url":"/openuniverse2024preview-firefly#imports","position":6},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Imports"},"content":"astropy.io.fits for accessing FITS files\n\nnumpy for numerical computing\n\ns3fs for browsing cloud buckets\n\nmatplotlib.pyplot for creating static visualizations of FITS images\n\nmatplotlib.patches for annotating visualizations of FITS images\n\nastropy.wcs for dealing with astronomical world coordinate systems\n\nastropy.units for dealing with astronomical units\n\nastropy.coordinates.SkyCoord for dealing with astronomical coordinates\n\nfirefly_client.FireflyClient for using the Firefly python client\n\nastropy.nddata.Cutout2D for making image cutouts\n\nitertools.product to support looping over Roman blocks\n\nreproject.reproject_interp to convert Roman coadds from STG to TAN projection\n\nio.BytesIO for writing a fits file to an in-memory stream\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install numpy astropy s3fs matplotlib firefly_client reproject\n\n\n\n\n\nfrom astropy.io import fits\nimport numpy as np\nimport s3fs\nfrom matplotlib import pyplot as plt\nfrom matplotlib import patches\nfrom astropy import wcs\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom firefly_client import FireflyClient\nfrom astropy.nddata import Cutout2D\nfrom itertools import product\nfrom reproject import reproject_interp\nfrom io import BytesIO\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#imports","position":7},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Learn where the OpenUniverse2024 data are hosted in the cloud."},"type":"lvl2","url":"/openuniverse2024preview-firefly#learn-where-the-openuniverse2024-data-are-hosted-in-the-cloud","position":8},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Learn where the OpenUniverse2024 data are hosted in the cloud."},"content":"The OpenUniverse2024 data preview is hosted in the cloud via Amazon Web Services (AWS). To access these data, you need to create a client to read data from Amazon’s Simple Storage Service (s3) buckets, and you need to know some information about those buckets. The OpenUniverse2024 data preview contains simulations of the Roman Wide-Area Survey (WAS) and the Roman Time Domain Survey (TDS). In this tutorial, we will focus on the WAS.\n\nBUCKET_NAME = \"nasa-irsa-simulations\"\nROMAN_PREFIX = \"openuniverse2024/roman/preview\"\nROMAN_COADD_PATH = f\"{ROMAN_PREFIX}/RomanWAS/images/coadds\"\nTRUTH_FILES_PATH = f\"{ROMAN_PREFIX}/roman_rubin_cats_v1.1.2_faint\"\n\nRUBIN_PREFIX = \"openuniverse2024/rubin/preview\"\nRUBIN_COADD_PATH = f\"{RUBIN_PREFIX}/u/descdm/preview_data_step3_2877_19_w_2024_12/20240403T150003Z/deepCoadd_calexp/2877/19\"\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#learn-where-the-openuniverse2024-data-are-hosted-in-the-cloud","position":9},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Roman Coadds"},"type":"lvl2","url":"/openuniverse2024preview-firefly#roman-coadds","position":10},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Roman Coadds"},"content":"The Nancy Grace Roman Space Telescope will carry out a wide-area survey (WAS) in the near infrared. The OpenUniverse2024 data preview includes coadded mosaics of simulated WAS data, created with the IMCOM algorithm (Rowe et al. 2011). Bands include F184, H158, J129, K213, Y106. In this section, we define some functions that make it convenient to retrieve a given cloud-hosted simulated Roman coadd based on position and filter.\n\n","type":"content","url":"/openuniverse2024preview-firefly#roman-coadds","position":11},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define Roman Simulated “Blocks”","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#define-roman-simulated-blocks","position":12},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define Roman Simulated “Blocks”","lvl2":"Roman Coadds"},"content":"The simulated Roman coadds are arranged in 100 arcsecond blocks, as described in Hirata et al. 2024. Below we define the RA and Dec of the block centers. This cell should not be altered.\n\n#Centers of roman data preview blocks. Do not alter.\nra_block_centers = np.array([9.76330352298415, 9.724522605135252, 9.68574158906671,\n                        9.646960496603766, 9.608179349571955, 9.56939816979703,\n                        9.530616979104877, 9.491835799321422, 9.453054652272561,\n                        9.414273559784032, 9.375492543681393, 9.336711625789874]) * u.deg\ndec_block_centers = np.array([-44.252584927082495, -44.22480733304182, -44.197029724175756,\n                            -44.16925210374898, -44.14147447502621, -44.11369684127218,\n                            -44.08591920575162, -44.05814157172923, -44.03036394246976,\n                            -44.0025863212379, -43.974808711298394, -43.94703111591591]) * u.deg\nblock_size = 100 * u.arcsec # each block is 100 arcsec across\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#define-roman-simulated-blocks","position":13},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define a function that returns the simulated Roman block column/row for a given RA/Dec.","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#define-a-function-that-returns-the-simulated-roman-block-column-row-for-a-given-ra-dec","position":14},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define a function that returns the simulated Roman block column/row for a given RA/Dec.","lvl2":"Roman Coadds"},"content":"Blocks are arranged in a grid with axes parallel to RA and Dec. A given RA represents a column of blocks and a given Dec represents a row of blocks. This function returns the block column or row given the block centers from the cell above and a given RA or Dec, respectively.\n\ndef get_block_axis(block_centers, coord, ra_or_dec):\n    ra_or_dec_coord = getattr(coord, ra_or_dec)\n    block_dist_array = np.absolute(block_centers - ra_or_dec_coord)\n    closest_block_idx = block_dist_array.argmin()\n    if (ra_or_dec_coord < block_centers.min()-block_size/2 \n        or ra_or_dec_coord > block_centers.max()+block_size/2):\n        raise ValueError(f\"Chosen {ra_or_dec}: {ra_or_dec_coord} not covered by OpenUniverse 2024 data preview simulated Roman coadds\")\n    else:\n        return closest_block_idx + 12 # preview covers central 12 rows 12 columns, in a grid of 36x36 blocks\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#define-a-function-that-returns-the-simulated-roman-block-column-row-for-a-given-ra-dec","position":15},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define a function that retrieves a Roman simulated coadd given a sky position and filter.","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#define-a-function-that-retrieves-a-roman-simulated-coadd-given-a-sky-position-and-filter","position":16},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define a function that retrieves a Roman simulated coadd given a sky position and filter.","lvl2":"Roman Coadds"},"content":"\n\nEach of the cloud-hosted simulated Roman coadds can be accessed via a S3 filepath. This function returns the access path for the simulated Roman coadd that includes a specified position on the sky and desired filter.\n\ndef get_roman_coadd_fpath(coord, filter):\n    col = get_block_axis(ra_block_centers, coord, 'ra')\n    row = get_block_axis(dec_block_centers, coord, 'dec')\n    \n    # Construct the coadd filename from the chosen filter, row, and column.\n    coadd_fname_root = f\"prod_{filter[0]}_{col}_{row}_map.fits\"\n    coadd_fpath = f\"{BUCKET_NAME}/{ROMAN_COADD_PATH}/{filter}/Row{row}/{coadd_fname_root}\"\n    return coadd_fpath\n\n\n\nNow we use this access path and prefix it with s3:// and use astropy.fits to extract a subset of it (for more info see \n\nthis section of astropy docs).\n\nWe use .section to retrieve just the science image data from fits HDU as a 2D numpy.array, and extract WCS information from fits header as astropy.wcs.WCS object. The following function returns a dictionary of both.\n\ndef get_roman_coadd(coord, filter):\n    # retrive fits file of block/tile from the coadd mosiac\n    coadd_s3_fpath = get_roman_coadd_fpath(coord, filter)\n    coadd_s3_uri = f\"s3://{coadd_s3_fpath}\"\n\n    with fits.open(coadd_s3_uri, fsspec_kwargs={\"anon\": True}) as hdul:\n        # retrieve science data from coadd fits\n        coadd_data = hdul[0].section[0,0, :, :]  # has (2688, 2688, 15, 1) shape, with 0th layer in the cube as science image\n\n        # make wcs using header\n        coadd_wcs = wcs.WCS(hdul[0].header, naxis=2)\n\n        return {'data': coadd_data, 'wcs': coadd_wcs}\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#define-a-function-that-retrieves-a-roman-simulated-coadd-given-a-sky-position-and-filter","position":17},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Inspect a simulated Roman Coadd","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#inspect-a-simulated-roman-coadd","position":18},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Inspect a simulated Roman Coadd","lvl2":"Roman Coadds"},"content":"\n\nChoose a filter and position that lies within the data preview region\n\ncoord = SkyCoord(ra=9.6055383, dec=-44.1895542, unit=\"deg\")\nfilter_roman = 'H158' #F184, H158, J129, K213, and Y106 are available in the data preview\n\n\n\nRetrieve the data and header information from the simulated Roman coadd corresponding to the chosen position and filter.\n\ncoadd_roman = get_roman_coadd(coord, filter_roman)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#inspect-a-simulated-roman-coadd","position":19},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Understand the size of a simulated Roman coadd.","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#understand-the-size-of-a-simulated-roman-coadd","position":20},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Understand the size of a simulated Roman coadd.","lvl2":"Roman Coadds"},"content":"\n\n# Number of pixels (Y, X)\ncoadd_roman['data'].shape\n\n\n\n\n\n# Pixel size (scale Y, scale X) [degrees/pixel]\ncoadd_roman['wcs'].proj_plane_pixel_scales()\n\n\n\n\n\n# Coadd size (FOV Y, FOV X)\n[(num * size).to('arcsec') for num, size in zip(\n    coadd_roman['data'].shape, coadd_roman['wcs'].proj_plane_pixel_scales())]\n\n\n\nThe field of view of Roman coadd is ~100 arcsec.\n\n","type":"content","url":"/openuniverse2024preview-firefly#understand-the-size-of-a-simulated-roman-coadd","position":21},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use the WCS from the Roman simulated header to convert the specified coordinate into a pixel position.","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-the-wcs-from-the-roman-simulated-header-to-convert-the-specified-coordinate-into-a-pixel-position","position":22},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use the WCS from the Roman simulated header to convert the specified coordinate into a pixel position.","lvl2":"Roman Coadds"},"content":"\n\ndef coord_to_xy(w, coord):\n    return w.world_to_array_index(coord)[::-1] #reverse since 0th axis is y, 1st axis is x\n\ncoord_arr_idx = coord_to_xy(coadd_roman['wcs'], coord)\ncoord_arr_idx\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-the-wcs-from-the-roman-simulated-header-to-convert-the-specified-coordinate-into-a-pixel-position","position":23},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use matplotlib imshow to create a static visualization of the Roman simulated coadd and overplot the selected position.","lvl2":"Roman Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-matplotlib-imshow-to-create-a-static-visualization-of-the-roman-simulated-coadd-and-overplot-the-selected-position","position":24},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use matplotlib imshow to create a static visualization of the Roman simulated coadd and overplot the selected position.","lvl2":"Roman Coadds"},"content":"\n\ndef stretch_color(data, clipPercent):\n    return np.percentile(data, (0 + clipPercent, 100 - clipPercent))\n\nplt.imshow(coadd_roman['data'], origin='lower', \n           clim=stretch_color(coadd_roman['data'], 1)\n           )\n\nplt.plot(*coord_arr_idx, 'r+', markersize=15)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-matplotlib-imshow-to-create-a-static-visualization-of-the-roman-simulated-coadd-and-overplot-the-selected-position","position":25},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Rubin Coadds"},"type":"lvl2","url":"/openuniverse2024preview-firefly#rubin-coadds","position":26},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Rubin Coadds"},"content":"The OpenUniverse2024 data preview includes coadded mosaics in the following filters: u, g, r, i, z, y. In this section, we define some functions that make it convenient to retrieve a given cloud-hosted simulated Roman coadd based on position and filter.\n\n","type":"content","url":"/openuniverse2024preview-firefly#rubin-coadds","position":27},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Retrieve Rubin Coadds","lvl2":"Rubin Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#retrieve-rubin-coadds","position":28},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Retrieve Rubin Coadds","lvl2":"Rubin Coadds"},"content":"The OpenUniverse2024 data preview includes only one simulated Rubin coadd per filter. Below we define functions that make it convenient to retrieve the simulated Rubin coadd corresponding to the desired filter. We return data in same structure as the functions we defined above for Roman.\n\ndef get_rubin_coadd_fpath(filter): \n    coadd_fname_root = f\"deepCoadd_calexp_2877_19_{filter}_DC2_u_descdm_preview_data_step3_2877_19_w_2024_12_20240403T150003Z.fits\"\n    coadd_fpath = f\"{BUCKET_NAME}/{RUBIN_COADD_PATH}/{filter}/{coadd_fname_root}\"\n    return coadd_fpath\n\n\n\n\n\ndef get_rubin_coadd(filter):\n    coadd_s3_fpath = get_rubin_coadd_fpath(filter)\n\n    with fits.open(f\"s3://{coadd_s3_fpath}\", fsspec_kwargs={\"anon\": True}) as hdul:\n        # retrieve science data from coadd fits\n        coadd_data = hdul[1].section[:,:]\n\n        # make wcs using header\n        coadd_wcs = wcs.WCS(hdul[1].header)\n\n        return {'data': coadd_data, 'wcs': coadd_wcs}\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#retrieve-rubin-coadds","position":29},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Inspect a simulated Rubin Coadd","lvl2":"Rubin Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#inspect-a-simulated-rubin-coadd","position":30},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Inspect a simulated Rubin Coadd","lvl2":"Rubin Coadds"},"content":"\n\nChoose a filter and retrieve the data and header information from the simulated Rubin coadd corresponding to that filter.\n\nfilter_rubin = 'r'\ncoadd_rubin = get_rubin_coadd(filter_rubin)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#inspect-a-simulated-rubin-coadd","position":31},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Understand the size of a simulated Rubin coadd.","lvl2":"Rubin Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#understand-the-size-of-a-simulated-rubin-coadd","position":32},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Understand the size of a simulated Rubin coadd.","lvl2":"Rubin Coadds"},"content":"\n\n# Number of pixels (Y, X)\ncoadd_rubin['data'].shape\n\n\n\n\n\n# Pixel size (scale Y, scale X) [degrees/pixel]\ncoadd_rubin['wcs'].proj_plane_pixel_scales()\n\n\n\n\n\n# Coadd size (FOV Y, FOV X)\n[(num * size).to('arcsec') for num, size in zip(\n    coadd_rubin['data'].shape, coadd_rubin['wcs'].proj_plane_pixel_scales())]\n\n\n\nThe field of view of Rubin coadd is 840 arcsec.\n\n","type":"content","url":"/openuniverse2024preview-firefly#understand-the-size-of-a-simulated-rubin-coadd","position":33},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use matplotlib imshow to create a static visualization of the Rubin simulated coadd and overplot the selected position.","lvl2":"Rubin Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-matplotlib-imshow-to-create-a-static-visualization-of-the-rubin-simulated-coadd-and-overplot-the-selected-position","position":34},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use matplotlib imshow to create a static visualization of the Rubin simulated coadd and overplot the selected position.","lvl2":"Rubin Coadds"},"content":"\n\nplt.imshow(coadd_rubin['data'], origin='lower', \n           clim=stretch_color(coadd_rubin['data'], 1)\n           )\n\nplt.plot(*coord_to_xy(coadd_rubin['wcs'], coord), 'r+', markersize=15)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-matplotlib-imshow-to-create-a-static-visualization-of-the-rubin-simulated-coadd-and-overplot-the-selected-position","position":35},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define a function that returns the URL for a given S3 filepath","lvl2":"Rubin Coadds"},"type":"lvl3","url":"/openuniverse2024preview-firefly#define-a-function-that-returns-the-url-for-a-given-s3-filepath","position":36},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Define a function that returns the URL for a given S3 filepath","lvl2":"Rubin Coadds"},"content":"Since the OpenUniverse2024 data is available through a public S3 bucket, we can access a given S3 file using HTTPS URL as follows:\n\ndef https_url(s3_fpath):\n    s3_fpath_without_bucket = s3_fpath.split('/', 1)[1]\n    return f\"https://{BUCKET_NAME}.s3.amazonaws.com/{s3_fpath_without_bucket}\"\n\n\n\nLet’s generate URL for the Rubin coadd we plotted above. Clicking on the returned URL will allow you to download this image locally.\n\ncoadd_s3_fpath_rubin = get_rubin_coadd_fpath(filter_rubin)\nhttps_url(coadd_s3_fpath_rubin)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#define-a-function-that-returns-the-url-for-a-given-s3-filepath","position":37},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"type":"lvl2","url":"/openuniverse2024preview-firefly#compare-simulated-roman-and-rubin-cutouts-for-a-selected-position","position":38},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"content":"\n\n","type":"content","url":"/openuniverse2024preview-firefly#compare-simulated-roman-and-rubin-cutouts-for-a-selected-position","position":39},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Choose cutout size","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"type":"lvl3","url":"/openuniverse2024preview-firefly#choose-cutout-size","position":40},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Choose cutout size","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"content":"\n\ncutout_size = 50*u.arcsec\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#choose-cutout-size","position":41},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Create the cutouts","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"type":"lvl3","url":"/openuniverse2024preview-firefly#create-the-cutouts","position":42},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Create the cutouts","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"content":"\n\ncutout_roman = Cutout2D(coadd_roman['data'], coord, size=cutout_size, wcs=coadd_roman['wcs'])\ncutout_rubin = Cutout2D(coadd_rubin['data'], coord, size=cutout_size, wcs=coadd_rubin['wcs'])\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#create-the-cutouts","position":43},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use matplotlib imshow to plot static side-by-side comparisons of the cutouts","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-matplotlib-imshow-to-plot-static-side-by-side-comparisons-of-the-cutouts","position":44},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use matplotlib imshow to plot static side-by-side comparisons of the cutouts","lvl2":"Compare simulated Roman and Rubin cutouts for a selected position"},"content":"\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n\naxs[0].imshow(cutout_roman.data, origin='lower', \n              clim=stretch_color(cutout_roman.data, .5)\n              )\naxs[0].plot(*coord_to_xy(cutout_roman.wcs, coord), 'r+', markersize=15)\naxs[0].set_title(f\"ROMAN in filter {filter_roman}\")\n\naxs[1].imshow(cutout_rubin.data, origin='lower', \n              clim=stretch_color(cutout_rubin.data, .5)\n              )\naxs[1].plot(*coord_to_xy(cutout_rubin.wcs, coord), 'r+', markersize=15)\naxs[1].set_title(f\"RUBIN in filter {filter_rubin}\")\n\nfig.suptitle(f\"Cutouts at ({coord.ra}, {coord.dec}) with {cutout_size} size\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.97])\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-matplotlib-imshow-to-plot-static-side-by-side-comparisons-of-the-cutouts","position":45},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl2","url":"/openuniverse2024preview-firefly#use-firefly-to-interactively-identify-a-blended-source","position":46},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Use Firefly to interactively identify a blended source"},"content":"Clearly, the simulated Roman coadd has higher spatial resolution than the Rubin simulated coadd. Let’s try to locate blended objects to compare in the simulated Rubin and Roman images. We will use Firefly’s interactive visualization to make this task easier.\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-firefly-to-interactively-identify-a-blended-source","position":47},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Launch and initialize Firefly","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl3","url":"/openuniverse2024preview-firefly#launch-and-initialize-firefly","position":48},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Launch and initialize Firefly","lvl2":"Use Firefly to interactively identify a blended source"},"content":"There are two ways to initialize a Firefly client from Python, depending on whether you’re running the notebook in JupyterLab or not. Assuming you have jupyter-firefly-extensions set up in your environment as explained \n\nhere, you can use make_lab_client() in JupyterLab, which will open the Firefly viewer in a new tab within the Lab. Otherwise, you can use make_client() in a Jupyter Notebook (or even a Python shell), which will open the Firefly viewer in a new web browser tab.\n\nYou also need a Firefly server to communicate with your Firefly Python client. In this notebook, we use a public Firefly server: the IRSA Viewer (\n\nhttps://​irsa​.ipac​.caltech​.edu​/irsaviewer). However, you can also run a local Firefly server via a \n\nFirefly Docker image and access it at http://localhost:8080/firefly. The URL of the Firefly server is read by both make_client() and make_lab_client() through the environment variable FIREFLY_URL. However, make_client() also allows you to pass the URL directly as the url parameter.\n\n# Uncomment when using within Jupyter Lab with jupyter_firefly_extensions installed\n# fc = FireflyClient.make_lab_client()\n\n# Uncomment for contexts other than above \nfc = FireflyClient.make_client(url=\"https://irsa.ipac.caltech.edu/irsaviewer\")\n\nfc.reinit_viewer() # to clean the state, if this cell ran earlier\n\n\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#launch-and-initialize-firefly","position":49},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Send the simulated Rubin coadd to Firefly using show_fits.","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl3","url":"/openuniverse2024preview-firefly#send-the-simulated-rubin-coadd-to-firefly-using-show-fits","position":50},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Send the simulated Rubin coadd to Firefly using show_fits.","lvl2":"Use Firefly to interactively identify a blended source"},"content":"For displaying the FITS image of Rubin coadd in Firefly, we use \n\nshow_fits:\n\ncoadd_ff_id_rubin = 'rubin-coadd-filter-r'\nfc.show_fits(url=https_url(coadd_s3_fpath_rubin),\n             plot_id=coadd_ff_id_rubin,\n             Title=\"Rubin Coadd\"\n             )\n\n\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#send-the-simulated-rubin-coadd-to-firefly-using-show-fits","position":51},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use ds9 region syntax to overplot the simulated Roman image blocks on the interactive display","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-ds9-region-syntax-to-overplot-the-simulated-roman-image-blocks-on-the-interactive-display","position":52},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use ds9 region syntax to overplot the simulated Roman image blocks on the interactive display","lvl2":"Use Firefly to interactively identify a blended source"},"content":"The Firefly client includes several methods related to controlling ds9 region overlays. To\noverlay a region layer on the loaded FITS images, we can use \n\noverlay_region_layer.\n\nRegion data is defined in ds9 region syntax that can be found \n\nhere.\n\n# mark the roman coadd blocks as boxes\nroman_regions = [\n    f'icrs;box {ra_block_center.value}d {dec_block_center.value}d {block_size.value}\" {block_size.value}\" 0d'\n    for (ra_block_center, dec_block_center) in product(ra_block_centers, dec_block_centers)\n]\n\nroman_regions_id = 'roman_regions'\nfc.overlay_region_layer(region_data=roman_regions,\n                        title='Roman Mosiac', \n                        region_layer_id=roman_regions_id)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-ds9-region-syntax-to-overplot-the-simulated-roman-image-blocks-on-the-interactive-display","position":53},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s pan and zoom capabilities to locate a region of interest (a blended source)","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-pan-and-zoom-capabilities-to-locate-a-region-of-interest-a-blended-source","position":54},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s pan and zoom capabilities to locate a region of interest (a blended source)","lvl2":"Use Firefly to interactively identify a blended source"},"content":"You can view the coordinates of your mouse pointer at the bottom left of the display window. To copy the coordinates for a specific coordinate:\n\nToggle the “Click Lock” to “on” in the bottom right of the image display.\n\nClick on the position of interest, and notice that the coordinate display is now frozen.\n\nClick on “EQ-J2000”, the coordinate label in the bottom left of the image display. In the dialog that opens, change copy options to “[Python] Astropy SkyCoord” so that we can directly work with them in python.\n\nClose the dialog and click on the copy icon next to the coordinate values display.\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-pan-and-zoom-capabilities-to-locate-a-region-of-interest-a-blended-source","position":55},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Copy the coordinates from the coordinate display to the Python notebook","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl3","url":"/openuniverse2024preview-firefly#copy-the-coordinates-from-the-coordinate-display-to-the-python-notebook","position":56},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Copy the coordinates from the coordinate display to the Python notebook","lvl2":"Use Firefly to interactively identify a blended source"},"content":"We have provided an example. You can change this based on your interests.\n\ncoords_of_interest = SkyCoord('0h38m25.35s -44d00m10.1s', frame='icrs') # located and copied through UI\ncoords_of_interest\n\n\n\nWe can now use this \n\nastropy SkyCoord object to compare our coadds.\n\n","type":"content","url":"/openuniverse2024preview-firefly#copy-the-coordinates-from-the-coordinate-display-to-the-python-notebook","position":57},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use ds9 region syntax to overplot the selected position","lvl2":"Use Firefly to interactively identify a blended source"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-ds9-region-syntax-to-overplot-the-selected-position","position":58},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use ds9 region syntax to overplot the selected position","lvl2":"Use Firefly to interactively identify a blended source"},"content":"For this we use the id of the region layer we defined above, and add more region data using \n\nadd_region_data.\n\npoint_region = f'icrs;point {coords_of_interest.ra.value}d {coords_of_interest.dec.value}d # point=cross 15 text={{Blended source}}'\nfc.add_region_data(region_data=point_region, region_layer_id=roman_regions_id)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-ds9-region-syntax-to-overplot-the-selected-position","position":59},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Plot cutouts of the identified blended source"},"type":"lvl2","url":"/openuniverse2024preview-firefly#plot-cutouts-of-the-identified-blended-source","position":60},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Plot cutouts of the identified blended source"},"content":"\n\ncoadd_roman = get_roman_coadd(coords_of_interest, filter_roman)\n\n\n\n\n\ncutout_size = 20*u.arcsec\n\n\n\n\n\ncutout_roman = Cutout2D(coadd_roman['data'], coords_of_interest, size=cutout_size, wcs=coadd_roman['wcs'])\ncutout_rubin = Cutout2D(coadd_rubin['data'], coords_of_interest, size=cutout_size, wcs=coadd_rubin['wcs'])\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\naxs[0].imshow(cutout_roman.data, origin='lower', \n              clim=stretch_color(cutout_roman.data, .5)\n              )\naxs[0].set_title(f\"ROMAN in filter {filter_roman}\")\n\n# Let's also encircle the blended source we identified \naxs[0].add_patch(patches.Circle(coord_to_xy(cutout_roman.wcs, coords_of_interest), \n                                radius=50, color='r', fill=False, linewidth=2))\n# and a bonus blended source that is close to it\nother_coords = SkyCoord(coords_of_interest.ra-9.2*u.arcsec, coords_of_interest.dec+5*u.arcsec)\naxs[0].add_patch(patches.Circle(coord_to_xy(cutout_roman.wcs, other_coords),\n                                radius=36, color='cyan', fill=False, linewidth=2))\n\n\naxs[1].imshow(cutout_rubin.data, origin='lower', \n              clim=stretch_color(cutout_rubin.data, .5)\n              )\naxs[1].set_title(f\"RUBIN in filter {filter_rubin}\")\n\n# Let's also encircle the source we identified \naxs[1].add_patch(patches.Circle(coord_to_xy(cutout_rubin.wcs, coords_of_interest),\n                                radius=10, color='r', fill=False, linewidth=2))\naxs[1].add_patch(patches.Circle(coord_to_xy(cutout_rubin.wcs, other_coords),\n                                radius=8, color='cyan', fill=False, linewidth=2))\n\n\nfig.suptitle(f\"Cutouts at ({coords_of_interest.ra:6f}, {coords_of_interest.dec:6f}) with {cutout_size} size\", fontsize=14);\nplt.tight_layout(rect=[0, 0, 1, 0.97])\n# plt.savefig(\"plot.pdf\", bbox_inches='tight', pad_inches=0.2)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#plot-cutouts-of-the-identified-blended-source","position":61},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"type":"lvl2","url":"/openuniverse2024preview-firefly#use-firefly-to-visualize-the-openuniverse2024-data-preview-catalogs","position":62},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"content":"Let’s inspect the properties of sources in the Rubin coadd image. For this we will use the input truth files present in S3 bucket.\n\nThe OpenUniverse2024 data preview includes the input truth files that were used to create the simulated images. These files are in Parquet and HDF5 format, and include information about the properties of galaxies, stars, and transients.\n\nTo list the available files, we use \n\ns3fs. We open a file-system like connection\nto AWS S3 and then simply list (ls) truth files directory:\n\ns3 = s3fs.S3FileSystem(anon=True) # to browse s3 bucket\ns3.ls(f\"{BUCKET_NAME}/{TRUTH_FILES_PATH}\")\n\n\n\n\n\n# Catalog table of star properties (in parquet format)\npointsource_cat_path = f\"{BUCKET_NAME}/{TRUTH_FILES_PATH}/pointsource_10307.parquet\"\npointsource_cat_path\n\n\n\n\n\n# Catalog table of galaxy properties (in parquet format)\ngalaxy_cat_path = f\"{BUCKET_NAME}/{TRUTH_FILES_PATH}/galaxy_10307.parquet\"\ngalaxy_cat_path\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-firefly-to-visualize-the-openuniverse2024-data-preview-catalogs","position":63},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s show_table to overlay the catalogs on interactive image of coadd","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-show-table-to-overlay-the-catalogs-on-interactive-image-of-coadd","position":64},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s show_table to overlay the catalogs on interactive image of coadd","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"content":"The input truth files cover a region much larger than the data preview, so we define filters on table to constraint the sources in catalog within the ra and dec bounds of preview data. (Note: you can remove filters through the table UI if you wish to see the entire data)\n\nYou can visualize catalogs interactively with Firefly using \n\nshow_table. This capability can take many parameters. Here we will simply send our catalog to Firefly so that we can (a) see an interactive table; (b) see this table plotted over the image that we’ve already sent; and (c) use the GUI to quickly create exploratory plots. See if you can use the GUI to quickly determine approximately how many galaxies cover the Rubin image and what the redshift distribution of these galaxies is.\n\ncat_filters = [\n    f'(\"ra\" >= {ra_block_centers.min().value} AND \"ra\" <= {ra_block_centers.max().value})', \n    f'(\"dec\" >= {dec_block_centers.min().value} AND \"dec\" <= {dec_block_centers.max().value})'\n]\ncat_filters\n\n\n\n\n\nfc.show_table(url=https_url(pointsource_cat_path),\n              title='Stars Catalog',\n              tbl_id='stars_cat',\n              filters=\" AND \".join(cat_filters))\n\n\n\n\n\n\n\ngal_cat_tbl_id = 'galaxy_cat'\n\n# may take ~1.25min, because galaxy catalog is a big file\nfc.show_table(url=https_url(galaxy_cat_path),\n              title='Galaxy Catalog',\n              tbl_id=gal_cat_tbl_id,\n              filters=\" AND \".join(cat_filters))\n\n\n\n\n\nFor each row in the table you can notice a marker in the image. Selecting a row or marker changes the corresponding marker or row, respectively. You can click on “Details” tab in the UI to show properties of each source selected in image/table.","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-show-table-to-overlay-the-catalogs-on-interactive-image-of-coadd","position":65},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s apply_table_filters to show only high-redshift galaxies","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-apply-table-filters-to-show-only-high-redshift-galaxies","position":66},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s apply_table_filters to show only high-redshift galaxies","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"content":"High redshift galaxies are the most interesting, so let’s filter the table we sent to Firefly to only include z>3 galaxies. Notice how the table display and image overlay change. Notice how the chart becomes a scatterplot from a heatmap because the sources reduce. You can remove this filter or add new ones through the GUI.\n\nFor filtering, we will use \n\napply_table_filters method on the galaxy table we loaded above.\n\nfc.apply_table_filters(tbl_id=gal_cat_tbl_id,\n                       filters=\" AND \".join(cat_filters+['\"redshift\" > 3']))\n\n\n\nYou can play with the filters directly from the UI as well. Try removing adding more filters in the tables and see how markers change.\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-apply-table-filters-to-show-only-high-redshift-galaxies","position":67},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s show_fits_3color to create a 3 color image of the simulated Rubin images","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-show-fits-3color-to-create-a-3-color-image-of-the-simulated-rubin-images","position":68},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s show_fits_3color to create a 3 color image of the simulated Rubin images","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"content":"\n\n# [R, G, B]\nROMAN_RGB_FILTERS = ['H158', 'J129', 'Y106']\nRUBIN_RGB_FILTERS = ['r', 'g', 'u']\n\n\n\nWe already have Rubin coadd with catalog overlaid, let’s make a 3 color image to see colors of marked objects more clearly. For this we will use \n\nshow_fits_3color method:\n\ncoadd_ff_id_rubin_3color = 'rubin-coadd-3color'\nthreeC = [\n    dict(url=https_url(get_rubin_coadd_fpath(filter_name)),\n         Title=\"Rubin Coadd 3 color\")\n    for filter_name in RUBIN_RGB_FILTERS\n]\n\nfc.show_fits_3color(three_color_params=threeC,\n                    plot_id=coadd_ff_id_rubin_3color)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-show-fits-3color-to-create-a-3-color-image-of-the-simulated-rubin-images","position":69},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s interactivity to identify a region of interest","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-interactivity-to-identify-a-region-of-interest","position":70},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s interactivity to identify a region of interest","lvl2":"Use Firefly to visualize the OpenUniverse2024 data preview catalogs"},"content":"For example, we found a region of the sky that seems to have a high number of high redshift sources and we copy it from the image display:\n\n# located and copied through UI\nhigh_z_gal_coords = SkyCoord('0h38m00.77s -44d12m10.2s', frame='icrs')\nhigh_z_gal_coords\n\n\n\n\n\n# let's also mark it in our region layer, so that it's easy to pinpoint later\npoint_region = f'icrs;point {high_z_gal_coords.ra.value}d {high_z_gal_coords.dec.value}d # point=cross 15 text={{z>3 mock galaxies}}'\nfc.add_region_data(region_data=point_region, region_layer_id=roman_regions_id)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-interactivity-to-identify-a-region-of-interest","position":71},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"type":"lvl2","url":"/openuniverse2024preview-firefly#plot-3-color-roman-coadd-containing-your-region-of-interest","position":72},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"content":"Let’s inspect WCS of Roman coadd first\n\ncoadd_roman['wcs']\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#plot-3-color-roman-coadd-containing-your-region-of-interest","position":73},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Prepare Roman coadds for displaying in Firefly","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"type":"lvl3","url":"/openuniverse2024preview-firefly#prepare-roman-coadds-for-displaying-in-firefly","position":74},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Prepare Roman coadds for displaying in Firefly","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"content":"Roman coadds have STG projection which cannot be read by Firefly yet. Firefly can display any FITS image but it needs to read the WCS for overlaying catalogs and other interactive features. So unlike Rubin 3 color image where we directly passed URL of coadd files to Firefly, we will read Roman coadd files in Python, reproject them from STG to TAN, and write them back to FITS to pass them to Firefly.\n\nLet’s first define functions to do so:\n\ndef reproject_to_TAN(coadd_roman):\n    # Define a new WCS with TAN projection (in CTYPE key)\n    output_wcs = coadd_roman['wcs'].deepcopy()\n    output_wcs.wcs.ctype = [ctype.replace('STG', 'TAN') for ctype in coadd_roman['wcs'].wcs.ctype]\n    \n    # Use reproject to convert a given data and wcs, to a desired wcs and shape\n    reprojected_data, _ = reproject_interp(\n        (coadd_roman['data'], coadd_roman['wcs']),\n        output_projection=output_wcs,\n        shape_out=coadd_roman['data'].shape\n    )\n\n    return {'data': reprojected_data, 'wcs': output_wcs}\n\n\n\n\n\ndef get_fits_stream(coadd_roman):\n    # Create a FITS PrimaryHDU object with the coadd data\n    hdu = fits.PrimaryHDU(data=coadd_roman['data'], header=coadd_roman['wcs'].to_header())\n\n    # Write the HDU to the in-memory stream (to save I/O time)\n    fits_stream = BytesIO()\n    hdu.writeto(fits_stream, overwrite=True)\n    fits_stream.seek(0) # to bring reading pointer to the beginning of file\n\n    return fits_stream\n\n\n\nThen we perform all 3 operations we mentioned above for the RGB filters of Roman:\n\ncoadds_rgb = []\ncoadds_rgb_reprojected = []\ncoadds_rgb_fits_stream = []\n\nfor filter_name in ROMAN_RGB_FILTERS:\n    print(f'\\nFILTER: {filter_name}')\n    print('Retrieving Roman coadd...')\n    coadd_roman = get_roman_coadd(high_z_gal_coords, filter_name)\n    coadds_rgb.append(coadd_roman)\n\n    print('Reprojecting to TAN...')\n    coadd_roman_reprojected = reproject_to_TAN(coadd_roman)\n    coadds_rgb_reprojected.append(coadd_roman_reprojected)\n\n    print('Writing back to fits stream...')\n    coadd_roman_fits_stream = get_fits_stream(coadd_roman_reprojected)\n    coadds_rgb_fits_stream.append(coadd_roman_fits_stream)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#prepare-roman-coadds-for-displaying-in-firefly","position":75},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s show_fits_3color to create a 3 color image of Roman coadds","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-show-fits-3color-to-create-a-3-color-image-of-roman-coadds","position":76},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s show_fits_3color to create a 3 color image of Roman coadds","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"content":"Now we upload each fits stream (in-memory fits file) to firefly using \n\nupload_fits_data() and prepare color params to pass to the \n\nshow_fits_3color().\n\nthree_color_params = [\n    {\n        'file': fc.upload_fits_data(fits_stream),\n        'Title': \"Roman Coadd 3 color\"\n    } for fits_stream in coadds_rgb_fits_stream]\n\n\n\n\n\ncoadd_ff_id_roman_3color = 'roman-coadd-3color-high_z_gal'\nfc.show_fits_3color(three_color_params=three_color_params,\n                    plot_id=coadd_ff_id_roman_3color)\n\n\n\nWe can see 3 color image of Roman coadd containing the high-redshift galaxy sources. Try panning and zomming out, you can notice it spans over one block compared to the Rubin coadd which is much larger.","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-show-fits-3color-to-create-a-3-color-image-of-roman-coadds","position":77},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s pan/zoom/align methods to locate high redshift sources","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-pan-zoom-align-methods-to-locate-high-redshift-sources","position":78},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s pan/zoom/align methods to locate high redshift sources","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"content":"Now, let’s pan & zoom to the region where we located high-redshift galaxy sources. Also align & lock all images being displayed by WCS. For these operations we use these 3 methods (click on them to see their documentation):\n\nset_pan\n\nset_zoom\n\nalign_images\n\nfc.set_pan(plot_id=coadd_ff_id_roman_3color, x=high_z_gal_coords.ra.deg, y=high_z_gal_coords.dec.deg, coord='j2000')\nfc.set_zoom(plot_id=coadd_ff_id_roman_3color, factor=1)\nfc.align_images(lock_match=True)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-pan-zoom-align-methods-to-locate-high-redshift-sources","position":79},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s set_stretch method to change the stretch of the image display via Python","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"type":"lvl3","url":"/openuniverse2024preview-firefly#use-fireflys-set-stretch-method-to-change-the-stretch-of-the-image-display-via-python","position":80},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl3":"Use Firefly’s set_stretch method to change the stretch of the image display via Python","lvl2":"Plot 3-color Roman coadd containing your region of interest"},"content":"The image has a lot of noise that obscures our high redshift sources of interest. You can use the Firefly GUI to change the stretch of the image display. We identify that squared stretch from -2 to 10 sigma highlights the colors of our sources better. You can also use the Firefly client’s \n\nset_stretch to do this via Python. This is helpful for reproducibility and for scaling up to many images.\n\nfc.set_stretch(plot_id=coadd_ff_id_roman_3color, stype='sigma', algorithm='squared', \n               band='ALL', lower_value=-2, upper_value=10)\n\n\n\n","type":"content","url":"/openuniverse2024preview-firefly#use-fireflys-set-stretch-method-to-change-the-stretch-of-the-image-display-via-python","position":81},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"About This Notebook"},"type":"lvl2","url":"/openuniverse2024preview-firefly#about-this-notebook","position":82},{"hierarchy":{"lvl1":"Using Firefly to Explore OpenUniverse2024 Data Preview Simulated Roman and Rubin Images","lvl2":"About This Notebook"},"content":"Author: Jaladh Singhal and Vandana Desai in conjunction with the IRSA Team\n\nUpdated: 2024-12-19\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/openuniverse2024preview-firefly#about-this-notebook","position":83},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs"},"type":"lvl1","url":"/cosmodc2-tap-access","position":0},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs"},"content":"This tutorial demonstrates how to access and query the CosmoDC2 Mock v1 catalogs using IRSA’s Table Access Protocol (TAP) service. Background information on the catalogs is available on the \n\nIRSA CosmoDC2 page.\n\nThe catalogs are served through IRSA’s Virtual Observatory–standard TAP \n\ninterface, which you can access programmatically in Python via the PyVO library. TAP queries are written in the Astronomical Data Query Language (ADQL) — a SQL-like language designed for astronomical catalogs (see the \n\nADQL specification).\n\nIf you are new to PyVO’s query modes, the documentation provides a helpful comparison between synchronous and asynchronous execution:  \n\nPyVO: Synchronous vs. Asynchronous Queries","type":"content","url":"/cosmodc2-tap-access","position":1},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"Tips for Working with CosmoDC2 via TAP"},"type":"lvl2","url":"/cosmodc2-tap-access#tips-for-working-with-cosmodc2-via-tap","position":2},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"Tips for Working with CosmoDC2 via TAP"},"content":"Use indexed columns for fast queries.\nCosmoDC2 is indexed on the following fields:\nra, dec, redshift, mag*_lsst, halo_mass, stellar_mass\nQueries involving these columns generally return much faster.\n\nEnsure your positional queries fall within the survey footprint.\nCosmoDC2 covers the area specified by the\nfollowing (R.A., decl.) coordinate pairs (J2000):\n(71.46,−27.25), (52.25,−27.25),\n(73.79,−44.33), (49.42,−44.33).\n\nAvoid overloading the TAP service.\nPreferentially use asynchronous queries for long running queries to avoid timing out.  The whole system will slow down if a lot of people are using it for large queries, or if you decide to kick off many large queries at the same time.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install numpy matplotlib pyvo\n\n\n\nimport pyvo as vo\nimport numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\n\n\n\nservice = vo.dal.TAPService(\"https://irsa.ipac.caltech.edu/TAP\")\n\n\n\n","type":"content","url":"/cosmodc2-tap-access#tips-for-working-with-cosmodc2-via-tap","position":3},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"1. List the available DC2 tables"},"type":"lvl2","url":"/cosmodc2-tap-access#id-1-list-the-available-dc2-tables","position":4},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"1. List the available DC2 tables"},"content":"\n\ntables = service.tables\nfor tablename in tables.keys():\n    if not \"tap_schema\" in tablename:\n        if \"dc2\" in tablename:\n            tables[tablename].describe()\n\n\n\n","type":"content","url":"/cosmodc2-tap-access#id-1-list-the-available-dc2-tables","position":5},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"2. Choose the DC2 catalog you want to work with."},"type":"lvl2","url":"/cosmodc2-tap-access#id-2-choose-the-dc2-catalog-you-want-to-work-with","position":6},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"2. Choose the DC2 catalog you want to work with."},"content":"IRSA currently offers 3 versions of the DC2 catalog.\n\ncosmodc2mockv1_new has been optimized to make searches with constraints on stellar mass and redshift fast.\n\ncosmodc2mockv1 has been optimized to make searches with spatial constraints fast.\n\ncosmodc2mockv1_heavy is the same as cosmodc2mockv1_new, except that it does not contain galaxies with stellar masses <= 10^7 solar masses.\n\nIf you are new to the DC2 catalog, we recommend that you start with cosmodc2mockv1_heavy\n\n# Choose the abridged table to start with.\n# Queries should be faster on smaller tables.\n\ntablename = 'cosmodc2mockv1_heavy'\n\n\n\n","type":"content","url":"/cosmodc2-tap-access#id-2-choose-the-dc2-catalog-you-want-to-work-with","position":7},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"3. What is the default maximum number of rows returned by the service?"},"type":"lvl2","url":"/cosmodc2-tap-access#id-3-what-is-the-default-maximum-number-of-rows-returned-by-the-service","position":8},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"3. What is the default maximum number of rows returned by the service?"},"content":"This service will return a maximum of 2 billion rows by default.\n\nservice.maxrec\n\n\n\nThis default maximum can be changed, and there is no hard upper limit to what it can be changed to.\n\nprint(service.hardlimit)\n\n\n\n","type":"content","url":"/cosmodc2-tap-access#id-3-what-is-the-default-maximum-number-of-rows-returned-by-the-service","position":9},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"4. List the columns in the chosen table"},"type":"lvl2","url":"/cosmodc2-tap-access#id-4-list-the-columns-in-the-chosen-table","position":10},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"4. List the columns in the chosen table"},"content":"This table contains 301 columns.\n\ncolumns = tables[tablename].columns\nprint(len(columns))\n\n\n\nLet’s learn a bit more about them.\n\nfor col in columns:\n    print(f'{f\"{col.name}\":30s}  {col.description}')\n\n\n\n","type":"content","url":"/cosmodc2-tap-access#id-4-list-the-columns-in-the-chosen-table","position":11},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"5. Retrieve a list of galaxies within a small area"},"type":"lvl2","url":"/cosmodc2-tap-access#id-5-retrieve-a-list-of-galaxies-within-a-small-area","position":12},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"5. Retrieve a list of galaxies within a small area"},"content":"Since we know that cosmoDC2 is a large catalog, we can start with a spatial search over a small square area. The ADQL that is needed for the spatial constraint is shown below.  We then show how to make a redshift histogram of the sample generated.\n\n# Setup the query\nadql = f\"\"\"\nSELECT redshift\nFROM {tablename}\nWHERE CONTAINS(\n    POINT('ICRS', ra, dec),\n    CIRCLE('ICRS', 54.0, -37.0, 0.05)\n) = 1\n\"\"\"\n\ncone_results = service.run_sync(adql)\n\n\n\n#how many redshifts does this return?\nprint(len(cone_results))\n\n\n\n# Now that we have a list of galaxy redshifts in that region, we can\n# create a histogram of the redshifts to see what redshifts this survey includes.\n\n# Plot a histogram\nnum_bins = 20\n# the histogram of the data\nn, bins, patches = plt.hist(cone_results['redshift'], num_bins,\n                            facecolor='blue', alpha = 0.5)\nplt.xlabel('Redshift')\nplt.ylabel('Number')\nplt.title(f'Redshift Histogram {tablename}')\n\n\n\nWe can see form this plot that the simulated galaxies go out to z = 3.\n\n","type":"content","url":"/cosmodc2-tap-access#id-5-retrieve-a-list-of-galaxies-within-a-small-area","position":13},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"6. Visualize galaxy colors: redshift search"},"type":"lvl2","url":"/cosmodc2-tap-access#id-6-visualize-galaxy-colors-redshift-search","position":14},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"6. Visualize galaxy colors: redshift search"},"content":"First, we’ll do a narrow redshift cut with no spatial constraint.  Then, from that redshift sample we will visualize the galaxy main sequence at z = 2.0.\n\n# Setup the query\nadql = f\"\"\"\nSELECT TOP 50000\n    mag_r_lsst,\n    (mag_g_lsst - mag_r_lsst) AS color,\n    redshift\nFROM {tablename}\nWHERE redshift BETWEEN 1.95 AND 2.05\n\"\"\"\nredshift_results = service.run_sync(adql)\n\n\n\n\n\nredshift_results\n\n\n\n\n\n# Construct a 2D histogram of the galaxy colors\nplt.hist2d(redshift_results['mag_r_lsst'], redshift_results['color'],\n            bins=100, cmap='plasma', cmax=500)\n\n# Plot a colorbar with label.\ncb = plt.colorbar()\ncb.set_label('Number')\n\n# Add title and labels to plot.\nplt.xlabel('LSST Mag r')\nplt.ylabel('LSST rest-frame g-r color')\n\n\n\n","type":"content","url":"/cosmodc2-tap-access#id-6-visualize-galaxy-colors-redshift-search","position":15},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"7. Suggestions for further queries:"},"type":"lvl2","url":"/cosmodc2-tap-access#id-7-suggestions-for-further-queries","position":16},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"7. Suggestions for further queries:"},"content":"TAP queries are extremely powerful and provide flexible ways to explore large catalogs like CosmoDC2, including spatial searches, photometric selections, cross-matching, and more.\nHowever, many valid ADQL queries can take minutes or longer to complete due to the size of the catalog, so we avoid running those directly in this tutorial.\nInstead, the examples here have so far focused on fast, lightweight queries that illustrate the key concepts without long wait times.\nIf you are interested in exploring further, here are some additional query ideas that are scientifically useful but may take longer to run depending on server conditions.","type":"content","url":"/cosmodc2-tap-access#id-7-suggestions-for-further-queries","position":17},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Count the total number of redshifts in the chosen table","lvl2":"7. Suggestions for further queries:"},"type":"lvl3","url":"/cosmodc2-tap-access#count-the-total-number-of-redshifts-in-the-chosen-table","position":18},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Count the total number of redshifts in the chosen table","lvl2":"7. Suggestions for further queries:"},"content":"The answer for the 'cosmodc2mockv1_heavy' table is 597,488,849 redshifts.adql = f\"SELECT count(redshift) FROM {tablename}\"","type":"content","url":"/cosmodc2-tap-access#count-the-total-number-of-redshifts-in-the-chosen-table","position":19},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Count galaxies in a sky region (cone search)","lvl2":"7. Suggestions for further queries:"},"type":"lvl3","url":"/cosmodc2-tap-access#count-galaxies-in-a-sky-region-cone-search","position":20},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Count galaxies in a sky region (cone search)","lvl2":"7. Suggestions for further queries:"},"content":"Generally useful for: estimating source density, validating spatial footprint, testing spatial completeness.adql = f\"\"\"\nSELECT COUNT(*)\nFROM {tablename}\nWHERE CONTAINS(POINT('ICRS', ra, dec), CIRCLE('ICRS', 54.2, -37.5, 0.2)) = 1\n\"\"\"","type":"content","url":"/cosmodc2-tap-access#count-galaxies-in-a-sky-region-cone-search","position":21},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Retrieve only a subset of columns (recommended for speed) and rows","lvl2":"7. Suggestions for further queries:"},"type":"lvl3","url":"/cosmodc2-tap-access#retrieve-only-a-subset-of-columns-recommended-for-speed-and-rows","position":22},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Retrieve only a subset of columns (recommended for speed) and rows","lvl2":"7. Suggestions for further queries:"},"content":"This use of “TOP 5000” just limits the number of rows returned.\nRemove it if you want all rows, but keep in mind such a query can take a much longer time.adql = f\"\"\"\nSELECT TOP 5000\n    ra,\n    dec,\n    redshift,\n    stellar_mass\nFROM {tablename}\"\"\"","type":"content","url":"/cosmodc2-tap-access#retrieve-only-a-subset-of-columns-recommended-for-speed-and-rows","position":23},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Explore the stellar–halo mass relation","lvl2":"7. Suggestions for further queries:"},"type":"lvl3","url":"/cosmodc2-tap-access#explore-the-stellar-halo-mass-relation","position":24},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Explore the stellar–halo mass relation","lvl2":"7. Suggestions for further queries:"},"content":"adql = f\"\"\"\nSELECT TOP 500000\n    stellar_mass,\n    halo_mass\nFROM {tablename}\nWHERE halo_mass > 1e11\"\"\"","type":"content","url":"/cosmodc2-tap-access#explore-the-stellar-halo-mass-relation","position":25},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Find the brightest galaxies at high redshift","lvl2":"7. Suggestions for further queries:"},"type":"lvl3","url":"/cosmodc2-tap-access#find-the-brightest-galaxies-at-high-redshift","position":26},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl3":"Find the brightest galaxies at high redshift","lvl2":"7. Suggestions for further queries:"},"content":"Return the results in ascending (ASC) order by r band magnitude.adql = f\"\"\"\nSELECT TOP 10000\n    ra, dec, redshift, mag_r_lsst\nFROM {tablename}\nWHERE redshift > 2.5\nORDER BY mag_r_lsst ASC\n\"\"\"\n\n","type":"content","url":"/cosmodc2-tap-access#find-the-brightest-galaxies-at-high-redshift","position":27},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"About this notebook"},"type":"lvl2","url":"/cosmodc2-tap-access#about-this-notebook","position":28},{"hierarchy":{"lvl1":"Querying the CosmoDC2 Mock v1 Catalogs","lvl2":"About this notebook"},"content":"Author: IRSA Data Science Team, including Vandana Desai, Jessica Krick, Troy Raen, Brigitta Sipőcz, Andreas Faisst, Jaladh Singhal\n\nUpdated: 2025-12-16\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.\n\nRuntime: As of the date above, this notebook takes about 2 minutes to run to completion on a machine with 8GB RAM and 2 CPU.  Large variations in this runtime can be expected if the TAP server is busy with many queries at once.","type":"content","url":"/cosmodc2-tap-access#about-this-notebook","position":29},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images"},"type":"lvl1","url":"/openuniverse2024-roman-simulated-timedomainsurvey","position":0},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images"},"content":"","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey","position":1},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Learning Goals:"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#learning-goals","position":2},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Learning Goals:"},"content":"By the end of this tutorial, you will:\n\nlearn more about the “observations” that make up the simulated Roman TDS preview.\n\nlearn how to find the locations of simulated supernovae in the preview data.\n\nlearn how to create aligned cutouts of simulated Roman images.\n\nlearn how to make an animated gif from these cutouts.\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#learning-goals","position":3},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Install and Import required modules"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#install-and-import-required-modules","position":4},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Install and Import required modules"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install astropy matplotlib numpy pandas pyarrow s3fs scipy\n\n\n\n\n\n# Import modules\nimport warnings\n\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.nddata import Cutout2D\nfrom astropy.nddata.utils import NoOverlapError\nfrom astropy.table import Table\nfrom astropy.wcs import WCS, FITSFixedWarning\nfrom matplotlib import animation\nfrom scipy.ndimage import rotate\n\n# Needed to access data in the cloud\nimport s3fs\ns3 = s3fs.S3FileSystem(anon=True)  # create an S3 client\n\n# Filter out the FITSFixedWarning, which is consequenceless and gets thrown every time you deal with a WCS\n# in a Roman openuniverse simulated image using astropy.\nwarnings.simplefilter('ignore', category=FITSFixedWarning)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#install-and-import-required-modules","position":5},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Define a module to get the date (mjd) of a particular pointing."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#define-a-module-to-get-the-date-mjd-of-a-particular-pointing","position":6},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Define a module to get the date (mjd) of a particular pointing."},"content":"\n\ndef get_mjd(pointing,\n            obseq_path=f's3://nasa-irsa-simulations/openuniverse2024/roman/preview/RomanTDS/Roman_TDS_obseq_11_6_23.fits'):\n\n    \"\"\"\n    Retrieve MJD of a given pointing.\n\n    :param pointing: Pointing ID.\n    :type pointing: int\n    :param obseq_path: Path to obseq file Roman_TDS_obseq_11_6_23.fits.\n    :type obseq_path: str, optional\n    :return: MJD of specified pointing.\n    :rtype: float\n    \"\"\"\n\n    with fits.open(obseq_path, fsspec_kwargs={\"anon\": True}) as obs:\n        obseq = Table(obs[1].data)\n    mjd = float(obseq['date'][int(pointing)])\n\n    return mjd\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#define-a-module-to-get-the-date-mjd-of-a-particular-pointing","position":7},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Define a module to create an animated gif from a collection of cutouts."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#define-a-module-to-create-an-animated-gif-from-a-collection-of-cutouts","position":8},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Define a module to create an animated gif from a collection of cutouts."},"content":"\n\ndef animate_stamps(stamps, savepath, no_whitespace=True,\n                   labels=[],labelxy=(0.05,0.95),\n                   **kwargs):\n    \"\"\"\n    Make an animation of a sequence of image stamps.\n\n    :param stamps: Must be in chronological order.\n    :type stamps: List of stamps from get_stamps or get_object_instances.\n    :param savepath: Path to save gif.\n    :type savepath: str\n    \"\"\"\n\n    if no_whitespace:\n        with_whitespace = np.invert(np.any((np.isnan(np.array(stamps))), axis=(1,2))) # NOTE: Your first axis (first indexing value) should return one stamp. e.g. stamps[0] is the first stamp.\n        idx_whitespace = np.where(with_whitespace)[0]\n        stamps = np.array(stamps)[idx_whitespace]\n        if len(labels) != 0:\n            labels = np.array(labels)[idx_whitespace]\n\n    fig, ax = plt.subplots(figsize=(5,5))\n    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n    plt.xticks([])\n    plt.yticks([])\n\n    im = ax.imshow(stamps[0], animated=True)\n\n    if len(labels) != 0:\n        txt = ax.text(labelxy[0],labelxy[1],labels[0],animated=True,color='white',transform=ax.transAxes,va='top',ha='left',**kwargs)\n\n    def animate(i):\n        im.set_array(stamps[i])\n        if len(labels) != 0:\n            txt.set_text(labels[i])\n\n            return [im] + [txt]\n        else:\n            return [im]\n\n    writer = animation.PillowWriter()\n    anim = animation.FuncAnimation(fig, animate, interval=600, frames=len(stamps))\n    anim.save(savepath, writer=writer)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#define-a-module-to-create-an-animated-gif-from-a-collection-of-cutouts","position":9},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Read in the Observation Sequence File to learn more about the “observations” that make up the simulated Roman Time Domain Survey."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#read-in-the-observation-sequence-file-to-learn-more-about-the-observations-that-make-up-the-simulated-roman-time-domain-survey","position":10},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Read in the Observation Sequence File to learn more about the “observations” that make up the simulated Roman Time Domain Survey."},"content":"\n\n# Read in the (simulated) Observation Sequence File.\n\nBUCKET_NAME = 'nasa-irsa-simulations'\nROMAN_PREFIX = 'openuniverse2024/roman/preview'\n\nROMAN_TDS_PATH = f'{ROMAN_PREFIX}/RomanTDS'\nFILENAME = 'Roman_TDS_obseq_11_6_23.fits'\nOBSEQ_PATH = f's3://{BUCKET_NAME}/{ROMAN_TDS_PATH}/{FILENAME}'\n\nobseq_hdu = fits.open(OBSEQ_PATH, fsspec_kwargs={\"anon\": True})\nobseq = pd.DataFrame(obseq_hdu[1].data)\n\nprint(obseq)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#read-in-the-observation-sequence-file-to-learn-more-about-the-observations-that-make-up-the-simulated-roman-time-domain-survey","position":11},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"What is the spatial and temporal coverage of the openuniverse2024 Roman TDS data preview?"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#what-is-the-spatial-and-temporal-coverage-of-the-openuniverse2024-roman-tds-data-preview","position":12},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"What is the spatial and temporal coverage of the openuniverse2024 Roman TDS data preview?"},"content":"\n\n# Find the ranges of RA, Dec, and date listed in the observation sequence file.\n\nra_min, dec_min = obseq[['ra','dec']].min()\nra_max, dec_max = obseq[['ra','dec']].max()\nmjd_min = obseq['date'].min()\nmjd_max = obseq['date'].max()\n\nprint(\"ra_min, ra_max:\", ra_min, ra_max)\nprint(\"mjd_min, mjd_max:\", mjd_min, mjd_max)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#what-is-the-spatial-and-temporal-coverage-of-the-openuniverse2024-roman-tds-data-preview","position":13},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Read in the Supernova Analysis (SNANA) file."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#read-in-the-supernova-analysis-snana-file","position":14},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Read in the Supernova Analysis (SNANA) file."},"content":"\n\nparquet_file = f's3://{BUCKET_NAME}/{ROMAN_PREFIX}/roman_rubin_cats_v1.1.2_faint/snana_10307.parquet'\ntransients = pd.read_parquet(parquet_file, filesystem=s3)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#read-in-the-supernova-analysis-snana-file","position":15},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Let’s find a relatively nearby SN Ia that lies within the region of the data preview."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#lets-find-a-relatively-nearby-sn-ia-that-lies-within-the-region-of-the-data-preview","position":16},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Let’s find a relatively nearby SN Ia that lies within the region of the data preview."},"content":"\n\n#List the unique models in the SNANA file.\nunique_models = pd.Series(transients['model_name']).drop_duplicates().tolist()\nunique_models\n\n\n\n\n\n# Most of the models are non SNIa (NON1ASED).\n# Choose only the SNIa\nsn1a = transients[transients['model_name'] == 'SALT3.NIR_WAVEEXT'] # SNe Ia only.\nprint('Number of SN1a in SNANA file: ', len(sn1a))\n\n\n\n\n\n# Choose the SNIa that overlap with the spatial extent of the OpenUniverse2024 Roman TDS data preview.\nra_mask = np.logical_and(sn1a['ra'] > ra_min, sn1a['ra'] < ra_max)\ndec_mask = np.logical_and(sn1a['dec'] > dec_min, sn1a['dec'] < dec_max)\nmjd_mask = np.logical_and(sn1a['start_mjd'] > mjd_min, sn1a['end_mjd'] < mjd_max)\nall_mask = np.logical_and.reduce((ra_mask,dec_mask,mjd_mask))\npreview_sn1a = sn1a[all_mask]\nprint('Number of SNIa in OpenUniverse2024 data preview:', len(preview_sn1a))\n\n\n\n\n\n# Choose the SNIa in the data preview that are nearby, at redshifts less than 0.7.\nnearby_preview_sn1a = preview_sn1a[preview_sn1a['z_CMB'] < 0.7]\nprint('Number of nearby SNIa in OpenUniverse2024 data preview:', len(nearby_preview_sn1a))\n\n\n\n\n\n# Let's choose SN 20000808.\noid = 20000808\nchosen_object = nearby_preview_sn1a[nearby_preview_sn1a['id'] == oid]\nra = chosen_object.get('ra')\ndec = chosen_object.get('dec')\nra, dec = 9.619282, -44.313894\ncoord = SkyCoord(ra*u.deg, dec*u.deg)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#lets-find-a-relatively-nearby-sn-ia-that-lies-within-the-region-of-the-data-preview","position":17},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Read in the auxiliary file that lists the simulated Roman TDS images covering the chosen SNIa."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#read-in-the-auxiliary-file-that-lists-the-simulated-roman-tds-images-covering-the-chosen-snia","position":18},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Read in the auxiliary file that lists the simulated Roman TDS images covering the chosen SNIa."},"content":"\n\n# The auxiliary file contains all the images this thing is in.\n# If you need to download this file, see https://irsa.ipac.caltech.edu/docs/notebooks/.\ncsvfile = './openuniverse2024_roman_demodata_20000808_instances.csv'\ninstances = pd.read_csv(csvfile, usecols=['filter','pointing','sca'])\ninstances\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#read-in-the-auxiliary-file-that-lists-the-simulated-roman-tds-images-covering-the-chosen-snia","position":19},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Create cutouts of the chosen SNIa in the band of your choice."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#create-cutouts-of-the-chosen-snia-in-the-band-of-your-choice","position":20},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Create cutouts of the chosen SNIa in the band of your choice."},"content":"\n\nband = 'R062'\ninstances = instances[instances['filter'] == band]\n\n\n\n\n\n#Make the cutouts; this will take a couple of minutes.\nstamps = []\nmjd = []\nfor i, row in enumerate(instances.itertuples()):\n    band, pointing, sca = row[1], row[2], row[3]\n    imgpath = f's3://{BUCKET_NAME}/{ROMAN_TDS_PATH}/images/simple_model/{band}/{pointing}/Roman_TDS_simple_model_{band}_{pointing}_{sca}.fits.gz'\n    print(imgpath)\n    with fits.open(imgpath, fsspec_kwargs={\"anon\": True}) as hdu:\n        img = hdu[1].data\n        header = hdu[0].header\n        wcs = WCS(header)\n        x, y = wcs.world_to_pixel(coord)\n\n        # Manually rotate the images so they are all aligned.\n        CDmat = np.array([header['CD1_1'], header['CD1_2'],\n                          header['CD2_1'], header['CD2_2']]).reshape(2,2)\n\n        orientation = hdu[0].header['ORIENTAT']\n\n        # These chips are \"flipped\".\n        if sca % 3 == 0:\n            orientation += 180\n\n        # Build rotation matrix.\n        CD1_1_rot = np.cos(-orientation*np.pi/180)\n        CD1_2_rot = -np.sin(-orientation*np.pi/180)\n        CD2_1_rot = np.sin(-orientation*np.pi/180)\n        CD2_2_rot = np.cos(-orientation*np.pi/180)\n\n        RotMat = np.array([CD1_1_rot, CD1_2_rot,\n                          CD2_1_rot, CD2_2_rot]).reshape(2,2)\n\n        RotMat_inv = np.array([CD1_1_rot, -CD1_2_rot,\n                              -CD2_1_rot, CD2_2_rot]).reshape(2,2)\n\n        # Apply rotation to the CDi_j header keywords.\n        CDmat_rot = np.dot(CDmat,RotMat_inv)\n\n        # Update header.\n        header['CD1_1'], header['CD1_2'] = CDmat_rot[0]\n        header['CD2_1'], header['CD2_2'] = CDmat_rot[1]\n        header['ORIENTAT'] -= orientation\n\n        # Rotate the image.\n        rot_img = rotate(img,angle=orientation,reshape=False,cval=np.nan)\n        hdu[1].data = rot_img\n\n        rot_wcs = WCS(header)\n\n        try:\n            # Make cutout around SN Ia location.\n            cutout = Cutout2D(rot_img,coord,100,wcs=rot_wcs,mode='partial')\n            stamps.append(cutout.data)\n            mjd.append(get_mjd(pointing))\n        except NoOverlapError:\n            pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#create-cutouts-of-the-chosen-snia-in-the-band-of-your-choice","position":21},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Make an animated gif out of the cutouts."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#make-an-animated-gif-out-of-the-cutouts","position":22},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"Make an animated gif out of the cutouts."},"content":"\n\nsavepath = f'SN{oid}.gif'\nsavepath\nanimate_stamps(stamps, savepath, labels=mjd)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#make-an-animated-gif-out-of-the-cutouts","position":23},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"About this notebook"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-timedomainsurvey#about-this-notebook","position":24},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman Time Domain Survey images","lvl2":"About this notebook"},"content":"Author: Lauren Aldoroty (\n\nlaurenaldoroty@gmail​.com) with minor subsequent modifications to match repository style\n\nUpdated: 2024-06-10\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/openuniverse2024-roman-simulated-timedomainsurvey#about-this-notebook","position":25},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images"},"type":"lvl1","url":"/openuniverse2024-roman-simulated-wideareasurvey","position":0},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images"},"content":"","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey","position":1},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Learning Goals"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#learning-goals","position":2},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will learn how to do the following:\n\nIdentify the row and column that contains a particular ra, dec coordinate.\n\nBrowse the bucket containing the simulated Roman coadds.\n\nIdentify a simulated Roman coadd by filter, row, and column.\n\nTake a closer look at a simulated coadd file.","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#learning-goals","position":3},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Install and Import required modules"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#install-and-import-required-modules","position":4},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Install and Import required modules"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install astropy numpy s3fs\n\n\n\n\n\n#Import modules\nfrom astropy.io import fits\nimport numpy as np\nimport s3fs  # browse buckets\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#install-and-import-required-modules","position":5},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Identify the row and column that contains a particular ra, dec coordinate."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#identify-the-row-and-column-that-contains-a-particular-ra-dec-coordinate","position":6},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Identify the row and column that contains a particular ra, dec coordinate."},"content":"The full simulation covers a 1 deg x 1 deg area centered around RA = 9.5 deg and\nDec = -44.1 deg. This region is divided into 1296 blocks (36 rows and 36 columns),\neach 100 arcsec across.\n\nThe data preview presented here covers the central 20x20 arcmin, corresponding to 144 blocks (12 rows and 12 columns).\n\n#Choose an RA, DEC of interest.\nra = 9.5981595\ndec = -44.2026950\n\n#Centers of data preview blocks. Do not alter.\nra_block_centers = np.array([9.76330352298415, 9.724522605135252, 9.68574158906671,\n                    9.646960496603766, 9.608179349571955, 9.56939816979703,\n                    9.530616979104877, 9.491835799321422, 9.453054652272561,\n                    9.414273559784032, 9.375492543681393, 9.336711625789874])\ndec_block_centers = np.array([-44.252584927082495, -44.22480733304182, -44.197029724175756,\n                              -44.16925210374898, -44.14147447502621, -44.11369684127218,\n                              44.08591920575162, -44.05814157172923, -44.03036394246976,\n                              -44.0025863212379, -43.974808711298394, -43.94703111591591])\n\nra_difference_array = np.absolute(ra_block_centers-ra)\nra_block_centers_index = ra_difference_array.argmin()\nclosest_ra_center = ra_block_centers[ra_block_centers_index]\nra_dist = 3600. * ra_difference_array[ra_block_centers_index]\nif ra_dist > 50:\n    print(\"Chosen ra not covered by OpenUniverse 2024 data preview simulated Roman coadds\")\nelse:\n    COLUMN = ra_block_centers_index + 12\n    print(\"COLUMN:\", COLUMN)\n    print(\"\")\n\ndec_difference_array = np.absolute(dec_block_centers-dec)\ndec_block_centers_index = dec_difference_array.argmin()\nclosest_dec_center = dec_block_centers[dec_block_centers_index]\ndec_dist = 3600. * dec_difference_array[dec_block_centers_index]\n\nif dec_dist > 50:\n    print(\"Chosen dec not covered by OpenUniverse 2024 data preview simulated Roman coadds\")\nelse:\n    ROW = dec_block_centers_index + 12\n    print(\"ROW:\", ROW)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#identify-the-row-and-column-that-contains-a-particular-ra-dec-coordinate","position":7},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Browse the bucket containing the simulated Roman coadds."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#browse-the-bucket-containing-the-simulated-roman-coadds","position":8},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Browse the bucket containing the simulated Roman coadds."},"content":"\n\ns3 = s3fs.S3FileSystem(anon=True) # create an S3 client\n\nBUCKET_NAME = \"nasa-irsa-simulations\"\nROMAN_PREFIX = \"openuniverse2024/roman/preview\"\nCOADD_PATH = f\"{ROMAN_PREFIX}/RomanWAS/images/coadds\"\n\ns3.ls(f\"{BUCKET_NAME}/{COADD_PATH}\")\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#browse-the-bucket-containing-the-simulated-roman-coadds","position":9},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Identify a Roman simulated coadd by filter, row, and column."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#identify-a-roman-simulated-coadd-by-filter-row-and-column","position":10},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Identify a Roman simulated coadd by filter, row, and column."},"content":"A simulated coadd can be uniquely identified by filter, row, and column.\n\n#Choose a filter, row, and column\nFILTER = 'H158' #Filters F184, H158, J129, K213, and Y106 are available in the data preview.\n#ROW = 12 #Rows 12-23 are available in the data preview.\n#COLUMN = 12 #Columns 12-23 are available in the data preview.\n\n#Construct the coadd filename from the chosen filter, row, and column.\nfilename_root = f\"prod_{FILTER[0]}_{COLUMN}_{ROW}_map.fits\"\n\n#Construct the full coadd path from the chosen filter, row, and column.\ns3_uri = f\"s3://{BUCKET_NAME}/{COADD_PATH}/{FILTER}/Row{ROW}/{filename_root}\"\n\n#List this filename to make sure it is found.\ns3.ls(s3_uri)\n\n\n\n","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#identify-a-roman-simulated-coadd-by-filter-row-and-column","position":11},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Take a closer look at the simulated coadd file you identified."},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#take-a-closer-look-at-the-simulated-coadd-file-you-identified","position":12},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"Take a closer look at the simulated coadd file you identified."},"content":"\n\n#Show a summary of extensions for this file.\n\nwith fits.open(s3_uri, fsspec_kwargs={\"anon\": True}) as hdul:\n    hdul.info()\n\n\n\nThe Primary HDU for the coadded image is a cube with 15 layers, i.e., its shape is 1x15x2688x2688. The layers are as follows:\n\n0 = simulated “Science” image (Roman+Rubin simulation, units of e/(0.11 arcsec)^2/exposure)\n\n1 = lab noise: based on dark frames from the April 2023 test, masked at 3 e/p/s. Units: e/(0.11 arcsec)^2/s\n\n2 = GalSim stars, on HEALPix resolution 14 grid, normalized to total flux of 1\n\n3 = noisy stars, on HEALPix resolution 14 grid, normalized to total flux of 2.4e5 e with self-Poisson noise, including 86 e^2/input pixel background variance\n\n4 = stars, on HEALPix resolution 14 grid, total flux 1, but on in only one of the passes (to test transient response)\n\n5 = stars, on HEALPix resolution 14 grid, with total flux that varies by 5% from center to edge of the focal plane (to test what happens when the filter bandpass varies; 5% is highly exaggerated)\n\n6 = GalSim extended objects, on HEALPix resolution 14 grid, right now exponential profiles. The scale radius is log-distributed between 0.125 and 0.500 arcsec, and the ellipticity (g1,g2) is uniformly distributed in the disc of radius 0.5, i.e., g1^2+g2^2<0.5^2.\n\n7,8,9 = same objects as layer 6, but with applied shear of 0.02. The shear orientations are spaced by 60° in tangent vector space, so that in the (g1,g2)-space they are spaced by 120° and can be used for finite differences. Specifically, the directions are: layer 7 -> in East-West direction (shear PA = 270°). (g1,g2) = (0.02,0) layer 8 -> in NNW-SSE direction (shear PA = 330°). (g1,g2) = (-0.02/2,0.02√3/2) layer 9 -> in NNE-SSW direction (shear PA = 30°). (g1,g2) = (-0.02/2,-0.02√3/2)\n\n10 = coadded 1/f noise map, normalized to variance per ln f of 1\n\n11,12,13,14 = coadded white noise maps, different seeds, normalized to variance of 1 in each input pixel\n\nThe following HDUs contain additional information:\n\nCONFIG = the configuration file\n\nINDATA = the input images used, as a binary table. The columns are: obsid (int32) -> observation ID sca (int16) -> SCA (1 through 18, inclusive) ra (float64) -> right ascension of pointing center in degrees dec (float64) -> declination of pointing center in degrees pa (float64) -> position angle of pointing in degrees valid (logical) -> input science data file is valid (should be True)\n\nINWEIGHT = the mean input weights for how much each 1.25x1.25 arcsec postage stamp depends on each input exposure. The shape is 1 x Nin x 84 x 84, where Nin is the number of input images listed in INDATA. Note that each postage stamp is 32 output pixels, so 84x32=2688.\n\nIf summed on axis 1, this will normally be something close to 1. Deviations of ~10% are common, due to plate scale variations and the normalization issues introduced by diffraction spikes.\n\nINWEIGHTFLAT = a reshape of INWEIGHT suitable for display as a single image in a viewer such as DS9. The contributions from the Nin input exposures are rearranged into a 1 x 84 x (N_in*84) array.\n\nFIDELITY, SIGMA, INWTSUM, EFFCOVER = maps of U_alpha/C, S_alpha, sum_i T_{alpha i}, and the effective coverage as rescaled int16 maps. See Rowe et al. (2011) for details on the definitions of these quantities. The comment in the ‘UNIT’ keyword indicates how to rescale these.\n\n","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#take-a-closer-look-at-the-simulated-coadd-file-you-identified","position":13},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"About this notebook"},"type":"lvl2","url":"/openuniverse2024-roman-simulated-wideareasurvey#about-this-notebook","position":14},{"hierarchy":{"lvl1":"Analyzing cloud-hosted simulated Roman coadded images","lvl2":"About this notebook"},"content":"Author: Vandana Desai (Science Lead, IRSA) in conjunction with the IPAC Science Platform team\n\nUpdated: 2024-06-10\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/openuniverse2024-roman-simulated-wideareasurvey#about-this-notebook","position":15},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift"},"type":"lvl1","url":"/roman-hlss-number-density","position":0},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift"},"content":"","type":"content","url":"/roman-hlss-number-density","position":1},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Learning Goals"},"type":"lvl2","url":"/roman-hlss-number-density#learning-goals","position":2},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Learning Goals"},"content":"learn how to use the mock simulations for Roman Space Telescope \n\nZhai et al. 2021\n\nmodify code to reduce memory usage when using large catalogs\n\nmake a plot of number density of galaxies as a function of redshift (recreate figure 3 from \n\nWang et al., 2021).","type":"content","url":"/roman-hlss-number-density#learning-goals","position":3},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Introduction"},"type":"lvl2","url":"/roman-hlss-number-density#introduction","position":4},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Introduction"},"content":"The Nancy Grace Roman Space Telescope will be a transformative tool for cosmology due to its unprecedented combination of wide field-of-view and high-resolution imaging. We choose to demonstrate the usefulness of the mock Roman suimulations in generating number density plots because they provide crucial insights into the large-scale structure of the universe and the distribution of matter over cosmic time. These plots illustrate how the number density of galaxies, quasars, or dark matter halos changes with redshift, revealing information about the processes of galaxy formation, evolution, and clustering. By analyzing number density plots, cosmologists can test theoretical models of structure formation, constrain cosmological parameters, and understand the influence of dark matter and dark energy on the evolution of the universe.","type":"content","url":"/roman-hlss-number-density#introduction","position":5},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"Instructions","lvl2":"Introduction"},"type":"lvl3","url":"/roman-hlss-number-density#instructions","position":6},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"Instructions","lvl2":"Introduction"},"content":"Most of the notebook can be run with a single mock galaxy catalog file which easily fits in memory (3G). The final section uses all the data to make a final number density plot.  The dataset in its entirety does not fit into memory all at once(out of memory) so we have carefully designed this function to use the least possible amount of memory.  The dataset in its entirety also takes ~30 minutes to download and requires ~30G of space to store.  We suggest only running download_simulations with download_all= True if you are ready to wait for the download and have enough storage space.  Default is to work with only one catalog which is 10% of the dataset.\n\nTechniques employed for working with out of memory datasets:- read in only some columns from the original dataset\n- convert data from float64 to float32 where high precision is not necessary\n- careful management of the code structure to not need to read in all datasets to one dataframe","type":"content","url":"/roman-hlss-number-density#instructions","position":7},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"Input","lvl2":"Introduction"},"type":"lvl3","url":"/roman-hlss-number-density#input","position":8},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"Input","lvl2":"Introduction"},"content":"None, the location of the simulated dataset is hardcoded for convenience","type":"content","url":"/roman-hlss-number-density#input","position":9},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"Output","lvl2":"Introduction"},"type":"lvl3","url":"/roman-hlss-number-density#output","position":10},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"Output","lvl2":"Introduction"},"content":"plots of number of galaxies per square degree as a function of redshift\n\noptional numpy file with the counts saved for ease of working with the information later\n\n","type":"content","url":"/roman-hlss-number-density#output","position":11},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Imports"},"type":"lvl2","url":"/roman-hlss-number-density#imports","position":12},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Imports"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install h5py pandas matplotlib numpy requests\n\n\n\n\n\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport concurrent.futures\nimport re\nimport h5py\nimport requests\n\n%matplotlib inline\n\n\n\n","type":"content","url":"/roman-hlss-number-density#imports","position":13},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"1. Read in the mock catalogs from IRSA"},"type":"lvl2","url":"/roman-hlss-number-density#id-1-read-in-the-mock-catalogs-from-irsa","position":14},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"1. Read in the mock catalogs from IRSA"},"content":"figure out how to get the files from IRSA\n\npull down one of them for testing\n\nconvert to a pandas df for ease of use\n\ndo some data exploration\n\n","type":"content","url":"/roman-hlss-number-density#id-1-read-in-the-mock-catalogs-from-irsa","position":15},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.1 Download the files from IRSA","lvl2":"1. Read in the mock catalogs from IRSA"},"type":"lvl3","url":"/roman-hlss-number-density#id-1-1-download-the-files-from-irsa","position":16},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.1 Download the files from IRSA","lvl2":"1. Read in the mock catalogs from IRSA"},"content":"This is about 30G of data in total\n\ndef download_file(file_url, save_path):\n    \"\"\"\n    Downloads a file from a given URL and saves it to a specified local path.\n    The file is downloaded in chunks to reduce memory usage.\n\n    Parameters:\n    - file_url (str): The URL of the file to download.\n    - save_path (str): The local path where the file will be saved.\n\n    Returns:\n    - str: The save_path if the download succeeds.\n\n    Raises:\n    - requests.HTTPError: If the HTTP request returned an unsuccessful status code.\n    - OSError: If there is an issue creating directories or writing the file.\n    \"\"\"\n    import os\n    import requests\n\n    response = requests.get(file_url, stream=True)\n    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx, 5xx)\n\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):  # 8KB chunks\n            if chunk:  # Filter out keep-alive chunks\n                f.write(chunk)\n\n    print(f\"Downloaded: {save_path}\")\n    return save_path\n\n\n\n\n\ndef download_files_in_parallel(file_url_list, save_dir):\n    \"\"\"\n    Downloads multiple files in parallel using a ThreadPoolExecutor.\n\n    Parameters:\n    - file_url_list (list of str): List of file URLs to download.\n    - save_dir (str): Directory where files will be saved.\n\n    Returns:\n    - list of str: List of paths where the files were successfully downloaded.\n    \"\"\"\n    import os\n    import concurrent.futures\n\n    downloaded_files = []\n    #set to a reasonable number for efficiency and scalability\n    max_workers = min(len(file_url_list), os.cpu_count() * 5, 32)\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_url = {\n            executor.submit(download_file, url, os.path.join(save_dir, os.path.basename(url))): url\n            for url in file_url_list\n        }\n\n        for future in concurrent.futures.as_completed(future_to_url):\n            file_path = future.result()\n            if file_path:\n                downloaded_files.append(file_path)\n\n    return downloaded_files\n\n\n\n\n\ndef download_simulations(download_dir, download_all=True):\n    \"\"\"\n    Download .hdf5 simulation files from the Roman Zhai2021 URL using the provided checksums.md5 file.\n\n    This function reads the list of available .hdf5 files from the checksums.md5 file, filters them,\n    and downloads the requested files to a specified directory. If a file already exists locally,\n    it will be skipped.\n\n    Parameters\n    ----------\n    download_dir : str\n        Directory where the downloaded .hdf5 files will be stored.\n        The directory will be created if it does not already exist.\n\n    download_all : bool, optional\n        If True, download all .hdf5 files listed in the checksum file.\n        If False, download only the first file. Default is True.\n\n    Returns\n    -------\n    None\n        This function does not return any value but prints the download status.\n\n    Notes\n    -----\n    - Downloads are performed in parallel using a maximum of 4 workers.\n    - Files that already exist locally are skipped.\n    \"\"\"\n    import os\n    import pandas as pd\n\n    base_url = 'https://irsa.ipac.caltech.edu/data/theory/Roman/Zhai2021'\n    checksum_url = f'{base_url}/checksums.md5'\n\n    # Read the checksums file to get the filenames\n    df = pd.read_csv(checksum_url, sep=r\"\\s+\", names=[\"checksum\", \"filename\"])\n    hdf5_files = df[df[\"filename\"].str.endswith(\".hdf5\")][\"filename\"].tolist()\n\n    if not download_all:\n        hdf5_files = hdf5_files[:1]\n\n    file_urls = [f\"{base_url}/{fname}\" for fname in hdf5_files]\n\n    os.makedirs(download_dir, exist_ok=True)\n\n    # Filter out files that already exist\n    files_to_download = []\n    for file_url in file_urls:\n        filename = os.path.basename(file_url)\n        file_path = os.path.join(download_dir, filename)\n        if os.path.exists(file_path):\n            print(f\"The file {filename} already exists. Skipping download.\")\n        else:\n            files_to_download.append(file_url)\n\n    if files_to_download:\n        print(f\"Starting parallel download of {len(files_to_download)} files.\")\n        downloaded_files = download_files_in_parallel(files_to_download, download_dir)\n        print(f\"Successfully downloaded {len(downloaded_files)} files.\")\n    else:\n        print(\"All requested files are already downloaded.\")\n\n\n\n\n\n# Set download_all to True to download all files, or False to download only the first file\n# only a single downloaded file is required to run the notebook in its entirety,\n# however section 4 will be more accuate with all 10 files\n\n# Downloading 10 files, each 3GB in size, may take between 1 and 30 minutes,\n# or even longer, depending on your proximity to the data.\n\ndownload_dir = 'downloaded_hdf5_files'\ndownload_simulations(download_dir, download_all=False)\n\n\n\n\n\n","type":"content","url":"/roman-hlss-number-density#id-1-1-download-the-files-from-irsa","position":17},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.2 read files into a pandas dataframe so we can work with them","lvl2":"1. Read in the mock catalogs from IRSA"},"type":"lvl3","url":"/roman-hlss-number-density#id-1-2-read-files-into-a-pandas-dataframe-so-we-can-work-with-them","position":18},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.2 read files into a pandas dataframe so we can work with them","lvl2":"1. Read in the mock catalogs from IRSA"},"content":"\n\ndef fetch_column_names():\n    \"\"\"\n    Fetches column names from the Readme_small.txt file at the specified URL.\n\n    Returns:\n    - List of column names (str): A list of extracted column names, or an empty list if no column names are found.\n\n    Raises:\n    - HTTPError: If the HTTP request returns an unsuccessful status code.\n    \"\"\"\n    import requests\n    import re\n\n    url = 'https://irsa.ipac.caltech.edu/data/theory/Roman/Zhai2021/Readme_small.txt'\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n\n    lines = response.text.splitlines()\n    column_names = []\n\n    for line in lines:\n        if line.strip().lower().startswith('column'):\n            match = re.search(r':\\s*([^\\s]+)', line)\n            if match:\n                column_names.append(match.group(1))\n\n    if column_names:\n        return column_names\n    else:\n        print(\"No column names found in Readme_small.txt.\")\n        return []\n\n\n\n\n\ndef read_hdf5_to_pandas(file_path, columns_to_keep, columns_to_convert):\n    \"\"\"\n    Read selected columns from an HDF5 file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the HDF5 file.\n    columns_to_keep : list of str\n        List of column names to extract from the dataset.\n    columns_to_convert : list of str\n        List of column names to convert to float32 for memory efficiency.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DataFrame containing the selected columns from the HDF5 file,\n        with specified columns cast to float32.\n    \"\"\"\n    import h5py\n    import pandas as pd\n\n    # Map of column names to their corresponding index in the HDF5 dataset\n    col_map = {\n        \"RA\": 0,\n        \"DEC\": 1,\n        \"redshift_cosmological\": 2,\n        \"redshift_observed\": 3,\n        \"velocity\": 4,\n        \"stellar\": 5,\n        \"SFR\": 6,\n        \"halo\": 7,\n        \"flux_Halpha6563\": 8,\n        \"flux_OIII5007\": 9,\n        \"M_F158_Av1.6523\": 10,\n        \"nodeIsIsolated\": 11\n    }\n\n    # Open the HDF5 file and extract only the requested columns\n    with h5py.File(file_path, \"r\") as file:\n        dataset = file[\"data\"]\n        data = {}\n        for col in columns_to_keep:\n            idx = col_map[col]\n            data[col] = dataset[:, idx]\n\n    # Create a DataFrame from the selected columns\n    df = pd.DataFrame(data)\n\n    # Convert specified columns to float32 to reduce memory usage\n    conversion_map = {col: \"float32\" for col in columns_to_convert}\n    df = df.astype(conversion_map)\n\n    return df\n\n\n\n\n\ndef assign_redshift_bins(df, z_min, z_max, dz=0.1):\n    \"\"\"\n    Assign redshift bins to a DataFrame based on 'redshift_observed'.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The input DataFrame containing a 'redshift_observed' column.\n    z_min : float\n        The minimum redshift for binning.\n    z_max : float\n        The maximum redshift for binning.\n    dz : float, optional\n        The redshift bin width (default is 0.1).\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The input DataFrame with a new 'z_bin' categorical column.\n    z_bins : numpy.ndarray\n        The redshift bin edges used to define bins.\n    z_bin_centers : list of float\n        The central redshift value for each bin.\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n\n    z_bins = np.arange(z_min, z_max + dz, dz)\n    df[\"z_bin\"] = pd.cut(df[\"redshift_observed\"], bins=z_bins, include_lowest=True, right=False)\n    z_bin_centers = [np.mean([b.left, b.right]) for b in df[\"z_bin\"].cat.categories]\n    z_bin_centers = sorted(z_bin_centers)\n    return df, z_bins, z_bin_centers\n\n\n\n\n\n#don't change this, the above code downloads the files to this directory\nfile_path = 'downloaded_hdf5_files/Roman_small_V2_0.hdf5'\n\n#out of consideration for the size of these files and the amount of memory required to work with them\n# we only keep the columns that we are going to use and convert some to 32bit instead of 64 where\n# the higher precision is not necessary to the science\ncolumns_to_keep = ['RA', 'DEC', 'redshift_observed', 'flux_Halpha6563']\ncolumns_to_convert = ['redshift_observed', 'flux_Halpha6563']\n\ndf = read_hdf5_to_pandas(file_path, columns_to_keep, columns_to_convert)\n\n\n\n","type":"content","url":"/roman-hlss-number-density#id-1-2-read-files-into-a-pandas-dataframe-so-we-can-work-with-them","position":19},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.3 Pre-process dataset","lvl2":"1. Read in the mock catalogs from IRSA"},"type":"lvl3","url":"/roman-hlss-number-density#id-1-3-pre-process-dataset","position":20},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.3 Pre-process dataset","lvl2":"1. Read in the mock catalogs from IRSA"},"content":"Add some value-added columns to the dataframe, and calculate some variables which are used by mulitple functions\n\n#add a binned redshift column and calculate the edges of the bins\nz_min = 1.0\nz_max = 3.0\ndz = 0.1\ndf, z_bins, z_bin_centers = assign_redshift_bins(df, z_min=z_min, z_max=z_max, dz=dz)\n\n#Shift RA values to continuous range for the entire DataFrame\n#This is necessary because the current range goes over 360 which appears to the code\n#as a non-contiguous section of the sky.  By shifting the RA values, we don't have the\n#jump from RA = 360 to RA = 0.\n\nra_min1=330\ndf['RA_shifted'] = (df['RA'] - ra_min1) % 360  # Shift RA to continuous range\n\n\n\n\n\n#quick check to see what we've got:\ndf\n\n\n\n","type":"content","url":"/roman-hlss-number-density#id-1-3-pre-process-dataset","position":21},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.3 Data Exploration","lvl2":"1. Read in the mock catalogs from IRSA"},"type":"lvl3","url":"/roman-hlss-number-density#id-1-3-data-exploration","position":22},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"1.3 Data Exploration","lvl2":"1. Read in the mock catalogs from IRSA"},"content":"Let’s explore the dataset a bit to see what we are working with.\n\nThis function prints the following information:\n\nFirst 5 rows of the DataFrame.\n\nDataFrame info (data types, non-null counts).\n\nSummary statistics for numeric columns.\n\nMemory usage of the DataFrame and each column.\n\nColumn names.\n\nCheck for missing (NaN) values, including columns with NaNs and their counts.\n\nNote that quite a few of the z_bin values are NaN because they are outside the redshift range of interest for this plot (1 < z < 3)\n\n# Display the first 5 rows of the dataframe\ndf.head()\n\n\n\n\n\n# Get general information about the dataframe (e.g., data types, non-null counts)\nprint(\"\\nDataFrame info (Data Types, Non-null counts):\")\ndf.info()\n\n\n\n\n\n# Display summary statistics for numeric columns\nprint(\"\\nSummary statistics for numeric columns:\")\ndf.describe()\n\n\n\n\n\n\n\n# Print memory usage of the DataFrame\nprint(\"\\nMemory usage of the DataFrame:\")\nprint(df.memory_usage(deep=True))  # Shows memory usage of each column\nprint(f\"Total memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")  # Total memory usage in MB\n\n\n\n\n\n# Check if there are any NaN (missing) values in the DataFrame\nprint(\"\\nChecking for NaN (missing) values in the DataFrame:\")\nif df.isnull().any().any():\n    print(\"There are NaN values in the DataFrame.\")\n    print(\"\\nColumns with NaN values and the number of missing values in each:\")\n    print(df.isnull().sum())\nelse:\n    print(\"There are no NaN values in the DataFrame.\")\n\n\n\n\n\n","type":"content","url":"/roman-hlss-number-density#id-1-3-data-exploration","position":23},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"2.0 Make some helpful histograms"},"type":"lvl2","url":"/roman-hlss-number-density#id-2-0-make-some-helpful-histograms","position":24},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"2.0 Make some helpful histograms"},"content":"Do the numbers we are getting and their distributions make sense?\n\nplot of the number of galaxies as a function of dec bin\n\nplot the number of galaxies per square degree as a function of dec bin\n\ndef get_bin_area(dec_edges, ra_width):\n    \"\"\"\n    Calculate the area of each Dec bin in square degrees, given the RA width.\n\n    Parameters:\n    - dec_edges: The edges of the Dec bins (1D array).\n    - ra_width: The width of the RA bins in degrees.\n\n    Returns:\n    - areas: List of areas of the Dec bins in square degrees.\n    \"\"\"\n    import numpy as np\n\n    dec_heights = np.diff(dec_edges)  # Heights of the Dec bins\n    areas = []\n    for i in range(len(dec_edges) - 1):\n        dec_center = (dec_edges[i] + dec_edges[i+1]) / 2  # Center of the Dec bin\n        area = ra_width * dec_heights[i] * np.cos(np.radians(dec_center))\n        areas.append(area)\n\n    return np.array(areas)\n\n\n\n\n\ndef calculate_counts_per_deg2(df):\n    \"\"\"\n    Computes galaxy number counts per square degree by binning data in declination.\n\n    This function shifts Right Ascension (RA) values into a continuous range,\n    bins galaxies based on Declination (DEC), and calculates the number of galaxies\n    per square degree for each declination bin. It assumes a fixed declination range\n    and divides it into 10 equal bins. The function also computes bin areas to normalize\n    the galaxy counts.\n\n    Parameters:\n    - df (pd.DataFrame): A Pandas DataFrame containing at least the 'RA' and 'DEC' columns.\n\n    Returns:\n    - galaxy_counts (np.ndarray): An array containing the number of galaxies in each declination bin.\n    - counts_per_deg2 (np.ndarray): An array of galaxy counts per square degree for each declination bin.\n    - dec_bins (np.ndarray): The edges of the declination bins.\n\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n\n    # Compute RA edges based on the data\n    ra_min = df['RA_shifted'].min()\n    ra_max = df['RA_shifted'].max()\n    ra_edges = np.linspace(ra_min, ra_max, 10 + 1)\n\n    # Set up Declination bins for constant declination binning\n    #manually fixing these here for this particular survey\n    dec_min =  -20# df['DEC'].min()\n    dec_max =  20# df['DEC'].max()\n    dec_bins = np.linspace(dec_min, dec_max, 10 + 1)  # Create declination bins\n\n    # Calculate bin areas for constant declination bins\n    ra_width = np.diff(ra_edges)[0]  # Width of RA bins (all bins assumed equal width)\n    bin_areas = get_bin_area(dec_bins, ra_width)\n\n    df['dec_bin'] = pd.cut(df['DEC'], bins=dec_bins, include_lowest=True, right=False).astype('category')\n\n    # Count galaxies in each Declination bin\n    galaxy_counts = df.groupby('dec_bin', observed=False).size().values\n\n    # Calculate galaxy counts per square degree\n    counts_per_deg2 = galaxy_counts / bin_areas\n\n    return galaxy_counts, counts_per_deg2, dec_bins\n\n\n\n\n\ndef plot_dec_bins(dec_bins, galaxy_counts):\n    \"\"\"\n    Plots a histogram of galaxy counts across declination bins.\n\n    This function visualizes the number of galaxies in different declination (DEC) bins\n    using a bar plot. The y-axis is set to a logarithmic scale to enhance visibility of\n    variations in galaxy counts. The function assumes uniform bin sizes.\n\n    Parameters:\n    - dec_bins (np.ndarray): An array containing the edges of the declination bins.\n    - galaxy_counts (np.ndarray): An array containing the number of galaxies in each declination bin.\n\n     \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Calculate the width of each bin (assumes uniform bin sizes)\n    bin_width = dec_bins[1] - dec_bins[0]\n\n\n    # Plot the histogram for the DEC column with 10 bins\n    plt.figure(figsize=(8, 6))\n    plt.bar(dec_bins[:-1], galaxy_counts, width=bin_width, color='skyblue', edgecolor='black', align='edge')\n    plt.xlabel('Declination (DEC)')\n    plt.ylabel('Number of Galaxies')\n    plt.title('Galaxy counts as a function of declination')\n    plt.yscale('log')  # Set y-axis to logarithmic scale\n\n    #plt.ylim(1e5, 1.6e5)  # Set y-axis range\n    plt.grid(False)\n    plt.show()\n    plt.close()  # Explicitly close the plot\n\n\n\n\n\ndef plot_dec_bins_per_deg2(dec_bins, counts_per_deg2):\n    \"\"\"\n    Plots a histogram of galaxy number density across declination bins.\n\n    This function visualizes the number of galaxies per square degree as a function\n    of declination using a bar plot. The y-axis is set to a logarithmic scale to\n    improve visibility of variations in galaxy density. The function assumes uniform\n    bin sizes.\n\n    Parameters:\n    - dec_bins (np.ndarray): An array containing the edges of the declination bins.\n    - counts_per_deg2 (np.ndarray): An array containing the number of galaxies per square degree\n      for each declination bin.\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Calculate the width of each bin (assumes uniform bin sizes)\n    bin_width = dec_bins[1] - dec_bins[0]\n\n\n    # Plot the histogram for the DEC column with 10 bins\n    plt.figure(figsize=(8, 6))\n    plt.bar(dec_bins[:-1], counts_per_deg2, width=bin_width, color='skyblue', edgecolor='black', align='edge')\n    plt.xlabel('Declination (DEC)')\n    plt.ylabel('Number of galaxies per square degree')\n    plt.title('Number density as a function of declination')\n    plt.yscale('log')  # Set y-axis to logarithmic scale\n    #plt.ylim(1e5, 1.6e5)  # Set y-axis range\n    plt.grid(False)\n    plt.show()\n    plt.close()\n\n\n\n\n\ngalaxy_counts, counts_per_deg2, dec_bins = calculate_counts_per_deg2(df)\nplot_dec_bins(dec_bins, galaxy_counts)\nplot_dec_bins_per_deg2(dec_bins, counts_per_deg2)\n\n\n\n\n\nFigure Caption: Both plots have Declination on their X-axis.  The top panel shows the total number of galaxies.  The bottom panel shows the number of galaxies per square degree.  Note that they y-axis is displayed in a log scale, and shows a quite small range.  The goal of this plot is to explore the data a bit and see what variation we have as a function of declination bins.\n\n","type":"content","url":"/roman-hlss-number-density#id-2-0-make-some-helpful-histograms","position":25},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"3.0 Make Number density plot"},"type":"lvl2","url":"/roman-hlss-number-density#id-3-0-make-number-density-plot","position":26},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"3.0 Make Number density plot"},"content":"This is a first pass at making a number density plot with just a single data file.  We divide the sample into 10 declination bins in order to do a jackknife sampling and obtain error bars for the plot. We choose to use constant dec bins instead of constant RA bins since area is function of cosine dec so we don’t want to average over too much dec.\n\ndef jackknife_galaxy_count(df, grid_cells=10):\n    \"\"\"\n    Calculate mean galaxy counts per square degree for each redshift bin, using jackknife resampling to estimate uncertainties.\n\n    Parameters:\n    df : pandas.DataFrame\n        The DataFrame containing galaxy data, including 'RA', 'DEC', and 'redshift_observed' columns.\n    grid_cells : int, optional\n        Number of bins to divide the declination range into for jackknife resampling (default: 10).\n\n    Returns:\n    - mean_counts: List of mean galaxy counts per square degree for each redshift bin.\n    - std_dev_counts: List of jackknife uncertainties (standard deviations) for each redshift bin.\n    - z_bin_centers: central values of the redshift bins\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n\n    original_galaxy_count = len(df)\n    print(f\"Original galaxy count: {original_galaxy_count}\")\n\n\n\n    # compute RA edges based on the data\n    #keeping this in here in case we make bins over RA in the future\n    ra_min = df['RA_shifted'].min()\n    ra_max = df['RA_shifted'].max()\n    ra_edges = np.linspace(ra_min, ra_max, grid_cells + 1)\n\n    # Set up Declination bins for constant declination binning\n    #manually fixing these here for this particular survey\n    dec_min =  -20# df['DEC'].min()\n    dec_max =  20# df['DEC'].max()\n    dec_bins = np.linspace(dec_min, dec_max, grid_cells + 1)  # Create declination bins\n\n    # Calculate bin areas for constant declination bins\n    ra_width = np.diff(ra_edges)[0]  # Width of RA bins (all bins assumed equal width)\n    bin_areas = get_bin_area(dec_bins, ra_width)\n\n    # Initialize lists\n    mean_counts = []\n    std_dev_counts = []\n    sum_counts = []\n\n    # Loop over the redshift bins\n    for z_bin in df['z_bin'].cat.categories:\n        # Step 8: Select galaxies within the current redshift bin\n        df_z_bin = df[df['z_bin'] == z_bin].copy()\n\n        # Step 9: Bin galaxies into Declination bins using precomputed dec_bins\n        # Explicitly cast the 'ra_bin' column to 'category' before assignment\n        df_z_bin['dec_bin'] = pd.cut(df_z_bin['DEC'], bins=dec_bins, include_lowest=True, right=False).astype('category')\n\n        #df_z_bin.loc[:, 'dec_bin'] = pd.cut(df_z_bin['DEC'], bins=dec_bins, include_lowest=True, right=False)\n        #df_z_bin.loc[:, 'dec_bin'] = df_z_bin['dec_bin'].astype('category')  # Use .loc to avoid SettingWithCopyWarning\n\n        # Step 10: Count galaxies in each Declination bin\n        galaxy_counts = df_z_bin.groupby('dec_bin', observed=False).size().values\n\n        # Step 11: Calculate mean galaxy counts per square degree\n        counts_per_deg2 = galaxy_counts / bin_areas\n\n        # Step 12: Compute jackknife resampling to estimate uncertainties\n        jackknife_means = []\n        for i in range(len(counts_per_deg2)):\n            jackknife_sample = np.delete(counts_per_deg2, i)\n            jackknife_means.append(np.mean(jackknife_sample))\n\n        # Step 13: Calculate mean and standard deviation of galaxy counts per square degree\n        mean_counts.append(np.mean(counts_per_deg2))\n        std_dev_counts.append(np.std(jackknife_means))\n\n    return mean_counts, std_dev_counts\n\n\n\n\n\ndef plot_galaxy_counts_vs_redshift_with_jackknife(counts, std_dev_counts, z_bin_centers):\n    \"\"\"\n    Plot the galaxy counts per square degree as a function of redshift with jackknife error bars.\n\n    Parameters:\n    - counts (numpy.ndarray): Array of galaxy counts per square degree for each redshift bin.\n      Shape: (number of redshift bins,) or (1, number of redshift bins).\n    - std_dev_counts (numpy.ndarray): Array of jackknife standard deviations for each redshift bin,\n      representing uncertainties. Shape: (number of redshift bins,).\n    - z_bin_centers (numpy.ndarray): Array of central values of the redshift bins.\n      Shape: (number of redshift bins,).\n\n    Notes:\n    - The input arrays (`counts`, `std_dev_counts`, and `z_bin_centers`) must have the same length,\n      corresponding to the number of redshift bins.\n    - The plot is designed specifically for redshifts in the range of 1.0 to 3.0 and may need adjustments\n      for datasets with different redshift ranges.\n    \"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Ensure counts is a NumPy array\n    if isinstance(counts, list):\n        counts = np.array(counts)\n\n    # Flatten counts if it has a shape of [1, N]\n    if counts.ndim == 2 and counts.shape[0] == 1:\n        counts = counts.flatten()\n\n    # setup figure\n    plt.figure(figsize=(10, 6))\n\n    # Plot with error bars (with jackknife standard deviation as error bars)\n    plt.errorbar(z_bin_centers, counts, yerr=std_dev_counts, fmt='o', color='blue',\n                 ecolor='gray', elinewidth=2, capsize=3, label='Galaxy Counts per Square Degree (Jackknife)')\n    plt.yscale('log')\n\n    # Set x-range to redshifts between 1 and 3\n    plt.xlim(1, 3.0)\n    plt.ylim(1E1, 3E5)\n\n    # Add labels and title\n    plt.xlabel('Redshift', fontsize=12)\n    plt.ylabel('Galaxy Count per Square Degree', fontsize=12)\n    plt.title('Galaxy Counts per Square Degree vs Redshift with Jackknife Error Bars', fontsize=14)\n\n    # Show the plot\n    plt.legend()\n    plt.show()\n    plt.close()\n\n\n\n\n\n#setup redshift binning here for consistency in the rest of the code\n#these match our dataset, so only change if you want to reduce the range\n# or if you change the dataset\n\nz_min = 1.0\nz_max = 3.0\ndz = 0.1\n\n#do the counting\nmean_counts, std_dev_counts = jackknife_galaxy_count(df, grid_cells=10)\n#make the plot\nplot_galaxy_counts_vs_redshift_with_jackknife(mean_counts, std_dev_counts, z_bin_centers)\n\n\n\n\n\nFigure Caption: Number density plot.  Note that the jackknife error bars are displayed on the plot but they are smaller than the point size.  We see the expected shape of this plot, with a fall off toward higher redshift.\n\n#cleanup memory when done with this file:\ndel df\n\n\n\n","type":"content","url":"/roman-hlss-number-density#id-3-0-make-number-density-plot","position":27},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"4.0 Expand to out-of-memory sized catalogs"},"type":"lvl2","url":"/roman-hlss-number-density#id-4-0-expand-to-out-of-memory-sized-catalogs","position":28},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"4.0 Expand to out-of-memory sized catalogs"},"content":"Originally this section attempted to use dask instead of pandas to hold the data and be able to do this work.  Dask is supposed to be able to do this sort of magic in the background with out of memory catalogs, but in reality, this does not seem to be possible, or at least not possible with a week or so of working on it.  Instead, we will read in the catalogs one at a time, store counts of mean and standard deviation per square degree and then move on to the next catalog.\n\ndef calculate_area(df):\n    \"\"\"\n    Calculate the total survey area in square degrees based on DEC range and RA width.\n\n    Parameters:\n    - df: The DataFrame containing galaxy data.\n\n    Returns:\n    - area: Total survey area in square degrees.\n    \"\"\"\n    import numpy as np\n\n    ra_min = df['RA_shifted'].min()\n    ra_max = df['RA_shifted'].max()\n    dec_min = df['DEC'].min()\n    dec_max = df['DEC'].max()\n\n    # Step 4: Calculate area\n    ra_width = ra_max - ra_min\n    dec_height = dec_max - dec_min\n\n    # Approximate area calculation\n    dec_center = (dec_min + dec_max) / 2\n    area = ra_width * dec_height * np.cos(np.radians(dec_center))\n\n    return area\n\n\n\n\n\ndef galaxy_counts_per_hdf5_binned(\n    df,\n    z_bins,\n    z_bin_centers,\n    find_halpha_bins=False,\n    halpha_flux_thresholds=None,\n):\n    \"\"\"\n    Calculate the galaxy counts per square degree for each redshift bin.\n    Optionally calculate counts for Halpha flux bins.\n\n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing galaxy data.\n    - zbins (): edges of the redshift bins\n    - z_bin_centers (numpy.ndarray): Array of central values of the redshift bins.\n    - find_halpha_bins (bool, optional): Whether to calculate counts for Halpha bins (default: False).\n    - halpha_flux_thresholds (list, optional): Custom thresholds for Halpha flux binning. If None, default thresholds are used.\n\n    Returns:\n    - counts_per_deg2 (numpy.ndarray): Array of galaxy counts per square degree for each redshift bin.\n    - halpha_counts (numpy.ndarray): Counts for each Halpha bin (if `find_halpha_bins` is True).\n    \"\"\"\n    import numpy as np\n\n    # Default Halpha flux thresholds if not provided\n    if find_halpha_bins and halpha_flux_thresholds is None:\n        halpha_flux_thresholds = [0.5e-16, 0.7e-16, 0.9e-16, 1.1e-16, 1.3e-16, 1.5e-16, 1.7e-16, 1.9e-16]\n\n    # Calculate survey area\n    area = calculate_area(df)\n\n    # Initialize counts\n    counts = np.zeros((1, len(z_bins) - 1))\n    halpha_counts = None\n\n    # Prepare for Halpha binning if requested\n    if find_halpha_bins:\n        halpha_counts = np.zeros((len(halpha_flux_thresholds), len(z_bins) - 1))\n\n    # Group data by redshift bin\n    grouped = df.groupby(\"z_bin\", observed= False)\n\n    for i, (z_bin, group) in enumerate(grouped):\n        # Count galaxies in the redshift bin\n        galaxy_count = len(group)\n        counts[0, i] = galaxy_count\n\n        if find_halpha_bins:\n            # Count galaxies for each Halpha bin within the current redshift bin\n            for j, threshold in enumerate(halpha_flux_thresholds):\n                halpha_count = len(group[group[\"flux_Halpha6563\"] > threshold])\n                halpha_counts[j, i] = halpha_count\n                #print(f\"redshift: {z_bin}, threshold: {threshold}, halpha_count: {halpha_count}, count: {galaxy_count}\")\n\n\n    # Sort results based on z_bin_centers\n    sorted_indices = np.argsort(z_bin_centers)\n    counts_per_deg2 = (counts / area)[:, sorted_indices]\n\n    if find_halpha_bins:\n        # Normalize Halpha counts to counts per square degree\n        halpha_counts_per_deg2 = (halpha_counts / area)[:, sorted_indices]\n    else:\n        halpha_counts_per_deg2 = None\n\n    return counts_per_deg2, halpha_counts_per_deg2\n\n\n\n\n\ndef plot_binned_galaxy_counts_vs_redshift_with_jackknife(\n    counts_summary, z_bin_centers, halpha_flux_thresholds, halpha_summary,\n    ):\n    \"\"\"\n    Plots galaxy counts per square degree as a function of redshift with jackknife error bars.\n\n    This function visualizes the galaxy number density using binned redshift data. It includes\n    jackknife-based uncertainty estimates and optionally plots counts for Hα flux-selected bins.\n\n    Parameters:\n    - counts_summary (dict): A dictionary containing:\n        - 'sum_counts' (numpy.ndarray): Array of total galaxy counts per square degree for each redshift bin.\n        - 'std_dev_counts' (numpy.ndarray): Array of jackknife standard deviations for each redshift bin.\n    - z_bin_centers (numpy.ndarray): Array of central values of the redshift bins.\n    - halpha_flux_thresholds (list, optional): List of Hα flux threshold values (used for labeling the lines).\n    - halpha_summary (dict, optional): A dictionary containing:\n        - 'halpha_sum_counts' (numpy.ndarray): Counts for each Hα flux-selected bin.\n        - 'halpha_std_dev_counts' (numpy.ndarray): Jackknife standard deviations for each Hα flux-selected bin.\n\n    Notes:\n    - The input arrays in `counts_summary` and `halpha_summary` must have the same length as `z_bin_centers`.\n    - The function automatically sets the y-axis to a logarithmic scale (`plt.yscale(\"log\")`).\n    - The x-axis is limited to the redshift range [1, 3].\n    - The function ensures proper memory management by explicitly closing the plot.\n    \"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Unpack counts_summary\n    counts = counts_summary[\"sum_counts\"]\n    std_dev_counts = counts_summary[\"std_dev_counts\"]\n\n    # Unpack halpha_summary if provided, otherwise set to None\n    if halpha_summary is not None:\n        halpha_counts = halpha_summary.get(\"halpha_sum_counts\")\n        halpha_std_dev_counts = halpha_summary.get(\"halpha_std_dev_counts\")\n    else:\n        halpha_counts = None\n        halpha_std_dev_counts = None\n\n    # Ensure counts is a NumPy array\n    if isinstance(counts, list):\n        counts = np.array(counts)\n\n    # Flatten counts if it has a shape of [1, N]\n    if counts.ndim == 2 and counts.shape[0] == 1:\n        counts = counts.flatten()\n\n    # Setup figure\n    plt.figure(figsize=(10, 6))\n\n    # Plot the main counts with error bars\n    jackknife_line = plt.errorbar(\n        z_bin_centers, counts, yerr=std_dev_counts, fmt=\"o-\", color=\"blue\",\n        ecolor=\"blue\", elinewidth=2, capsize=3, label=\"All Galaxy Counts per Square Degree (Jackknife)\"\n    )\n\n    # Plot Hα bins if the data is present\n    halpha_lines = []\n    if halpha_flux_thresholds is not None and halpha_counts is not None:\n        for i in range(len(halpha_flux_thresholds)):\n            label = f\"Flux Hα > {halpha_flux_thresholds[i]:.1e} erg/s/cm²\"\n            line, = plt.plot(z_bin_centers, halpha_counts[i], marker=\"o\", label=label)\n            halpha_lines.append(line)\n\n    # Adjust legend order if Hα lines were added\n    if halpha_lines:\n        handles, labels = plt.gca().get_legend_handles_labels()\n        new_order = [jackknife_line] + halpha_lines\n        plt.legend(new_order, [labels[handles.index(h)] for h in new_order])\n    else:\n        plt.legend()\n\n    # Set log scale for y-axis and limit the x-axis\n    plt.yscale(\"log\")\n    plt.xlim(1, 3.0)\n\n    # Add labels and title\n    plt.xlabel(\"Redshift\", fontsize=12)\n    plt.ylabel(\"Galaxy Count per Square Degree\", fontsize=12)\n    plt.title(\"Number Density with Jackknife Error Bars\", fontsize=14)\n\n    # Display the plot\n    plt.show()\n    plt.close()\n\n\n\n\n\ndef jackknife_wrapper(file_list,  halpha_flux_thresholds, find_halpha_bins=False):\n    \"\"\"\n    Performs jackknife resampling over multiple HDF5 files to estimate uncertainties\n    in galaxy counts per square degree for each redshift bin. Optionally calculates\n    counts for Hα flux-selected bins.\n\n    Parameters:\n    - file_list (list of str): List of file paths to HDF5 datasets to process.\n    - halpha_flux_thresholds (list): List of Hα flux thresholds used for binning (if `find_halpha_bins` is True).\n    - find_halpha_bins (bool, optional): Whether to calculate counts for Hα bins (default: False).\n\n    Returns:\n    - counts_summary (dict): A dictionary containing:\n        - 'sum_counts' (numpy.ndarray): Summed galaxy counts across all HDF5 files for each redshift bin.\n          Shape: (number of redshift bins,).\n        - 'std_dev_counts' (numpy.ndarray): Jackknife standard deviations for galaxy counts per square degree.\n          Shape: (number of redshift bins,).\n       Shape: (number of redshift bins,).\n    - halpha_summary (dict or None): A dictionary containing Hα bin statistics if `find_halpha_bins` is True, otherwise `None`.\n      When present, the dictionary includes:\n        - 'halpha_sum_counts' (numpy.ndarray): Summed counts for each Hα bin.\n          Shape: (number of Hα bins, number of redshift bins).\n        - 'halpha_std_dev_counts' (numpy.ndarray): Jackknife standard deviations for Hα bins.\n          Shape: (number of Hα bins, number of redshift bins).\n\n    Notes:\n    - The function loads multiple HDF5 files and extracts galaxy counts using `galaxy_counts_per_hdf5_binned()`.\n    - Jackknife resampling is applied across files to compute standard deviations.\n    - The function saves `all_counts` to a NumPy file for debugging purposes.\n    - If `find_halpha_bins` is False, `halpha_summary` is returned as `None` to indicate that Hα binning was not performed.\n    \"\"\"\n    import os\n    import numpy as np\n\n    #fix these in case the above cells are not run\n    ra_min1=330\n    z_min = 1.0\n    z_max = 3.0\n    dz = 0.1\n    columns_to_keep = ['RA', 'DEC', 'redshift_observed', 'flux_Halpha6563']\n    columns_to_convert = ['redshift_observed', 'flux_Halpha6563']\n\n    #setup to track counts\n    all_counts = []\n    all_halpha_counts = [] if find_halpha_bins else None\n\n    for file in file_list:\n        # Load data from the file\n        df = read_hdf5_to_pandas(file, columns_to_keep, columns_to_convert)\n\n        #pre-process dataset\n        df, z_bins, z_bin_centers = assign_redshift_bins(df, z_min, z_max, dz)\n        df['RA_shifted'] = (df['RA'] - ra_min1) % 360  # Shift RA to continuous range\n\n        # Get counts (with or without Halpha binning)\n        result = galaxy_counts_per_hdf5_binned(\n            df, z_bins, z_bin_centers, find_halpha_bins=find_halpha_bins,\n            halpha_flux_thresholds=halpha_flux_thresholds\n        )\n\n        counts_per_deg2, halpha_counts_per_deg2 = result\n        all_counts.append(counts_per_deg2.flatten())  #make sure shape is correct\n\n        if find_halpha_bins:\n            all_halpha_counts.append(halpha_counts_per_deg2)\n\n        # Explicitly delete DataFrame for improved memory management\n        del df\n\n    # Save all_counts to a file for debugging or post-processing\n    os.makedirs(\"output\", exist_ok=True)\n    np.save(\"output/all_counts.npy\", all_counts)\n\n    # Convert to NumPy array for easier manipulation\n    all_counts = np.array(all_counts)\n\n    #debugging\n    print(\"all_counts shape:\", all_counts.shape)\n    print(\"all_counts sample:\", all_counts[:3])  # preview first 3 entries\n    print(\"all_counts variance:\", np.var(all_counts, axis=0))\n    print(\"all_counts dtype:\", all_counts.dtype)\n\n    if find_halpha_bins:\n        all_halpha_counts = np.array(all_halpha_counts)\n\n    # Compute sum counts\n    sum_counts = np.sum(all_counts, axis=0)\n\n    # Compute jackknife standard deviations for galaxy counts\n    std_dev_counts = np.sqrt(\n        (len(all_counts) - 1) * np.var(all_counts, axis=0, ddof=0)\n    )\n\n    # Create counts summary dictionary\n    counts_summary = {\n        \"sum_counts\": sum_counts,\n        \"std_dev_counts\": std_dev_counts\n    }\n\n    # Compute jackknife standard deviations for Halpha counts (if applicable)\n    if find_halpha_bins:\n        halpha_sum_counts = np.sum(all_halpha_counts, axis=0)\n        halpha_std_dev_counts = np.sqrt(\n            (len(all_halpha_counts) - 1) * np.var(all_halpha_counts, axis=0, ddof=0)\n        )\n        # Create halpha counts summary dictionary\n        halpha_summary = {\n            \"halpha_sum_counts\": halpha_sum_counts,\n            \"halpha_std_dev_counts\": halpha_std_dev_counts\n        }\n    else:\n        halpha_summary = None  # No Halpha binning\n\n    return counts_summary, halpha_summary\n\n\n\n\n\n# This uses data downloaded in section 1 above.\n# If you want to run this with more data, return to section 1 to get that downloaded.\ndownload_dir = 'downloaded_hdf5_files'\n# Get all HDF5 files from the directory\nfile_paths = [os.path.join(download_dir, f) for f in os.listdir(download_dir) if f.endswith('.hdf5')]\n\n#out of consideration for the size of these files and the amount of memory required to work with them\n# we only keep the columns that we are going to use and convert some to 32bit instead of 64 where\n# the higher precision is not necessary to the science\ncolumns_to_keep = ['RA', 'DEC', 'redshift_observed', 'flux_Halpha6563']\ncolumns_to_convert = ['redshift_observed', 'flux_Halpha6563']\n\n# Run jackknife with Halpha binning enabled\nhalpha_flux_thresholds=[0.5e-16, 0.7e-16, 0.9e-16, 1.1e-16, 1.3e-16, 1.5e-16, 1.7e-16, 1.9e-16]\nresults = jackknife_wrapper(\n    file_paths, halpha_flux_thresholds, find_halpha_bins=True\n)\n\n# Unpack results\ncounts_summary, halpha_summary = results\n\n# Plot the results\nplot_binned_galaxy_counts_vs_redshift_with_jackknife(\n    counts_summary, z_bin_centers, halpha_flux_thresholds, halpha_summary)\n\n\n\n\n\nFigure Caption: Number Density plots color coded by Halpha flux.  Slight variations are noted in the shape of the curves as a function of Halpha flux, especially at higher redshifts.  This plot may be a factor of 10 below the section 3 number density plot if you have not let it download all 10 input files.  The 10 input files cover the same area on the sky, so the density will look smaller if fewer files are used.\n\n","type":"content","url":"/roman-hlss-number-density#id-4-0-expand-to-out-of-memory-sized-catalogs","position":29},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"4.1 Explore jackknife uncertainties","lvl2":"4.0 Expand to out-of-memory sized catalogs"},"type":"lvl3","url":"/roman-hlss-number-density#id-4-1-explore-jackknife-uncertainties","position":30},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl3":"4.1 Explore jackknife uncertainties","lvl2":"4.0 Expand to out-of-memory sized catalogs"},"content":"The uncertainties are plotted as error bars on the data points above.  There are no jackknife uncertainties if you are only using one file, so they will in that case not be visible on the plot.  This section is for the case where you are using more than one downloaded input file.  Here we explore and plot the jackknife uncertainties in a way that makes them visible.\n\ndef plot_jackknife_fractional_uncertainty(z_bin_centers, counts, std_dev_counts):\n    \"\"\"\n    Plots the fractional jackknife uncertainty (σ / N) as a function of redshift.\n\n    Parameters\n    ----------\n    z_bin_centers : array-like\n        The center of each redshift bin.\n    counts : array-like\n        The total galaxy counts per redshift bin.\n    std_dev_counts : array-like\n        The jackknife standard deviation per redshift bin.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    fractional_uncertainty = std_dev_counts / counts\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(z_bin_centers, fractional_uncertainty, marker='o', linestyle='-', color='purple')\n    #plt.ylim(0, 0.01)\n    plt.xlabel('Redshift')\n    plt.ylabel('Fractional Uncertainty')\n    plt.title('Jackknife Fractional Uncertainty vs. Redshift')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\n\n\n\n#make fractional uncertainty plot\ncounts_per_deg2 = counts_summary[\"sum_counts\"].flatten()\nstd_dev_counts = counts_summary[\"std_dev_counts\"].flatten()\nplot_jackknife_fractional_uncertainty(z_bin_centers, counts_per_deg2, std_dev_counts)\n\n\n\nFigure Caption:  The fractional uncertainty is the standard deviation from jackknife sampling divided by the number of galaxies per square degree, here plotted as a function of redshift.  Note that if you are using only 1 downloaded file this will be a flat plot with no uncertainties.  However, if you are using all 10 input files, you will see the expected rise in uncertainty with increasing redshift.\n\n","type":"content","url":"/roman-hlss-number-density#id-4-1-explore-jackknife-uncertainties","position":31},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Acknowledgements"},"type":"lvl2","url":"/roman-hlss-number-density#acknowledgements","position":32},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"Acknowledgements"},"content":"IPAC-IRSA\n\n","type":"content","url":"/roman-hlss-number-density#acknowledgements","position":33},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"About this notebook"},"type":"lvl2","url":"/roman-hlss-number-density#about-this-notebook","position":34},{"hierarchy":{"lvl1":"Number Density as a Function of Redshift","lvl2":"About this notebook"},"content":"Authors: Jessica Krick in conjunction with the IPAC Science Platform Team\n\nContact: \n\nIRSA Helpdesk with questions\nor problems.\n\nUpdated: 2025-04-01\n\nRuntime: As of the date above, running on the \n\nFornax Science Platform, this notebook takes about 9 minutes to run to completion on\na machine with 8GB RAM and 4 CPU.\nThis runtime inlcudes the notebook as is, using only one datafile out of 10.  Runtime will be greater than 30 min. longer if the code is allowed to download and work with all 10 datafiles.","type":"content","url":"/roman-hlss-number-density#about-this-notebook","position":35},{"hierarchy":{"lvl1":"Simulated Data Tutorial Notebooks"},"type":"lvl1","url":"/simulated","position":0},{"hierarchy":{"lvl1":"Simulated Data Tutorial Notebooks"},"content":"IRSA hosts a diverse collection of simulated astronomical datasets spanning multiple missions and science domains; designed to support survey planning, algorithm development, and scientific exploration.\nBecause this collection is heterogeneous in coverage, structure, and intended use, the simulated products are released with detailed documentation.\nAccess methods are tailored to the structure and scale of each product.\nThese tutorials are designed to help users get started with accessing, visualizing, and analyzing simulated datasets hosted at IRSA.\n\nRoman HLSS Number Density - Illustrate how to query the catalog and derive galaxy number density.\n\nOpenUniverse2024 Roman Coadds - Access OpenUniverse2024 wide-area simulated survey data.\n\nOpenUniverse2024 Visualization - Use Firefly to get an overview of survey structure and visualize content.\n\nOpenUniverse2024 Time Domain - Access and analyze time-domain OpenUniverse2024 survey.\n\nCosmoDC2 Data Access - Access, query, and visualize the CosmoDC2 catalog.","type":"content","url":"/simulated","position":1},{"hierarchy":{"lvl1":"Spherex Tutorial Notebooks"},"type":"lvl1","url":"/spherex","position":0},{"hierarchy":{"lvl1":"Spherex Tutorial Notebooks"},"content":"SPHEREx (Spectro-Photometer for the History of the Universe, Epoch of Reionization, and Ices Explorer) is a NASA space mission designed to perform the first all-sky spectral survey in the near-infrared.\nSPHEREx observes the sky from roughly 0.75–5.0 µm using a single instrument that provides low-resolution spectroscopy (R ≈ 40–150) in hundreds of spectral channels for every point on the sky.\nIts science goals span cosmology, galaxy evolution, and the interstellar medium, enabling measurements of large-scale structure, the cosmic history of star formation, and the distribution of key molecules and ices in the Milky Way and nearby galaxies.\n\nSPHEREx data releases include weekly \n\nQuick Release spectral image products (multi-extension FITS files containing calibrated near-infrared surface brightness, variance, flags, modeled backgrounds, PSFs, and wavelength WCS) along with ancillary calibration and metadata files such as gain matrices, dark current maps, solid angle pixel maps, and detailed spectral WCS products for each detector.\n\nData Overview - Identify available data products and select the appropriate FITS extensions for different use cases.\n\nSpectral Image Cutouts - Generate and work with spatial and spectral cutouts.\n\nPSF Models - Understand how SPHEREx point spread function (PSF) information is organized and accessed.","type":"content","url":"/spherex","position":1},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file"},"type":"lvl1","url":"/spherex-cutouts","position":0},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file"},"content":"","type":"content","url":"/spherex-cutouts","position":1},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"1. Learning Goals"},"type":"lvl2","url":"/spherex-cutouts#id-1-learning-goals","position":2},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"1. Learning Goals"},"content":"Perform a query for the list of SPHEREx Spectral Image Multi-Extension FITS files (MEFs) that overlap a given coordinate.\n\nRetrieve cutouts for every entry in this list and package the cutouts as a new MEF.\n\nLearn how to use parallel or serial processing to retrieve the cutouts.","type":"content","url":"/spherex-cutouts#id-1-learning-goals","position":3},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"2. SPHEREx Overview"},"type":"lvl2","url":"/spherex-cutouts#id-2-spherex-overview","position":4},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"2. SPHEREx Overview"},"content":"SPHEREx is a NASA Astrophysics Medium Explorer mission that launched in March 2025.\nDuring its planned two-year mission, SPHEREx will obtain 0.75-5 micron spectroscopy over the entire sky, with deeper data in the SPHEREx Deep Fields.\nSPHEREx data will be used to:\n\nconstrain the physics of inflation by measuring its imprints on the three-dimensional large-scale distribution of matter,\n\ntrace the history of galactic light production through a deep multi-band measurement of large-scale clustering,\n\ninvestigate the abundance and composition of water and biogenic ices in the early phases of star and planetary disk formation.\n\nThe community will also mine SPHEREx data and combine it with synergistic data sets to address a variety of additional topics in astrophysics.\n\nMore information is available in the \n\nSPHEREx Explanatory Supplement.","type":"content","url":"/spherex-cutouts#id-2-spherex-overview","position":5},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"3. Imports"},"type":"lvl2","url":"/spherex-cutouts#id-3-imports","position":6},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"3. Imports"},"content":"The following packages must be installed to run this notebook.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install numpy astropy pyvo matplotlib\n\n\n\nimport concurrent.futures\nimport time\n\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pyvo\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.wcs import WCS\n\n# Suppress logging temporarily to prevent astropy\n# from repeatedly printing out warning notices related to alternate WCSs\nimport logging\nlogging.getLogger('astropy').setLevel(logging.ERROR)\n\n# The time it takes to read SPHEREx files can exceed\n# astropy's default timeout limit. Increase it.\nfrom astropy.utils.data import conf\nconf.remote_timeout = 120\n\n\n\n","type":"content","url":"/spherex-cutouts#id-3-imports","position":7},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"4. Specify inputs and outputs"},"type":"lvl2","url":"/spherex-cutouts#id-4-specify-inputs-and-outputs","position":8},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"4. Specify inputs and outputs"},"content":"Specify a right ascension, declination, cutout size, and a SPHEREx bandpass (e.g., ‘SPHEREx-D2’).\n\nIn this example, we are creating cutouts of the Pinwheel galaxy (M101) for the SPHEREx detector D2.\n\n# Choose a position.\nra = 210.80227 * u.degree\ndec = 54.34895 * u.degree\n\n# Choose a cutout size.\nsize = 0.1 * u.degree\n\n# Choose the bandpass of interest.\nbandpass = 'SPHEREx-D2'\n\n# Choose an output filename root for the output MEF file.\noutput_filename = 'spherex_cutouts_mef.fits'\n\n\n\n","type":"content","url":"/spherex-cutouts#id-4-specify-inputs-and-outputs","position":9},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"5. Query IRSA for a list of cutouts that satisfy the criteria specified above."},"type":"lvl2","url":"/spherex-cutouts#id-5-query-irsa-for-a-list-of-cutouts-that-satisfy-the-criteria-specified-above","position":10},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"5. Query IRSA for a list of cutouts that satisfy the criteria specified above."},"content":"Here we show how to use the pyvo TAP SQL query to retrieve all images that overlap with the position defined above.\nThis query will retrieve a table of URLs that link to the MEF cutouts.\nEach row in the table corresponds to a single cutout and includes the data access URL and an observation timestamp.\nThe results are sorted from oldest to newest.\n\n# Define the service endpoint for IRSA's Table Access Protocol (TAP)\n# so that we can query SPHEREx metadata tables.\nservice = pyvo.dal.TAPService(\"https://irsa.ipac.caltech.edu/TAP\")\n\n# Define a query that will search the appropriate SPHEREx metadata tables\n# for spectral images that cover the chosen coordinates and match the\n# specified bandpass. Return the cutout data access URL and the time of observation.\n# Sort by observation time.\nquery = f\"\"\"\nSELECT\n    'https://irsa.ipac.caltech.edu/' || a.uri || '?center={ra.value},{dec.value}d&size={size.value}' AS uri,\n    p.time_bounds_lower\nFROM spherex.artifact a\nJOIN spherex.plane p ON a.planeid = p.planeid\nWHERE 1 = CONTAINS(POINT('ICRS', {ra.value}, {dec.value}), p.poly)\n        AND p.energy_bandpassname = '{bandpass}'\nORDER BY p.time_bounds_lower\n\"\"\"\n\n# Execute the query and return as an astropy Table.\nt1 = time.time()\nresults = service.search(query)\nprint(\"Time to do TAP query: {:2.2f} seconds.\".format(time.time() - t1))\nprint(\"Number of images found: {}\".format(len(results)))\n\n\n\nNote\n\nSPHEREx data are also available via SIA which can provide a simpler interface for many queries, as demonstrated in \n\nIntroduction to SPHEREx Spectral Images.\nAn advantage of the method shown above is that it provides access to data immediately after ingestion (which occurs weekly) and is not subject to the same ~1 day delay as SIA.","type":"content","url":"/spherex-cutouts#id-5-query-irsa-for-a-list-of-cutouts-that-satisfy-the-criteria-specified-above","position":11},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"6. Define a function that processes a list of SPHEREx Spectral Image Cutouts"},"type":"lvl2","url":"/spherex-cutouts#id-6-define-a-function-that-processes-a-list-of-spherex-spectral-image-cutouts","position":12},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"6. Define a function that processes a list of SPHEREx Spectral Image Cutouts"},"content":"This function takes in a row of the catalog that we created above and does the following:\n\nIt downloads the cutout\n\nIt computes the wavelength of the center pixel of the cutout (in micro-meters)\n\nIt combines the image HDUs into a new HDU and adds it to the table row.\n\nNote that the values of the rows are being added in place.\n\ndef process_cutout(row, ra, dec, cache):\n    '''\n    Downloads the cutouts given in a row of the table including all SPHEREx images overlapping with a position.\n\n    Parameters:\n    ===========\n\n    row : astropy.table row\n        Row of a table that will be changed in place by this function. The table\n        is created by the SQL TAP query.\n    ra,dec : coordinates (astropy units)\n        Ra and Dec coordinates (same as used for the TAP query) with attached astropy units\n    cache : bool\n        If set to `True`, the output of cached and the cutout processing will run faster next time.\n        Turn this feature off by setting `cache = False`.\n    '''\n\n    with fits.open(row[\"uri\"], cache=cache) as hdulist:\n        # There are seven HDUs:\n        # 0 contains minimal metadata in the header and no data.\n        # 1 through 6 are: IMAGE, FLAGS, VARIANCE, ZODI, PSF, WCS-WAVE\n        header = hdulist[1].header\n\n        # Compute pixel coordinates corresponding to cutout position.\n        spatial_wcs = WCS(header)\n        x, y = spatial_wcs.world_to_pixel(SkyCoord(ra=ra, dec=dec, unit=\"deg\", frame=\"icrs\"))\n\n        # Compute wavelength at cutout position.\n        spectral_wcs = WCS(header, fobj=hdulist, key=\"W\")\n        spectral_wcs.sip = None\n        wavelength, bandwidth = spectral_wcs.pixel_to_world(x, y)\n        row[\"central_wavelength\"] = wavelength.to(u.micrometer).value\n\n        # Collect the HDUs for this cutout and append the row's cutout_index to the EXTNAME.\n        hdus = []\n        for hdu in hdulist[1:]:  # skip the primary header\n            hdu.header[\"EXTNAME\"] = f\"{hdu.header['EXTNAME']}{row['cutout_index']}\"\n            hdus.append(hdu.copy())  # Copy so the data is available after the file is closed\n        row[\"hdus\"] = hdus\n\n\n\n","type":"content","url":"/spherex-cutouts#id-6-define-a-function-that-processes-a-list-of-spherex-spectral-image-cutouts","position":13},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"7. Download the Cutouts"},"type":"lvl2","url":"/spherex-cutouts#id-7-download-the-cutouts","position":14},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"7. Download the Cutouts"},"content":"This process can take a while.\nIf run in series, it can take about 5 minutes for 700 images on a typical laptop machine.\nHere, we therefore exploit two different methods.\nFirst we show the serial approach and next we show how to parallelize the methods.\nThe latter can be run on many CPUs and is therefore significantly faster.","type":"content","url":"/spherex-cutouts#id-7-download-the-cutouts","position":15},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl3":"7.1 Serial Approach","lvl2":"7. Download the Cutouts"},"type":"lvl3","url":"/spherex-cutouts#id-7-1-serial-approach","position":16},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl3":"7.1 Serial Approach","lvl2":"7. Download the Cutouts"},"content":"First, we implement the serial approach -- a simple for loop.\nBefore that, we turn the results into an astropy table and add some place holders that will be filled in by the process_cutout() function.\n\nWarning\n\nRunning the cell below may take a while for a large number of cutouts.\nApproximately 5-7 minutes for 700 images of cutout size 0.01 degree on a typical machine.\n\nTip\n\nThe astropy fits.open() supports a caching argument.\nThis can be passed through in the process_cutout() function.\nIf cache=True is set, the images are cached and the cutout creation is sped up next time the code is run (even if the Jupyter kernel is restarted!).\nThe downside is that the images are saved on the machine where this notebook is run (usually in ~/.astropy/cache/).\nIf many cutouts are created, this can sum up to a large cached data volume, in which case cache=False is preferred.\n\nTo learn more about the cache please read the \n\nastropy cache management documentation.\n\nresults_table_serial = results.to_table()\nresults_table_serial[\"cutout_index\"] = range(1, len(results_table_serial) + 1)\nresults_table_serial[\"central_wavelength\"] = np.full(len(results_table_serial), np.nan)\nresults_table_serial[\"hdus\"] = np.full(len(results_table_serial), None)\n\nt1 = time.time()\nfor row in results_table_serial:\n    process_cutout(row, ra, dec, cache=False)\nprint(\"Time to create cutouts in serial mode: {:2.2f} minutes.\".format((time.time() - t1) / 60))\n\n\n\n","type":"content","url":"/spherex-cutouts#id-7-1-serial-approach","position":17},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl3":"7.2 Parallel Approach","lvl2":"7. Download the Cutouts"},"type":"lvl3","url":"/spherex-cutouts#id-7-2-parallel-approach","position":18},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl3":"7.2 Parallel Approach","lvl2":"7. Download the Cutouts"},"content":"Next, we implement parallel processing, which will make the cutout creation faster.\nThe maximal number of workers can be limited by setting the max_workers argument.\nThe choice of this value depends on the number of cores but also on the number of parallel calls that can be digested by the IRSA server.\n\nTip\n\nA good value for the maximum number of workers is between 7 and 12 for a machine with 8 cores.\n\nTip\n\nThe astropy fits.open() supports a caching argument.\nThis can be passed through in the process_cutout() function.\nIf cache=True is set, the images are cached and the cutout creation is sped up next time the code is run (even if the Jupyter kernel is restarted!).\nThe downside is that the images are saved on the machine where this notebook is run (usually in ~/.astropy/cache/).\nIf many cutouts are created, this can sum up to a large cached data volume, in which case cache=False is preferred.\n\nTo learn more about the cache please read the \n\nastropy cache management documentation.\n\nAgain, before running the cutout processing we define some place holders.\n\nresults_table_parallel = results.to_table()\nresults_table_parallel[\"cutout_index\"] = range(1, len(results_table_parallel) + 1)\nresults_table_parallel[\"central_wavelength\"] = np.full(len(results_table_parallel), np.nan)\nresults_table_parallel[\"hdus\"] = np.full(len(results_table_parallel), None)\n\nt1 = time.time()\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    futures = [executor.submit(process_cutout, row, ra, dec, False) for row in results_table_parallel]\n    concurrent.futures.wait(futures)\nprint(\"Time to create cutouts in parallel mode: {:2.2f} minutes.\".format((time.time() - t1) / 60))\n\n\n\n","type":"content","url":"/spherex-cutouts#id-7-2-parallel-approach","position":19},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"8. Create a summary table HDU with renamed columns"},"type":"lvl2","url":"/spherex-cutouts#id-8-create-a-summary-table-hdu-with-renamed-columns","position":20},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"8. Create a summary table HDU with renamed columns"},"content":"In the following, we continue to use the output of the parallel mode.\nThe following cell does the following:\n\nCreate a summary FITS table.\n\nCreate the final FITS HDU including the summary table.\n\n# Create a summary table HDU with renamed columns\ncols = fits.ColDefs([\n    fits.Column(name=\"cutout_index\", format=\"J\", array=results_table_parallel[\"cutout_index\"], unit=\"\"),\n    fits.Column(name=\"observation_date\", format=\"D\", array=results_table_parallel[\"time_bounds_lower\"], unit=\"d\"),\n    fits.Column(name=\"central_wavelength\", format=\"D\", array=results_table_parallel[\"central_wavelength\"], unit=\"um\"),\n    fits.Column(name=\"access_url\", format=\"A200\", array=results_table_parallel[\"uri\"], unit=\"\"),\n])\ntable_hdu = fits.BinTableHDU.from_columns(cols)\ntable_hdu.header[\"EXTNAME\"] = \"CUTOUT_INFO\"\n\n\n\n","type":"content","url":"/spherex-cutouts#id-8-create-a-summary-table-hdu-with-renamed-columns","position":21},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"9. Create the final MEF"},"type":"lvl2","url":"/spherex-cutouts#id-9-create-the-final-mef","position":22},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"9. Create the final MEF"},"content":"Now, we create a primary HDU and combine it with the summary table HDU and the cutout image and wavelength HDUs to a final HDU list.\n\nprimary_hdu = fits.PrimaryHDU()\nhdulist_list = [primary_hdu, table_hdu]\nhdulist_list.extend(hdu for fits_hdulist in results_table_parallel[\"hdus\"] for hdu in fits_hdulist)\ncombined_hdulist = fits.HDUList(hdulist_list)\n\n\n\n","type":"content","url":"/spherex-cutouts#id-9-create-the-final-mef","position":23},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"10. Write the final MEF"},"type":"lvl2","url":"/spherex-cutouts#id-10-write-the-final-mef","position":24},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"10. Write the final MEF"},"content":"Finally, we save the full HDU list to a multi-extension FITS file to disk.\n\n# Write the final MEF\ncombined_hdulist.writeto(output_filename, overwrite=True)\n\n\n\n","type":"content","url":"/spherex-cutouts#id-10-write-the-final-mef","position":25},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"11. Test and visualize the final result"},"type":"lvl2","url":"/spherex-cutouts#id-11-test-and-visualize-the-final-result","position":26},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"11. Test and visualize the final result"},"content":"We can now open the new MEF FITS file that we have created above.\n\nLoading the summary table (including wavelength information of each cutout) is straight forward:\n\nsummary_table = Table.read(output_filename , hdu=1)\n\n\n\nLet’s also extract the first 10 images (or all of the extracted cutouts if less than 10).\n\nnbr_images = np.nanmin([10, len(summary_table)])\nimgs = []\nwith fits.open(output_filename) as hdul:\n    for ii in range(nbr_images):\n        extname = \"IMAGE{}\".format(summary_table[\"cutout_index\"][ii])\n        imgs.append(hdul[extname].data)\n\n\n\nPlot the images with the wavelength corresponding to their central pixel.\n\nfig = plt.figure(figsize=(15, 5))\naxs = [fig.add_subplot(2, 5, ii + 1) for ii in range(10)]\n\nfor ii, img in enumerate(imgs):\n    axs[ii].imshow(imgs[ii], norm=\"log\", origin=\"lower\")\n    axs[ii].text(0.05, 0.05, r\"$\\lambda_{\\rm center} = %2.4g \\,{\\rm \\mu m}$\" % summary_table[\"central_wavelength\"][ii],\n                 va=\"bottom\", ha=\"left\", color=\"white\", transform=axs[ii].transAxes)\n\nplt.show()\n\n\n\n","type":"content","url":"/spherex-cutouts#id-11-test-and-visualize-the-final-result","position":27},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"Acknowledgements"},"type":"lvl2","url":"/spherex-cutouts#acknowledgements","position":28},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"Acknowledgements"},"content":"Caltech/IPAC-IRSA","type":"content","url":"/spherex-cutouts#acknowledgements","position":29},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"About this notebook"},"type":"lvl2","url":"/spherex-cutouts#about-this-notebook","position":30},{"hierarchy":{"lvl1":"Download a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file","lvl2":"About this notebook"},"content":"Authors: IRSA Data Science Team, including Vandana Desai, Andreas Faisst, Troy Raen, Brigitta Sipőcz, Jessica Krick, Shoubaneh Hemmati\n\nUpdated: 24 October 2025\n\nContact: \n\nIRSA Helpdesk with questions or problems.\n\nRuntime: As of the date above, this notebook takes about 3 minutes to run to completion on a machine with 8GB RAM and 4 CPU.","type":"content","url":"/spherex-cutouts#about-this-notebook","position":31},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images"},"type":"lvl1","url":"/spherex-intro","position":0},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images"},"content":"","type":"content","url":"/spherex-intro","position":1},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"1. Learning Goals"},"type":"lvl2","url":"/spherex-intro#id-1-learning-goals","position":2},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"1. Learning Goals"},"content":"Query for SPHEREx Spectral Image Multi-Extension FITS files (MEFs) that overlap a given coordinate on the sky.\n\nRead in a SPHEREx Spectral Image MEF using Astropy and understand its structure.\n\nInteractively visualize a SPHEREx Spectral Image MEF using Firefly.\n\nExplore each extension. For the Spectral Image extension, this includes understanding the astrometric and spectral WCS systems.\n\n","type":"content","url":"/spherex-intro#id-1-learning-goals","position":3},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"2. SPHEREx Overview"},"type":"lvl2","url":"/spherex-intro#id-2-spherex-overview","position":4},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"2. SPHEREx Overview"},"content":"SPHEREx is a NASA Astrophysics Medium Explorer mission that launched in March 2025. During its planned two-year mission, SPHEREx will obtain 0.75-5 micron spectroscopy over the entire sky, with deeper data in the SPHEREx Deep Fields. SPHEREx data will be used to:\n\nconstrain the physics of inflation by measuring its imprints on the three-dimensional large-scale distribution of matter,\n\ntrace the history of galactic light production through a deep multi-band measurement of large-scale clustering,\n\ninvestigate the abundance and composition of water and biogenic ices in the early phases of star and planetary disk formation.\n\nThe community will also mine SPHEREx data and combine it with synergistic data sets to address a variety of additional topics in astrophysics.\n\nMore information is available in the \n\nSPHEREx Explanatory Supplement.\n\n","type":"content","url":"/spherex-intro#id-2-spherex-overview","position":5},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"3. Requirements"},"type":"lvl2","url":"/spherex-intro#id-3-requirements","position":6},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"3. Requirements"},"content":"The following packages must be installed to run this notebook.\n\n# Uncomment the next line to install dependencies if needed.\n# %pip install numpy matplotlib astropy astroquery firefly-client\n\n\n\n","type":"content","url":"/spherex-intro#id-3-requirements","position":7},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"4. Imports"},"type":"lvl2","url":"/spherex-intro#id-4-imports","position":8},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"4. Imports"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom astropy.io import fits\nfrom astropy.wcs import WCS\nfrom astropy.table import Table\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\n\nfrom astroquery.ipac.irsa import Irsa\n\nfrom firefly_client import FireflyClient\n\n# The time it takes to read SPHEREx files can exceed\n# astropy's default timeout limit. Increase it.\nfrom astropy.utils.data import conf\nconf.remote_timeout = 120\n\n\n\n","type":"content","url":"/spherex-intro#id-4-imports","position":9},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"5. Search for SPHEREx Spectral Image MEFs that overlap coordinates you are interested in"},"type":"lvl2","url":"/spherex-intro#id-5-search-for-spherex-spectral-image-mefs-that-overlap-coordinates-you-are-interested-in","position":10},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"5. Search for SPHEREx Spectral Image MEFs that overlap coordinates you are interested in"},"content":"\n\nDefine some coordinates of interest.\n\nra_deg = 304.693508808\ndec_deg = 42.4436872991\n\ncoord = SkyCoord(ra_deg, dec_deg, unit='deg')\nsearch_radius = 1 * u.arcsec\n\n\n\nQuery IRSA for a list of Spectral Image MEFs that overlap this position. We use the \n\nIRSA module in astroquery and the Simple Image Access (SIA) API.\n\nTip\n\nThe IRSA SIA collections can be listed using using the list_collections method, we can filter on the ones containing “spherex” in the collection name:Irsa.list_collections(filter='spherex')\n\nThe collections are documented at \n\nSPHEREx Data Access: Application Program Interfaces (APIs)\nThere are currently three collections available for the second Quick Release:\n\n'spherex_qr2' -- Quick Release 2 Spectral Image MEFs that are part of the SPHEREx Wide Survey\n\n'spherex_qr2_cal' -- Quick Release 2 Calibration files\n\n'spherex_qr2_deep' -- Quick Release 2 Spectral Image MEFs that are part of the SPHEREx Deep Survey\n\nresults = Irsa.query_sia(pos=(coord, search_radius), collection='spherex_qr2')\n\n\n\nNote\n\nSPHEREx data are ingested on a weekly basis.\nDue to the nature of the ingestion process, availability via SIA will lag on the order of a day.\nTo avoid this delay, users can access data through the browsable directories or the SPHEREx Data Explorer GUI (see \n\nSPHEREx Data Access), or do a TAP query as shown in \n\nDownload a collection of SPHEREx Spectral Image cutouts as a multi-extension FITS file.\n\nEach row of the results of your query represents a different spectral image.\nBecause SPHEREx data will be released on a weekly basis, the number of rows returned will change\ndepending on when you submit the query.\nLet’s see how many images are returned today.\n\nlen(results)\n\n\n\nThe query results provide a lot of metadata about each spectral image. These columns have standard names as defined by the IVOA. Let’s list them:\n\nresults.colnames\n\n\n\nThe 'access_url' column is particularly important because it tells you how to access the data. Let’s look at the 'access_url' value for the first row:\n\nspectral_image_url = results['access_url'][0]\nprint(spectral_image_url)\n\n\n\nYou can put this URL into a browser to download the file. Or you can work with it in Python, as shown below.\n\n","type":"content","url":"/spherex-intro#id-5-search-for-spherex-spectral-image-mefs-that-overlap-coordinates-you-are-interested-in","position":11},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"6. Examine the header of one of the SPHEREx Spectral Image MEFs"},"type":"lvl2","url":"/spherex-intro#id-6-examine-the-header-of-one-of-the-spherex-spectral-image-mefs","position":12},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"6. Examine the header of one of the SPHEREx Spectral Image MEFs"},"content":"\n\nUse Astropy to examine the header of the URL from the previous step.\n\nhdulist = fits.open(spectral_image_url)\nhdulist.info()\n\n\n\nYou can see that the Level 2 Spectral Image files are multi-extension FITS files (MEFs) with the following extensions:\n\nIMAGE: Calibrated fluxes for a detector array in scientific units of MJy/sr.\n\nFLAGS: A bitmap with per-pixel status and processing flags.\n\nVARIANCE: A per-pixel estimate of the variance.\n\nZODI: An model of the zodiacal dust background signal. Note that this is not subtracted from the IMAGE extension.\n\nPSF: An image cube (3D array) where each plane represents a Point Spread Function (PSF) for a cutout in over-sampled pixel space.\n\nWCS-WAVE: Spectral WCS lookup table that maps pixel coordinates to the central wavelength and bandwidth of each pixel.\n\nWe will tour each extension in the sections below.\n\n","type":"content","url":"/spherex-intro#id-6-examine-the-header-of-one-of-the-spherex-spectral-image-mefs","position":13},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"7. Visualize a SPHEREx Spectral Image MEF using the Firefly Python Client."},"type":"lvl2","url":"/spherex-intro#id-7-visualize-a-spherex-spectral-image-mef-using-the-firefly-python-client","position":14},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"7. Visualize a SPHEREx Spectral Image MEF using the Firefly Python Client."},"content":"\n\nWe will use the open-source astronomy data visualization software Firefly visualize SPHEREx data. Firefly has a Python client and it understands the alternative WCS coordinates of SPHEREx, allowing users to see how the wavelength and bandwidth vary with spectral image pixels.\n\nOpen a Firefly viewer in a separate browser tab and initialize it.\n\nfc = FireflyClient.make_client(url=\"https://irsa.ipac.caltech.edu/irsaviewer\")\n\nfc.reinit_viewer()\n\n\n\n\n\nVisualize a spectral image MEF by sending its URL to the viewer.\n\nfc.show_fits_image(file_input=spectral_image_url,\n             plot_id=\"spectral_image\",\n             Title=\"Spectral Image\"\n             )\n\n\n\nTry use the interactive tools in the viewer to explore the data.\n\n","type":"content","url":"/spherex-intro#id-7-visualize-a-spherex-spectral-image-mef-using-the-firefly-python-client","position":15},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"8. Explore the first extension: IMAGE"},"type":"lvl2","url":"/spherex-intro#id-8-explore-the-first-extension-image","position":16},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"8. Explore the first extension: IMAGE"},"content":"The first extension of the MEF is the the calibrated surface brightness flux density in units of MJy/sr, stored as a 2040 x 2040 image. No zodiacal light subtraction is applied.\n\nThe SPHEREx focal plane is split with a dichroic to three short-wavelength and three long-wavelength detector arrays. Two focal plane assemblies (FPAs) simultaneously image the sky through a dichroic beam splitter. Each FPA contains three 2K x 2K detector arrays placed behind a set of linear variable filters (LVFs), providing narrow-band response with a band center that varies along one axis of the array. SPHEREx obtains spectra through multiple exposures, placing a given source at multiple positions in the field of view, where it is measured at multiple wavelengths by repointing the spacecraft.\n\nBand 1: λ= 0.75 - 1.09 µm; R=39\n\nBand 2: λ= 1.10 - 1.62 µm; R=41\n\nBand 3: λ= 1.63 - 2.41 µm; R=41\n\nBand 4: λ= 2.42 - 3.82 µm; R=35\n\nBand 5: λ= 3.83 - 4.41 µm; R=112\n\nBand 6: λ= 4.42 - 5.00 µm; R=128\n\nExamine the header of the first extension, printing out select keywords.\n\nspectral_image_header = hdulist[1].header\n\nkeywords_to_print = ['EXTNAME', 'NAXIS1', 'NAXIS2', 'BUNIT', 'DETECTOR', 'OBSID', 'DATE', 'PSF_FWHM']\n\nfor keyword in keywords_to_print:\n    value = spectral_image_header.get(keyword, 'Keyword not found')\n    print(f\"{keyword}: {value}\")\n\n\n\nWe can see that this image was taken with Detector 2, so we can expect the wavelength to vary from 1.10 - 1.62 µm. Try examining the Firefly visualization to confirm this.\n\nNotice that there is more than one WCS!\n\nspectral_image_header['WCSNAME*']\n\n\n\nThe main WCS describes the astrometric registration of the image, including optical distortion parameters, based on the FITS pixel convention starting with 1.\n\nThere are two alternative WCS systems:\n\nWCSNAMEA describes zero-based pixel coordinates.\n\nWCSNAMEW describes spectral coordinates ‘Wavelength’ and ‘Bandwidth’. This WCS contains a reference to the lookup table in the ‘WCS-WAVE’ extension.\n\n","type":"content","url":"/spherex-intro#id-8-explore-the-first-extension-image","position":17},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8a. Spatial WCS","lvl2":"8. Explore the first extension: IMAGE"},"type":"lvl3","url":"/spherex-intro#id-8a-spatial-wcs","position":18},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8a. Spatial WCS","lvl2":"8. Explore the first extension: IMAGE"},"content":"\n\nLoad the Spatial WCS\n\nwcs = WCS(spectral_image_header)\nwcs\n\n\n\nUse standard Astropy methods to resolve the central ra and dec (given by crval1 and crval2 in the header) into image pixel coordinates.\n\nra, dec = wcs.wcs.crval\nx, y = wcs.world_to_pixel(SkyCoord(ra=ra, dec=dec, unit=\"deg\"))\nprint(ra, dec, x, y)\n\n\n\n","type":"content","url":"/spherex-intro#id-8a-spatial-wcs","position":19},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8b. Spectral WCS","lvl2":"8. Explore the first extension: IMAGE"},"type":"lvl3","url":"/spherex-intro#id-8b-spectral-wcs","position":20},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8b. Spectral WCS","lvl2":"8. Explore the first extension: IMAGE"},"content":"The use of Linear Variable Filters (LVFs) is a key component of SPHEREx imaging. Each pixel of the detector corresponds to a slightly different wavelength and bandwidth due to the LVF’s gradual variation in spectral transmission across its surface. Contours of constant wavelength are curved due to the method of filter fabrication, so the wavelength-vs-pixel function is inherently two-dimensional.\n\nA compact, approximate representation of the wavelength and bandwidth per pixel is included in each image using the WAVE-TAB lookup-table mechanism defined in the FITS standard.\n\nBelow we illustrate how to use Spectral WCS to find an approximate wavelength at each pixel.\n\n# Load the Spectral WCS from the header.\n# Note that we need to provide a reference to HDU List, which contains a lookup table.\nspectral_wcs = WCS(header=hdulist[\"IMAGE\"].header, fobj=hdulist, key=\"W\")\n\n\n\nNote: The previous line triggers an Astropy INFO printout,\nwhich implies that the SIP distortion coefficients from the main WCS are preserved in the alternative WCS.\nThis is because the SIP convention, not formally part of the FITS standard,\nis ambiguous as to whether it is meant to apply to ‘alternative’ (lettered) WCSes in addition to the primary WCS.\nSee \n\nastropy​/astropy​#13105.\n\nThe wavelength per pixel is a property of the detector-filter combination and is independent of optical distortion in the telescope,\nand is modeled accordingly in WCS ‘W’, so we turn the SIP distortion off for this WCS.\n\n# SIP distortions must be turned off for the spectral WCS.\nspectral_wcs.sip = None\n\n\n\n\n\n# The standard Astropy methods for converting pixel coordinates to world coordinates can also be used to obtain spectral coordinates.\n# Take the pixel coordinates that we determined for the image center and resolve them to the wavelength and bandwidth for that pixel\nwl, bw = spectral_wcs.pixel_to_world(x, y)\nwl, bw\n\n\n\n","type":"content","url":"/spherex-intro#id-8b-spectral-wcs","position":21},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8c. How does wavelength vary across the detector?","lvl2":"8. Explore the first extension: IMAGE"},"type":"lvl3","url":"/spherex-intro#id-8c-how-does-wavelength-vary-across-the-detector","position":22},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8c. How does wavelength vary across the detector?","lvl2":"8. Explore the first extension: IMAGE"},"content":"\n\n# Read in the spectral image data.\nspectral_image = hdulist[\"IMAGE\"]\nspectral_image_data = spectral_image.data\n\n# Get arrays of pixel coordinates for the image.\n(y, x) = np.indices(spectral_image.shape)\n\n# Use the spectral WCS to convert these pixel coordinates to spectral coordinates.\nspectral_coords = spectral_wcs.pixel_to_world(x, y)\n\n# Break out the two spectral coordinates (wavelength and bandwidth) from the spectral coordinates, and print the results.\nwavelength, bandwidth = spectral_coords\nprint(\"Wavelength: \\n\", wavelength)\nprint(\"Bandwidth: \\n\", bandwidth)\n\n\n\n\n\n# Plot the wavelength as a greyscale color map across the image pixels.\nplt.imshow(wavelength.value, origin='lower', cmap='gray')\nplt.colorbar(label=f\"Wavelength [{wavelength.unit}]\")\n\ndetector = hdulist[\"IMAGE\"].header[\"DETECTOR\"]\nplt.title(f\"Wavelength - Detector {detector}\")\nplt.show()\n\n\n\nAs expected, the wavelengths range from approximately 1.1=16 micron for Detector 2. You can see that the longest wavelengths are at the bottom and the shortest wavelengths are at the top. You can verify this is the case in the Firefly visualization, as well.\n\n","type":"content","url":"/spherex-intro#id-8c-how-does-wavelength-vary-across-the-detector","position":23},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8d. Number of flagged pixels in this image","lvl2":"8. Explore the first extension: IMAGE"},"type":"lvl3","url":"/spherex-intro#id-8d-number-of-flagged-pixels-in-this-image","position":24},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl3":"8d. Number of flagged pixels in this image","lvl2":"8. Explore the first extension: IMAGE"},"content":"Now let’s take a look at some header keywords that provide information about how many pixels have been flagged during processing.\n\nspectral_image_header['L2 N_*']\n\n\n\nThere are 14 flags in total. Typically, most of the pixels are identified as SOURCE pixels, which are pixels mapped to a known source. The remaining flags are described in Table 8 of the \n\nSPHEREx Explanatory Supplement.\n\n","type":"content","url":"/spherex-intro#id-8d-number-of-flagged-pixels-in-this-image","position":25},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"9. Explore the second extension: FLAGS"},"type":"lvl2","url":"/spherex-intro#id-9-explore-the-second-extension-flags","position":26},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"9. Explore the second extension: FLAGS"},"content":"The second extension (FLAGS) provides a bitmap of per-pixel status and processing flags, stored as a 2040 x 2040 image.\n\nLet’s take a look at the header of the FLAGS extension, and print out some header keywords of interest.\n\nflags_header = hdulist[2].header\n\nkeywords_to_print = ['EXTNAME', 'NAXIS1', 'NAXIS2', 'BUNIT']\n\nfor keyword in keywords_to_print:\n    value = flags_header.get(keyword, 'Keyword not found')\n    print(f\"{keyword}: {value}\")\n\n\n\nConveniently, the definitions of the flags are also provided in the FLAGS header.\n\nflags_header['MP*']\n\n\n\nTip: In the Firefly visualization, if you are looking at the first extension (IMAGE), you can open up the layers icon (top right) to enable overlaying a visualization of the flags.\n\n","type":"content","url":"/spherex-intro#id-9-explore-the-second-extension-flags","position":27},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"10. Explore the third extension: VARIANCE"},"type":"lvl2","url":"/spherex-intro#id-10-explore-the-third-extension-variance","position":28},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"10. Explore the third extension: VARIANCE"},"content":"The third extension of the MEF is the variance of the calibrated surface brightness flux in units of (MJy/sr)^2, stored as a 2,040 x 2,040 image. Let’s look at some specific header keywords:\n\nvariance_header = hdulist[3].header\n\nkeywords_to_print = ['EXTNAME', 'NAXIS1', 'NAXIS2', 'BUNIT']\n\nfor keyword in keywords_to_print:\n    value = variance_header.get(keyword, 'Keyword not found')\n    print(f\"{keyword}: {value}\")\n\n\n\n","type":"content","url":"/spherex-intro#id-10-explore-the-third-extension-variance","position":29},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"11. Explore the fourth extension: ZODI"},"type":"lvl2","url":"/spherex-intro#id-11-explore-the-fourth-extension-zodi","position":30},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"11. Explore the fourth extension: ZODI"},"content":"The fourth extension of the MEF is the modeled zodiacal light background flux in units of MJy/sr, stored as a 2040 x 2040 image. This has not been subtracted from the IMAGE extension. Let’s examine some header keywords:\n\nzodi_header = hdulist[4].header\n\nkeywords_to_print = ['EXTNAME', 'NAXIS1', 'NAXIS2', 'BUNIT']\n\nfor keyword in keywords_to_print:\n    value = zodi_header.get(keyword, 'Keyword not found')\n    print(f\"{keyword}: {value}\")\n\n\n\n","type":"content","url":"/spherex-intro#id-11-explore-the-fourth-extension-zodi","position":31},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"12. Explore the fifth extension: PSF"},"type":"lvl2","url":"/spherex-intro#id-12-explore-the-fifth-extension-psf","position":32},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"12. Explore the fifth extension: PSF"},"content":"The fifth extension of the MEF contains 121 Point-spread functions (PSFs); each PSF is represented as a 101 x 101 image and all 121 are assembled together into a cube. Each of the 121 layers represents a “super-resolution” PSF estimate in a different region (defined by an 11x11 grid) of the detector. Each PSF is a two-dimensional array with size of 101 × 101 pixels. The PSFs are oversampled such that 10 PSF pixels cover the same spatial extent as one spectral image pixel (0.615 arcsec). Let’s look at some specific header keywords for the PSF:\n\npsf_header = hdulist[5].header\n\nkeywords_to_print = ['EXTNAME', 'NAXIS1', 'NAXIS2', 'NAXIS3', 'BUNIT']\n\nfor keyword in keywords_to_print:\n    value = psf_header.get(keyword, 'Keyword not found')\n    print(f\"{keyword}: {value}\")\n\n\n\n","type":"content","url":"/spherex-intro#id-12-explore-the-fifth-extension-psf","position":33},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"13. Explore the sixth extension: WCS-WAVE"},"type":"lvl2","url":"/spherex-intro#id-13-explore-the-sixth-extension-wcs-wave","position":34},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"13. Explore the sixth extension: WCS-WAVE"},"content":"The sixth extension is a FITS-compliant spectral World Coordinate System (WCS) lookup table that maps spectral image pixel coordinates to central wavelengths and bandwidths. The lookup table consists of 1 row with 3 columns (X, Y, VALUES). X and Y are each arrays defining a grid of control points in spectral image pixel space. For each (X, Y) control point, VALUES defines a two-element array containing the central wavelength and the corresponding bandwidth. Originally adopted to support the unique nature of the SPHEREx LVF filters, this rarely-used part of the FITS standard has yet to be implemented by all readers. The Firefly client we use in this notebook does correctly interpret this lookup table.\n\nLet’s look at the header of the WCS-WAVE extension:\n\nwcs_wave_header = hdulist[6].header\nwcs_wave_header\n\n\n\nUnlike the other extensions, this is a binary table. Let’s read it iinto an Astropy table.\n\nwcs_wave_table = Table(hdulist[6].data)\n\nprint(\"Number of rows:\", len(wcs_wave_table))\nprint(\"Column names:\", wcs_wave_table.colnames)\n\n\n\nThis table consists of just 1 row with three columns. Let’s inspect the columns:\n\nx_array      = wcs_wave_table[\"X\"][0]\ny_array      = wcs_wave_table[\"Y\"][0]\nwavelengths  = wcs_wave_table[\"VALUES\"][0]\n\nprint(\"X:\",  x_array)\nprint(\"Y:\",y_array)\nprint(\"Dimensions of VALUES array:\",  wavelengths.shape)\n\n\n\n","type":"content","url":"/spherex-intro#id-13-explore-the-sixth-extension-wcs-wave","position":35},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"Acknowledgements"},"type":"lvl2","url":"/spherex-intro#acknowledgements","position":36},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"Acknowledgements"},"content":"IPAC-IRSA\n\n","type":"content","url":"/spherex-intro#acknowledgements","position":37},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"About this notebook"},"type":"lvl2","url":"/spherex-intro#about-this-notebook","position":38},{"hierarchy":{"lvl1":"Introduction to SPHEREx Spectral Images","lvl2":"About this notebook"},"content":"Authors: IPAC Science Platform Team, including Jessica Krick, Troy Raen, Brigitta Sipőcz, Jaladh Singhal,\nAndreas Faisst, Shoubaneh Hemmati, Vandana Desai\n\nContact: \n\nIRSA Helpdesk with questions\nor problems.\n\nUpdated: 24 October 2025\n\nRuntime: approximately 30 seconds","type":"content","url":"/spherex-intro#about-this-notebook","position":39},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout"},"type":"lvl1","url":"/spherex-psf","position":0},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout"},"content":"","type":"content","url":"/spherex-psf","position":1},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"1. Learning Goals"},"type":"lvl2","url":"/spherex-psf#id-1-learning-goals","position":2},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"1. Learning Goals"},"content":"Determine how pixels in a SPHEREx cutout map to the pixels in the parent SPHEREx spectral image.\n\nUnderstand the structure of the PSF extension in a SPHEREx cutout (which is the same as the PSF extension in the parent spectral image)\n\nLearn which plane in a SPHEREx cutout PSF extension cube most accurately describes the coordinates you are interested in.\n\n","type":"content","url":"/spherex-psf#id-1-learning-goals","position":3},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"2. SPHEREx Overview"},"type":"lvl2","url":"/spherex-psf#id-2-spherex-overview","position":4},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"2. SPHEREx Overview"},"content":"SPHEREx is a NASA Astrophysics Medium Explorer mission that launched in March 2025.\nDuring its planned two-year mission, SPHEREx will obtain 0.75-5 micron spectroscopy over the entire sky, with deeper data in the SPHEREx Deep Fields.\nSPHEREx data will be used to:\n\nconstrain the physics of inflation by measuring its imprints on the three-dimensional large-scale distribution of matter,\n\ntrace the history of galactic light production through a deep multi-band measurement of large-scale clustering,\n\ninvestigate the abundance and composition of water and biogenic ices in the early phases of star and planetary disk formation.\n\nThe community will also mine SPHEREx data and combine it with synergistic data sets to address a variety of additional topics in astrophysics.\n\nMore information is available in the \n\nSPHEREx Explanatory Supplement.\n\n","type":"content","url":"/spherex-psf#id-2-spherex-overview","position":5},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"3. Imports"},"type":"lvl2","url":"/spherex-psf#id-3-imports","position":6},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"3. Imports"},"content":"The following packages must be installed to run this notebook.\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install astropy numpy pyvo\n\n\n\n\n\nimport re\nimport time\n\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pyvo\nfrom astropy.coordinates import SkyCoord\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.wcs import WCS\n\n# The time it takes to read SPHEREx files can exceed\n# astropy's default timeout limit. Increase it.\nfrom astropy.utils.data import conf\nconf.remote_timeout = 120\n\n\n\n","type":"content","url":"/spherex-psf#id-3-imports","position":7},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"4. Get SPHEREx Cutout"},"type":"lvl2","url":"/spherex-psf#id-4-get-spherex-cutout","position":8},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"4. Get SPHEREx Cutout"},"content":"We first obtain a SPHEREx cutout for a given coordinate of interest from IRSA archive.\nFor this we define a coordinate and a size of the cutout.\nBoth should be defined using astropy units.\nThe goal is to obtain the cutout and then extract the PSF corresponding to the coordinates of interest.\n\nTip\n\nTo learn more about how to access SPHEREx spectral images and how to download cutouts, we refer to the \n\nSPHEREx Intro Tutorial and the \n\nSPHEREx Cutouts Tutorial.\n\nra = 305.59875000000005 * u.degree\ndec = 41.14888888888889 * u.degree\nsize = 0.01 * u.degree\n\n\n\nOnce we defined the coordinates of interest and the size of the cutout, we run a TAP query to gather all SPHEREx spectral images that cover the coordinates.\n\n# Define the service endpoint for IRSA's Table Access Protocol (TAP)\n# so that we can query SPHEREx metadata tables.\nservice = pyvo.dal.TAPService(\"https://irsa.ipac.caltech.edu/TAP\")\n\n# Define a query that will search the appropriate SPHEREx metadata tables\n# for spectral images that cover the chosen coordinates of interest.\n# Return the cutout data access URL and the time of observation.\n# Sort by observation time.\nquery = f\"\"\"\nSELECT\n    'https://irsa.ipac.caltech.edu/' || a.uri || '?center={ra.value},{dec.value}d&size={size.value}' AS uri,\n    p.time_bounds_lower\nFROM spherex.artifact a\nJOIN spherex.plane p ON a.planeid = p.planeid\nWHERE 1 = CONTAINS(POINT('ICRS', {ra.value}, {dec.value}), p.poly)\nORDER BY p.time_bounds_lower\n\"\"\"\n\n# Execute the query and return as an astropy Table.\nt1 = time.time()\nresults = service.search(query)\nprint(\"Time to do TAP query: {:2.2f} seconds.\".format(time.time() - t1))\nprint(\"Number of images found: {}\".format(len(results)))\n\n\n\nNote\n\nSPHEREx data are also available via SIA which can provide a simpler interface for many queries, as demonstrated in \n\nIntroduction to SPHEREx Spectral Images.\nAn advantage of the method shown above is that it provides access to data immediately after ingestion (which occurs weekly) and is not subject to the same ~1 day delay as SIA.\n\nFor this example, we focus on the first one of the retrieved SPHEREx spectral images.\n\nspectral_image_url = results['uri'][0]\nprint(spectral_image_url)\n\n\n\n","type":"content","url":"/spherex-psf#id-4-get-spherex-cutout","position":9},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"5. Read in a SPHEREx Cutout"},"type":"lvl2","url":"/spherex-psf#id-5-read-in-a-spherex-cutout","position":10},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"5. Read in a SPHEREx Cutout"},"content":"Next, we use standard astropy tools to open the fits image and to read the different headers and data.\n\nTip\n\nAs we do below, you can use hdul.info() to print the list of FITS layers of the downloaded cutout.\n\nwith fits.open(spectral_image_url) as hdul:\n    hdul.info()\n    cutout_header = hdul['IMAGE'].header\n    psf_header = hdul['PSF'].header\n    cutout = hdul['IMAGE'].data\n    psfcube = hdul['PSF'].data\n\n\n\nThe downloaded SPHEREx image cutout contains 5 FITS layers, which are described in the \n\nSPHEREx Explanatory Supplement.\nWe focus in this example on the extensions IMAGE and PSF.\nWe have already loaded their data as well as their header.\n\npsfcube.shape\n\n\n\nThe shape of the psfcube is (121,101,101).\nThis corresponds to a grid of 11x11 PSFs across the image, each of them of the size 101x101 pixels.\n\nNote\n\nRemember that the PSFs are oversampled by a factor of 10.\nThis means that the actual size of the PSFs is about 10x10 SPHEREx pixels, which corresponds to about 60x60 arcseconds.\n\nLet’s look at a small part of the PSF header to understand its format:\n\npsf_header[22:40]\n\n\n\nWe confirm that the oversampling factor (OVERSAMP) is 10.\nThe PSFs are distributed in an even grid with 11x11 zones.\nEach of the 121 PSFs is responsible for one of these zones.\nThe PSF header therefore includes the center position of these zones as well as the width of the zones.\nThese center coordinate are specified with XCTR_i and YCTR_i, respectively, where i = 1...121.\nThe widths are specified with XWID_i and YWID_i, respectively, where again i = 1...121.\nThe zones have approximately equal widths and are arranged in an even grid.\nThe size of the zones is sufficient to capture well the changes of the PSF size and structure with wavelength and spatial coordinates.\n\nThe goal of this tutorial now is to find the PSF corresponding to our input coordinates of interest.\n\n","type":"content","url":"/spherex-psf#id-5-read-in-a-spherex-cutout","position":11},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"6. Determine the Pixel Location on the Parent SPHEREx Image"},"type":"lvl2","url":"/spherex-psf#id-6-determine-the-pixel-location-on-the-parent-spherex-image","position":12},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"6. Determine the Pixel Location on the Parent SPHEREx Image"},"content":"To identify the zone which covers the coordinates of interest, we first need to translate these coordinates to the pixel coordinates on the parent large SPHEREx image from which the cutout was created.\n\nWe do this by first determining the pixel (x,y) coordinates of our coordinates of interest on the cutout itself.\n\nwcs = WCS(cutout_header)\nxpix_cutout, ypix_cutout = wcs.world_to_pixel(SkyCoord(ra=ra, dec=dec))\n\nprint(f\"Pixel values of coordinates of interest on cutout image: x = {xpix_cutout}, y = {ypix_cutout}\")\n\n\n\nNext, we use the CRPIX1A and CRPIX1A header keywords (which describe the center of the cutout on the parent SPHEREx image) to shift the (x,y) coordinates of input to the parent SPHEREx image.\n\ncrpix1a = cutout_header[\"CRPIX1A\"]\ncrpix2a = cutout_header[\"CRPIX2A\"]\n\nxpix_orig = 1 + xpix_cutout - crpix1a\nypix_orig = 1 + ypix_cutout - crpix2a\n\nprint(f\"Pixel values of coordinates of interest on parent SPHEREx image: x = {xpix_orig}, y = {ypix_orig}\")\n\n\n\n","type":"content","url":"/spherex-psf#id-6-determine-the-pixel-location-on-the-parent-spherex-image","position":13},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"7. Determine the PSF Corresponding to Coordinates of Interest"},"type":"lvl2","url":"/spherex-psf#id-7-determine-the-psf-corresponding-to-coordinates-of-interest","position":14},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"7. Determine the PSF Corresponding to Coordinates of Interest"},"content":"Since we now know the (x,y) pixel values of the coordinates of interest on the parent SPHEREx image, we can identify the PSF zone.\nIn the following we first extract the zone pixel coordinates from the XCTR_* and YCTR_* keys in the PSF header.\n\nxctr = {}\nyctr = {}\n\nfor key, val in psf_header.items():\n    # Look for keys like XCTR* or YCTR*\n    xm = re.match(r'(XCTR*)', key)\n    if xm:\n        xplane = int(key.split(\"_\")[1])\n        xctr[xplane] = val\n    ym = re.match(r'(YCTR*)', key)\n    if ym:\n        yplane = int(key.split(\"_\")[1])\n        yctr[xplane] = val\n\n\n\nCheck that we got all of them!\n\nlen(xctr) == len(yctr)\n\n\n\nMake a nice table so we can easily search for the distance between zone center and coordinates of interest.\n\ntab = Table(names=[\"zone_id\" , \"x\" , \"y\"], dtype=[int, float, float])\nfor zone_id in xctr.keys():\n    tab.add_row([zone_id , xctr[zone_id] , yctr[zone_id]])\n\n\n\nOnce we have created this dictionary with zone pixel coordinates, we can simply search for the closest zone center to the coordinates of interest.\nFor this we first add the distance between zone center coordinates and coordinates of interest to the table. (Note that the x,y coordinates of the PSF zone centers are in 1,1 convention, therefore we have to subtract 1 pixels.)\n\ntab[\"distance\"] = np.sqrt((tab[\"x\"]-1 - xpix_orig)**2 + (tab[\"y\"]-1 - ypix_orig)**2)\n\n\n\nThen we can sort the table and pick the closest zone to coordinates of interest.\n\ntab.sort(\"distance\")\n\npsf_cube_plane = tab[0][\"zone_id\"]\ndistance_min = tab[0][\"distance\"]\n\nprint(f\"The PSF zone corresponding to coordinates of interest is {psf_cube_plane} with a distance of {distance_min} pixels\")\n\n\n\n","type":"content","url":"/spherex-psf#id-7-determine-the-psf-corresponding-to-coordinates-of-interest","position":15},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"8. Extract and Show the PSF"},"type":"lvl2","url":"/spherex-psf#id-8-extract-and-show-the-psf","position":16},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"8. Extract and Show the PSF"},"content":"Now that we know which zone corresponds to coordinates of interest, we can extract it and plot it.\n\npsf = psfcube[psf_cube_plane-1]\n\nfig = plt.figure(figsize=(5, 5))\nax1 = fig.add_subplot(1, 1, 1)\n\nax1.imshow(psf)\n\nplt.show()\n\n\n\n","type":"content","url":"/spherex-psf#id-8-extract-and-show-the-psf","position":17},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"9. Using the SPHEREx PSF in Forward Modeling (e.g., Tractor)"},"type":"lvl2","url":"/spherex-psf#id-9-using-the-spherex-psf-in-forward-modeling-e-g-tractor","position":18},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"9. Using the SPHEREx PSF in Forward Modeling (e.g., Tractor)"},"content":"The PSF returned by this notebook is oversampled relative to the native SPHEREx detector pixel grid.\nThis is intentional: the PSF is evaluated on a fine sub-pixel grid so that it can represent different intra-pixel source positions accurately.\n\nTools such as Tractor do not expect an oversampled PSF directly.\nInstead, they require a PSF that is pixel-integrated at the native detector resolution and evaluated at the correct sub-pixel phase of the source.\nIf you pass the oversampled PSF directly into Tractor without resampling, the effective PSF width and normalization will be incorrect, which can lead to systematic differences relative to the SPHEREx Spectrophotometry Tool.\n\nTo use this PSF for forward modeling or fitting, you must:\n\nShift the oversampled PSF to the source’s sub-pixel position,\n\nDownsample (integrate) it onto the native SPHEREx pixel grid, and\n\nNormalize the resulting PSF before passing it to Tractor.\n\n","type":"content","url":"/spherex-psf#id-9-using-the-spherex-psf-in-forward-modeling-e-g-tractor","position":19},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"Acknowledgements"},"type":"lvl2","url":"/spherex-psf#acknowledgements","position":20},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"Acknowledgements"},"content":"Caltech/IPAC-IRSA","type":"content","url":"/spherex-psf#acknowledgements","position":21},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"About this notebook"},"type":"lvl2","url":"/spherex-psf#about-this-notebook","position":22},{"hierarchy":{"lvl1":"Understanding and Extracting the PSF Extension in a SPHEREx Cutout","lvl2":"About this notebook"},"content":"Authors: IRSA Data Science Team, including Vandana Desai, Andreas Faisst, Brigitta Sipőcz, Troy Raen\n\nUpdated: 24 October 2025\n\nContact: Contact \n\nIRSA Helpdesk with questions or problems.\n\nRuntime: Approximately 30 seconds.","type":"content","url":"/spherex-psf#about-this-notebook","position":23},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products"},"type":"lvl1","url":"/siav2-seip","position":0},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products"},"content":"This notebook tutorial demonstrates the process of querying IRSA’s Simple Image Access v2 (SIAv2) service for the Spitzer Enhanced Imaging Products (SEIP), making a cutout image (thumbnail), and displaying the cutout.\n\n\n\n","type":"content","url":"/siav2-seip","position":1},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Learning Goals"},"type":"lvl2","url":"/siav2-seip#learning-goals","position":2},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nLearn how to search the NASA Astronomical Virtual Observatory Directory web portal for a service that provides access to IRSA’s Spitzer Enhanced Imaging Products (SEIP) images.\n\nUse the Python pyvo package to identify which of IRSA’s SEIP images cover a specified coordinate.\n\nDownload one of the identified images.\n\nCreate and display a cutout of the downloaded image.\n\n","type":"content","url":"/siav2-seip#learning-goals","position":3},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Introduction"},"type":"lvl2","url":"/siav2-seip#introduction","position":4},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Introduction"},"content":"The Spitzer Enhanced Imaging Products (SEIP) include Super Mosaics combining data from multiple programs if appropriate, and a high-reliability Source List of compact objects. The SEIP include data from the four channels of IRAC (3.6, 4.5, 5.8, 8 microns) and the 24 micron channel of MIPS. More information about the SEIP can be found at:\n\nhttps://​irsa​.ipac​.caltech​.edu​/data​/SPITZER​/Enhanced​/SEIP/\n\nThe \n\nNASA/IPAC Infrared Science Archive (IRSA) at Caltech is the archive for SEIP images and catalogs. The SEIP images that are the subject of this tutorial are made accessible via the \n\nInternational Virtual Observatory Alliance (IVOA) \n\nSimple Image Access (SIA) protocol. IRSA’s SEIP SIA service is registered in the NASA Astronomical Virtual Observatory (NAVO) \n\nDirectory. Based on the registered information, the Python package \n\npyvo can be used to query the SEIP SIA service for a list of images that meet specified criteria, and standard Python libraries can be used to download and manipulate the images.\nOther datasets at IRSA are available through other SIA services:\n\nhttps://​irsa​.ipac​.caltech​.edu​/docs​/program​_interface​/api​_images​.html\n\n","type":"content","url":"/siav2-seip#introduction","position":5},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Imports"},"type":"lvl2","url":"/siav2-seip#imports","position":6},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Imports"},"content":"pyvo for querying IRSA’s SEIP SIA service\n\nastropy.coordinates for defining coordinates\n\nastropy.nddata for creating an image cutout\n\nastropy.wcs for interpreting the World Coordinate System header keywords of a fits file\n\nastropy.units for attaching units to numbers passed to the SIA service\n\nmatplotlib.pyplot for plotting\n\nastropy.utils.data for downloading files\n\nastropy.io to manipulate FITS files\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install matplotlib astropy pyvo\n\n\n\n\n\nimport pyvo as vo\nfrom astropy.coordinates import SkyCoord\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom astropy.utils.data import download_file\nfrom astropy.io import fits\n\n\n\n","type":"content","url":"/siav2-seip#imports","position":7},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 1 - Setup"},"type":"lvl2","url":"/siav2-seip#section-1-setup","position":8},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 1 - Setup"},"content":"\n\nSet images to display in the notebook\n\n%matplotlib inline\n\n\n\nDefine coordinates of a bright star\n\nra = 314.30417\ndec = 77.595559\npos = SkyCoord(ra=ra, dec=dec, unit='deg')\n\n\n\n","type":"content","url":"/siav2-seip#section-1-setup","position":9},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 2 - Lookup and define a service for SEIP images"},"type":"lvl2","url":"/siav2-seip#section-2-lookup-and-define-a-service-for-seip-images","position":10},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 2 - Lookup and define a service for SEIP images"},"content":"\n\nStart at STScI VAO Registry at \n\nhttps://​vao​.stsci​.edu​/keyword​-search/\n\nSearch on “seip”\n\nLocate the COLLECTION=spitzer_seip from the SIA2 URL \n\nhttps://​irsa​.ipac​.caltech​.edu​/SIA​?COLLECTION​=​spitzer​_seip&​\n\nTo work with PyVO, define this SIAv2 service and use the collection argument separately\n\nseip_service2= vo.dal.sia2.SIA2Service('https://irsa.ipac.caltech.edu/SIA')\n\n\n\n","type":"content","url":"/siav2-seip#section-2-lookup-and-define-a-service-for-seip-images","position":11},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 3 - Search the service"},"type":"lvl2","url":"/siav2-seip#section-3-search-the-service","position":12},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 3 - Search the service"},"content":"\n\nSearch for images covering within 10 arcseconds of the star\n\nim_table = seip_service2.search(pos=(pos.ra.deg, pos.dec.deg, 10*u.arcsec),\n                                collection='spitzer_seip')\n\n\n\nDisplay the table of images that is returned\n\nim_table.to_table()\n\n\n\n","type":"content","url":"/siav2-seip#section-3-search-the-service","position":13},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 4 - Locate and download an image of interest"},"type":"lvl2","url":"/siav2-seip#section-4-locate-and-download-an-image-of-interest","position":14},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 4 - Locate and download an image of interest"},"content":"\n\nLet’s search the image results for a short-exposure IRAC Ch3 mosaic.\n\nfor i in range(len(im_table)):\n    if im_table[i].getdataurl().endswith('short.IRAC.3.mosaic.fits'):\n        break\nprint(im_table[i].getdataurl())\n\n\n\nDownload the image and open it in Astropy\n\nfname = download_file(im_table[i].getdataurl(), cache=True)\nimage1 = fits.open(fname)\n\n\n\n","type":"content","url":"/siav2-seip#section-4-locate-and-download-an-image-of-interest","position":15},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 5 - Extract a cutout and plot it"},"type":"lvl2","url":"/siav2-seip#section-5-extract-a-cutout-and-plot-it","position":16},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Section 5 - Extract a cutout and plot it"},"content":"\n\nwcs = WCS(image1[0].header)\n\n\n\n\n\ncutout = Cutout2D(image1[0].data, pos, (60, 60), wcs=wcs)\nwcs = cutout.wcs\n\n\n\n\n\nfig = plt.figure()\n\nax = fig.add_subplot(1, 1, 1, projection=wcs)\nax.imshow(cutout.data, cmap='gray_r', origin='lower', vmax = 10)\nax.scatter(ra, dec, transform=ax.get_transform('fk5'), s=500, edgecolor='red', facecolor='none')\n\n\n\n","type":"content","url":"/siav2-seip#section-5-extract-a-cutout-and-plot-it","position":17},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Exercise"},"type":"lvl2","url":"/siav2-seip#exercise","position":18},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Exercise"},"content":"Search the SEIP Source List using IRSA \n\nGator service, find an object of interest and repeat the steps above to extract cutouts in all four IRAC bands.\n\n\n\n\n\n\n\n","type":"content","url":"/siav2-seip#exercise","position":19},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"About this notebook"},"type":"lvl2","url":"/siav2-seip#about-this-notebook","position":20},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"About this notebook"},"content":"\n\nAuthor: David Shupe, IRSA Scientist, and the IRSA Science Team\n\nUpdated: 2022-02-14\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.\n\n","type":"content","url":"/siav2-seip#about-this-notebook","position":21},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Citations"},"type":"lvl2","url":"/siav2-seip#citations","position":22},{"hierarchy":{"lvl1":"Searching for Spitzer Enhanced Imaging Products","lvl2":"Citations"},"content":"\n\nIf you use astropy for published research, please cite the authors. Follow these links for more information about citing astropy:\n\nCiting astropy\n\nIf you use SEIP data in published research,  please cite the dataset Digital Object Identifier (DOI): \n\nSSC and IRSA (2020).","type":"content","url":"/siav2-seip#citations","position":23},{"hierarchy":{"lvl1":"Parallelizing image convolution"},"type":"lvl1","url":"/parallelize-convolution","position":0},{"hierarchy":{"lvl1":"Parallelizing image convolution"},"content":"","type":"content","url":"/parallelize-convolution","position":1},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Learning Goals"},"type":"lvl2","url":"/parallelize-convolution#learning-goals","position":2},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will be able to:\n\nEmploy three parallelization libraries to speed up a serial process.\n\nCalculate the speedup of the different approaches shown.\n\nEvaluate which library is suited to your task.\n\n","type":"content","url":"/parallelize-convolution#learning-goals","position":3},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Introduction"},"type":"lvl2","url":"/parallelize-convolution#introduction","position":4},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Introduction"},"content":"This notebook shows how to speed up an image convolution task using these three libraries:\n\nRay: an open-source unified compute framework that makes it easy to scale AI and Python workloads.\n\nMultiprocessing: part of the standard library; supports spawning processes using an API similar to the threading module; offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads.\n\nDask: developed to natively scale computational packages like numpy, pandas and scikit-learn, and the surrounding ecosystem, to multi-core machines and distributed clusters when datasets exceed memory.\n\n","type":"content","url":"/parallelize-convolution#introduction","position":5},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Imports"},"type":"lvl2","url":"/parallelize-convolution#imports","position":6},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Imports"},"content":"multiprocessing.Pool for multiprocessing using the standard library\n\ntime for timing the processes\n\ndask.distributed.Client for making a local Dask cluster\n\nnumpy and scipy.signal for numerical work\n\npsutil for finding the available processors on your machine\n\nray for scaling up Python tasks\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install dask[distributed] numpy ray scipy\n\n\n\n\n\nfrom multiprocessing import Pool\nimport time\n\nfrom dask.distributed import Client\nimport numpy as np\nimport psutil\nimport scipy.signal\nimport ray\n\n\n\n","type":"content","url":"/parallelize-convolution#imports","position":7},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Find the cpus available"},"type":"lvl2","url":"/parallelize-convolution#find-the-cpus-available","position":8},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Find the cpus available"},"content":"Find and print the number of cpus\n(taken from \n\nhttps://​towardsdatascience​.com​/10x​-faster​-parallel​-python​-without​-python​-multiprocessing​-e5017c93cce1)\n\nnum_cpus = psutil.cpu_count(logical=True)\nprint(num_cpus)\n\n\n\n","type":"content","url":"/parallelize-convolution#find-the-cpus-available","position":9},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process serially using a conventional loop"},"type":"lvl2","url":"/parallelize-convolution#process-serially-using-a-conventional-loop","position":10},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process serially using a conventional loop"},"content":"\n\nUse scipy.signal to convolve two 2-dimensional arrays and return a 5x5 downsampled result.\n\ndef fconv(image, random_filter):\n    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]\n\n\n\n\n\nfilters = [np.random.normal(size=(4, 4)) for _ in range(num_cpus)]\n\n\n\nProcess 100 iterations serially, then extrapolate to num_cpus*100\n\nstart = time.time()\nnum_iter = 100\nimage = np.zeros((3000, 3000))\nfor i in range(num_iter):\n    result = fconv(image, filters[i % num_cpus])\nduration_conv = time.time() - start\nprint(\"(scaled) conventional duration for {:d} iterations = {:.1f} seconds\"\n      .format(num_cpus*num_iter, duration_conv*num_cpus))\n\n\n\n","type":"content","url":"/parallelize-convolution#process-serially-using-a-conventional-loop","position":11},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process in parallel using Ray"},"type":"lvl2","url":"/parallelize-convolution#process-in-parallel-using-ray","position":12},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process in parallel using Ray"},"content":"\n\nDocumentation for ray\n\nThe warning raised by ray.init only affects shared object usage, which is not an issue for this tutorial. It may harm performance in other scenarios.\n\nray.init(num_cpus=num_cpus)\n\n\n\nUse scipy.signal to convolve two 2-dimensional arrays and return a 5x5 downsampled result. To use Ray, we decorate the function that is doing the work.\n\n@ray.remote\ndef fray(image, random_filter):\n    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]\n\n\n\nIn the following loop, ray.put places the image into shared memory. The call to ray.get retrieves the result.\n\nstart = time.time()\nimage = np.zeros((3000, 3000))\nfor _ in range(100):\n    image_id = ray.put(image)\n    ray.get([fray.remote(image_id, filters[i]) for i in range(num_cpus)])\nduration_ray = time.time() - start\nprint(\"Ray duration = {:.1f}, speedup = {:.2f}\"\n      .format(duration_ray, duration_conv*num_cpus / duration_ray))\n\n\n\n\n\nray.shutdown()\n\n\n\n","type":"content","url":"/parallelize-convolution#process-in-parallel-using-ray","position":13},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process in parallel using multiprocessing"},"type":"lvl2","url":"/parallelize-convolution#process-in-parallel-using-multiprocessing","position":14},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process in parallel using multiprocessing"},"content":"\n\nDocumentation for multiprocessing\n\nUse scipy.signal to convolve two 2-dimensional arrays and return a 5x5 downsampled result. The call to the function has a slightly different form than that for the serial loop.\n\n# Note: Mac and Windows users may need to copy the contents of this cell into a separate '.py' file\n# and then import it in order to use the `fmp` function with `multiprocessing`. This has to do with\n# differences in what does / does not get copied into the child processes in different operating systems.\nimport scipy.signal\n\ndef fmp(args):\n    image, random_filter = args\n    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]\n\n\n\nUse a multiprocessing pool with the number of cpus we found earlier.\n\npool = Pool(num_cpus)\n\n\n\nUsing pool.map is the closest analog in multiprocessing to the Ray API.\n\nstart = time.time()\nimage = np.zeros((3000, 3000))\nfor _ in range(100):\n    pool.map(fmp, zip(num_cpus * [image], filters))\nduration_mp = time.time() - start\nprint(\"Multiprocessing duration = {:.1f}, speedup = {:.2f}\"\n      .format(duration_mp, duration_conv*num_cpus / duration_mp))\n\n\n\n","type":"content","url":"/parallelize-convolution#process-in-parallel-using-multiprocessing","position":15},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process using Dask"},"type":"lvl2","url":"/parallelize-convolution#process-using-dask","position":16},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Process using Dask"},"content":"\n\nDocumentation for Dask\n\nDefine a Dask distributed client with number of workers set to the number of cpus we found earlier, and with one thread per worker.\n\nclient = Client(n_workers=num_cpus, threads_per_worker=1)\n\n\n\n\n\nprint(client)\n\n\n\nDask recommends scattering the large inputs across the workers, though this makes little difference in execution time.\n\nstart = time.time()\nimage = np.zeros((3000, 3000))\nfor _ in range(100):\n    for j in range(num_cpus):\n        big_future = client.scatter((image, filters[j % num_cpus]))\n        future = client.submit(fmp, big_future)\nduration_dask = time.time() - start\nprint(\"Dask duration = {:.1f}, speedup = {:.2f}\"\n      .format(duration_dask, duration_conv*num_cpus / duration_dask))\n\n\n\n\n\nclient.close()\n\n\n\n","type":"content","url":"/parallelize-convolution#process-using-dask","position":17},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Conclusions"},"type":"lvl2","url":"/parallelize-convolution#conclusions","position":18},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Conclusions"},"content":"\n\nRay is the most effective at speeding up the convolution workload by fully utilizing all available processes\n\nMultiprocessing is second in effectiveness\n\nDask delivers the least speedup; perhaps due to having only six processes on the dask.distributed client\n\n","type":"content","url":"/parallelize-convolution#conclusions","position":19},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"About this notebook"},"type":"lvl2","url":"/parallelize-convolution#about-this-notebook","position":20},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"About this notebook"},"content":"Author: David Shupe in conjunction with Jessica Krick and the IRSA Science Platform team at IPAC.\n\nUpdated: 2024-09-24\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/parallelize-convolution#about-this-notebook","position":21},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Citations"},"type":"lvl2","url":"/parallelize-convolution#citations","position":22},{"hierarchy":{"lvl1":"Parallelizing image convolution","lvl2":"Citations"},"content":"If you use these software packages in your work, please use the following citations:\n\nDask: Dask Development Team (2016). Dask: Library for dynamic task scheduling. URL \n\nhttps://dask.org\n\nRay: The Ray Development Team. URL \n\nhttps://docs.ray.io\n\n\n\n","type":"content","url":"/parallelize-convolution#citations","position":23},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs"},"type":"lvl1","url":"/seds-in-firefly","position":0},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs"},"content":"","type":"content","url":"/seds-in-firefly","position":1},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Learning Goals"},"type":"lvl2","url":"/seds-in-firefly#learning-goals","position":2},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Learning Goals"},"content":"\n\nBy the end of this tutorial, the following goals can be accomplished:\n\nSelect a position and categories of data for search of possible nearby sources\n\nPull photometry for different bands via TAP queries on chosen IRSA catalogs\n\nAdd data columns from result to table, including energy density values (converted from magnitudes)\n\nCreate a simple SED plot using compiled photometric data\n\nDisplay matching image cutouts from IRSA data sets for selected position\n\n","type":"content","url":"/seds-in-firefly#learning-goals","position":3},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Introduction"},"type":"lvl2","url":"/seds-in-firefly#introduction","position":4},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Introduction"},"content":"\n\nThe purpose of this notebook is to show how to make and vet Spectral Energy Distributions (SEDs) using Firefly. All examples use the python package pyvo to search for catalogs and images from IRSA.\n\nDetailed specifications for this notebook come from a paper led by Luisa Rebull on the selection of possible Young Stellar Objects (YSOs) in IC 417 (\n\nRebull et al. (2023)).\n\nIn this case, the SEDs generated by photometry data (from several different survey catalogs) near a given position can be compared with corresponding images to verify the location of a point source that could also be considered as a YSO if the graph shape is sound.\n\n","type":"content","url":"/seds-in-firefly#introduction","position":5},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Imports"},"type":"lvl2","url":"/seds-in-firefly#imports","position":6},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Imports"},"content":"\n\nPrior to starting Jupyterlab, you should specify the Firefly server by setting the FIREFLY_URL environment variable (use the value \n\nhttps://​irsa​.ipac​.caltech​.edu​/irsaviewer if you’re not sure), or by one of the other ways listed on the \n\njupyter_firefly_extension page.\n\nThe packages needed for this notebook are in the requirements.txt file. They can be installed with pip install -r requirements.txt before you start your Jupyterlab session.\n\nastropy.units - for specifying and manipulating units\n\nastropy.constants - for constants\n\nastropy.coordinates.Skycoord - for representing coordinates\n\nastropy.table.QTable - for tables with units\n\nfirefly_client.FireflyClient - Python API to Firefly for displaying tables, images and charts\n\nnumpy - for working with arrays\n\npyvo - for queries to Virtual Observatory services at the archives\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install numpy astropy pyvo firefly_client\n\n\n\nCall the imports below:\n\nimport astropy.units as u\nfrom astropy import constants as const\nfrom astropy.coordinates import SkyCoord\nfrom astropy.table import QTable\nfrom astropy.utils.data import download_file\nfrom firefly_client import FireflyClient\nimport numpy as np\nimport pyvo\n\n\n\n","type":"content","url":"/seds-in-firefly#imports","position":7},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Choose Target as Example"},"type":"lvl2","url":"/seds-in-firefly#choose-target-as-example","position":8},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Choose Target as Example"},"content":"\n\nFrom \n\nFigure 10 of the referenced paper, pick the source in the upper right corner (J052736.37+344940.6):\n\ntarget = SkyCoord(ra=\"05h27m36.37s\", dec=\"+34d49m40.6s\")\n\n\n\n","type":"content","url":"/seds-in-firefly#choose-target-as-example","position":9},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Get Photometry from IRSA"},"type":"lvl2","url":"/seds-in-firefly#get-photometry-from-irsa","position":10},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Get Photometry from IRSA"},"content":"\n\nThe available data at IRSA include the following catalogs (with matching radius at 1 arcsec except where noted by reference paper):\n\nGaia DR3\n\n2MASS\n\nSpitzer-IRAC (GLIMPSE? SEIP Source List?)\n\nSpitzer-MIPS-24 (GLIMPSE? SEIP Source List?)\n\nAllWISE\n\nCatWISE\n\nunWISE\n\nMSX\n\nAkari\n\nHerschel-PACS (70um and 160um)\n\nThe \n\nNITARP Wiki page on central wavelengths may be helpful for finding zero-point information for specific catalogs, as well as the more general \n\nastroquery service on SVO filter data.\n\nWhlie this notebook will focus on a subset of the above list (2MASS, Gaia DR3, and WISE data), note that there is a section where additional data can be included as well.\n\nThe variable phot_tbl is the table containing the photometry for the target and all the derived quantities needed to make SED plots:\n\nRight ascension (in degrees)\n\nDeclination (in degrees)\n\nWavelength for the bandpass (in microns)\n\nPhotometry measurement\n\nPhotometry uncertainty\n\nIs the measurement an upper limit? (Note: 1 means yes, 0 means no)\n\nZeropoint for the bandpass in units of Janskys (needed for converting photometric measurements to energy densities)\n\nSet the variables for the photmetry table and each quantity in that data structure:\n\nphot_tbl = QTable(names=['ra', 'dec', 'wavelength', 'mag', 'magerr', 'upper_limit', 'zeropoint'],\n                  units=[u.deg, u.deg, u.micron, None, None, None, u.Jy])\n\n\n\nWe query catalog data using IRSA’s TAP (VO Table Access Protocol) service. It’s sufficient to set up the service once, at the beginning of the session:\n\nirsa_tap = pyvo.dal.TAPService(\"https://irsa.ipac.caltech.edu/TAP\")\n\n\n\n","type":"content","url":"/seds-in-firefly#get-photometry-from-irsa","position":11},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve 2MASS photometry","lvl2":"Get Photometry from IRSA"},"type":"lvl3","url":"/seds-in-firefly#retrieve-2mass-photometry","position":12},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve 2MASS photometry","lvl2":"Get Photometry from IRSA"},"content":"\n\nTo start, define a function to retrieve 2MASS photometry via a TAP query to IRSA that includes upper limits (note that only the ph_qual column on photometric quality of source is used and not the rd_flg column on “read flag”):\n\ndef get_2mass_phot(target, radius=1*u.arcsec):\n    \"\"\"Get photometry from 2MASS\n\n    Parameters:\n    -----------\n    target: astropy.coordinates.SkyCoord\n          coordinates of the target(s)\n\n    radius: astropy.units.Quantity\n          matching radius, default 1*u.arcsec\n\n    Returns:\n    --------\n    rows: list\n         lists of photometry points to add to table\n    \"\"\"\n\n    query = f\"\"\"\n    SELECT ra,dec,j_m,j_msigcom,h_m,h_msigcom,k_m,k_msigcom,ph_qual,rd_flg\n    FROM fp_psc\n    WHERE CONTAINS(POINT('ICRS', ra, dec),\n    CIRCLE('ICRS', {target.ra.deg}, {target.dec.deg},\n    {radius.to('deg').value}))=1\n    \"\"\"\n\n    results = irsa_tap.search(query)\n\n    rowj = [results[\"ra\"]*u.deg,\n            results[\"dec\"]*u.deg,\n            1.235*u.micron,\n            results[\"j_m\"][0],\n            results[\"j_msigcom\"][0],\n            1 if results[\"ph_qual\"] == \"U\" else 0,\n            1594*u.Jy]\n\n    rowh = [results[\"ra\"]*u.deg,\n            results[\"dec\"]*u.deg,\n            1.662*u.micron,\n            results[\"h_m\"][0],\n            results[\"h_msigcom\"][0],\n            1 if results[\"ph_qual\"] == \"U\" else 0,\n            1024*u.Jy]\n\n    rowks = [results[\"ra\"]*u.deg,\n            results[\"dec\"]*u.deg,\n            2.159*u.micron,\n            results[\"k_m\"][0],\n            results[\"k_msigcom\"][0],\n            1 if results[\"ph_qual\"] == \"U\" else 0,\n            666.7*u.Jy]\n\n    return([rowj, rowh, rowks])\n\n\n\nConsider that while the search radius is currently set at 1 degree from the position, that value can be adjusted by changing what the number on the radius line is.\n\nNow gather the rows of 2MASS data and add each of them to the photometry table:\n\nrows = get_2mass_phot(target)\n\n\n\n\n\nfor row in rows:\n    phot_tbl.add_row(row)\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-2mass-photometry","position":13},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve AllWISE Photometry","lvl2":"Get Photometry from IRSA"},"type":"lvl3","url":"/seds-in-firefly#retrieve-allwise-photometry","position":14},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve AllWISE Photometry","lvl2":"Get Photometry from IRSA"},"content":"\n\nAccording to the paper referenced above, the detections from the AllWISE catalog were retained if the data quality flags were A, B, or C; if the data quality flag was Z, then the data were provisionally retained with a very large error bar, 30% larger than what appears in the catalog.\n\nBased on that information, define a function to release AllWISE photometry via a TAP query to IRSA that includes the needed data quality flags and retains upper limits:\n\ndef get_allwise_phot(target, radius=1*u.arcsec):\n    \"\"\"Get photometry from AllWISE\n\n    Parameters:\n    -----------\n    target: astropy.coordinates.SkyCoord\n          coordinates of the target(s)\n\n    radius: astropy.units.Quantity\n          matching radius, defaults to 1*u.arcsec\n\n    Returns:\n    --------\n    rows: list\n         lists of photometry points to add to table\n    \"\"\"\n\n    query = f\"\"\"\n    SELECT ra,dec,w1mpro,w1sigmpro,w2mpro,w2sigmpro,w3mpro,\n          w3sigmpro,w4mpro,w4sigmpro,ph_qual\n    FROM allwise_p3as_psd\n    WHERE CONTAINS(POINT('ICRS', ra, dec),\n    CIRCLE('ICRS', {target.ra.deg}, {target.dec.deg},\n    {radius.to('deg').value}))=1\n    \"\"\"\n\n    results = irsa_tap.search(query)\n\n    roww1 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             3.4*u.micron,\n             results[\"w1mpro\"][0],\n             results[\"w1sigmpro\"][0],\n             1 if results[\"ph_qual\"] == \"U\" else 0,\n             309.54*u.Jy]\n\n    roww2 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             4.6*u.micron,\n             results[\"w2mpro\"][0],\n             results[\"w2sigmpro\"][0],\n             1 if results[\"ph_qual\"] == \"U\" else 0,\n             171.79*u.Jy]\n\n    roww3 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             12*u.micron,\n             results[\"w3mpro\"][0],\n             results[\"w3sigmpro\"][0],\n             1 if results[\"ph_qual\"] == \"U\" else 0,\n             31.676*u.Jy]\n\n    roww4 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             22*u.micron,\n             results[\"w4mpro\"][0],\n             results[\"w4sigmpro\"][0],\n             1 if results[\"ph_qual\"] == \"U\" else 0,\n             8.3635*u.Jy]\n\n    return([roww1, roww2, roww3, roww4])\n\n\n\n\n\nrows = get_allwise_phot(target)\n\n\n\n\n\nfor row in rows:\n    phot_tbl.add_row(row)\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-allwise-photometry","position":15},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve Gaia DR3 Photometry","lvl2":"Get Photometry from IRSA"},"type":"lvl3","url":"/seds-in-firefly#retrieve-gaia-dr3-photometry","position":16},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve Gaia DR3 Photometry","lvl2":"Get Photometry from IRSA"},"content":"\n\nDefine a function to retrieve Gaia DR3 photometry via a TAP query to IRSA (not considering upper limits as Gaia DR3 ony provides “detections or not” info so actual limits are not given):\n\ndef get_gaia_phot(target, radius=1*u.arcsec):\n    \"\"\"Get photometry from Gaia DR3\n\n    Parameters:\n    -----------\n    target: astropy.coordinates.SkyCoord\n          coordinates of the target(s)\n\n    radius: astropy.units.Quantity\n          matching radius, default is 1*u.arcsec\n\n    Returns:\n    --------\n    rows: list\n         lists of photometry points to add to table\n    \"\"\"\n\n    query = f\"\"\"\n    SELECT designation,ra,ra_error,dec,dec_error,ra_dec_corr,phot_g_mean_flux_over_error,phot_g_mean_mag,\n       phot_bp_mean_flux_over_error,phot_bp_mean_mag,phot_rp_mean_flux_over_error,phot_rp_mean_mag\n    FROM gaia_dr3_source\n    WHERE CONTAINS(POINT('ICRS', ra, dec),\n    CIRCLE('ICRS', {target.ra.deg}, {target.dec.deg},\n    {radius.to('deg').value}))=1\n    \"\"\"\n\n    results = irsa_tap.search(query)\n\n    rowg = [results[\"ra\"]*u.deg,\n            results[\"dec\"]*u.deg,\n            0.622*u.micron,\n            results[\"phot_g_mean_mag\"][0],\n            2.5/(np.log(10)*results[\"phot_g_mean_flux_over_error\"][0]),\n            0,\n            3228.75*u.Jy]\n\n    rowbp = [results[\"ra\"]*u.deg,\n            results[\"dec\"]*u.deg,\n            0.511*u.micron,\n            results[\"phot_bp_mean_mag\"][0],\n            2.5/(np.log(10)*results[\"phot_bp_mean_flux_over_error\"][0]),\n            0,\n            3552.01*u.Jy]\n\n    rowrp = [results[\"ra\"]*u.deg,\n            results[\"dec\"]*u.deg,\n            0.7769*u.micron,\n            results[\"phot_rp_mean_mag\"][0],\n            2.5/(np.log(10)*results[\"phot_rp_mean_flux_over_error\"][0]),\n            0,\n            2554.95*u.Jy]\n\n    return([rowg, rowbp, rowrp])\n\n\n\n\n\nrows = get_gaia_phot(target)\n\n\n\n\n\nfor row in rows:\n    phot_tbl.add_row(row)\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-gaia-dr3-photometry","position":17},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve CatWISE Photometry","lvl2":"Get Photometry from IRSA"},"type":"lvl3","url":"/seds-in-firefly#retrieve-catwise-photometry","position":18},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve CatWISE Photometry","lvl2":"Get Photometry from IRSA"},"content":"\n\nDefine a function to retrieve CatWISE photometry via a TAP query to IRSA that includes upper limits (note that CatWISE2020 Catalog is being used rather than CatWISE Preliminary Catalog):\n\ndef get_catwise_phot(target, radius=1*u.arcsec):\n    \"\"\"Get photometry from CatWISE\n\n    Parameters:\n    -----------\n    target: astropy.coordinates.SkyCoord\n          coordinates of the target(s)\n\n    radius: astropy.units.Quantity\n          matching radius, defaults to 1*u.arcsec\n\n    Returns:\n    --------\n    rows: list\n         lists of photometry points to add to table\n    \"\"\"\n\n    query = f\"\"\"\n    SELECT ra,dec,w1mpro,w1sigmpro,w2mpro,w2sigmpro\n    FROM catwise_2020\n    WHERE CONTAINS(POINT('ICRS', ra, dec),\n    CIRCLE('ICRS', {target.ra.deg}, {target.dec.deg},\n    {radius.to('deg').value}))=1\n    \"\"\"\n\n    results = irsa_tap.search(query)\n\n    roww1 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             3.4*u.micron,\n             results[\"w1mpro\"][0],\n             results[\"w1sigmpro\"][0],\n             1 if results[\"w1sigmpro\"] is None else 0,\n             309.54*u.Jy]\n\n    roww2 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             4.6*u.micron,\n             results[\"w2mpro\"][0],\n             results[\"w2sigmpro\"][0],\n             1 if results[\"w2sigmpro\"] is None else 0,\n             171.79*u.Jy]\n\n    return([roww1, roww2])\n\n\n\n\n\nrows = get_catwise_phot(target)\n\n\n\n\n\nfor row in rows:\n    phot_tbl.add_row(row)\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-catwise-photometry","position":19},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve unWISE Photometry","lvl2":"Get Photometry from IRSA"},"type":"lvl3","url":"/seds-in-firefly#retrieve-unwise-photometry","position":20},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve unWISE Photometry","lvl2":"Get Photometry from IRSA"},"content":"\n\nDefine a function to retrieve unWISE photometry via a TAP query to IRSA(note that flux units are given in nano-mag, zero-point is carried over from AllWISE, and no flag for upper limits is given):\n\ndef get_unwise_phot(target, radius=1*u.arcsec):\n    \"\"\"Get photometry from CatWISE\n\n    Parameters:\n    -----------\n    target: astropy.coordinates.SkyCoord\n          coordinates of the target(s)\n\n    radius: astropy.units.Quantity\n          matching radius, defaults to 1*u.arcsec\n\n    Returns:\n    --------\n    rows: list\n         lists of photometry points to add to table\n    \"\"\"\n\n    query = f\"\"\"\n    SELECT ra,dec,flux_1,dflux_1,flux_2,dflux_2,fluxlbs_1\n    FROM unwise_2019\n    WHERE CONTAINS(POINT('ICRS', ra, dec),\n    CIRCLE('ICRS', {target.ra.deg}, {target.dec.deg},\n    {radius.to('deg').value}))=1\n    \"\"\"\n\n    results = irsa_tap.search(query)\n\n    roww1 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             3.4*u.micron,\n             pow(10,9)*results[\"flux_1\"][0],\n             pow(10,9)*results[\"dflux_1\"][0],\n             0,\n             309.54*u.Jy]\n\n    roww2 = [results[\"ra\"]*u.deg,\n             results[\"dec\"]*u.deg,\n             4.6*u.micron,\n             pow(10,9)*results[\"flux_2\"][0],\n             pow(10,9)*results[\"dflux_2\"][0],\n             0,\n             171.79*u.Jy]\n\n    return([roww1, roww2])\n\n\n\n\n\nrows = get_unwise_phot(target)\n\n\n\n\n\nfor row in rows:\n    phot_tbl.add_row(row)\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-unwise-photometry","position":21},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve Optional Photometry","lvl2":"Get Photometry from IRSA"},"type":"lvl3","url":"/seds-in-firefly#retrieve-optional-photometry","position":22},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Retrieve Optional Photometry","lvl2":"Get Photometry from IRSA"},"content":"\n\nInsert more data retrievals here, following the above formats.\n\nNote: After more columns are added in the conversion to spectral energy density, the phot_tbl.add_row functions will thrown an exception that number of values doesn’t match the number of columns.\n\n# SEIP\n# GLIMPSE\n\n# MSX, AKARI, Herschel PACS\n\n# Pan-STARRS, UKIDSS?\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-optional-photometry","position":23},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Convert Magnitudes to Energy Densities"},"type":"lvl2","url":"/seds-in-firefly#convert-magnitudes-to-energy-densities","position":24},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Convert Magnitudes to Energy Densities"},"content":"\n\nUse the zeropoint information embedded by the catalog retrieval functions to compute energy densities:\n\nfactor = (const.c*phot_tbl[\"wavelength\"]**-1).to(u.Hz)\n\n\n\n\n\nphot_tbl[\"lam_flam\"] = ((factor*phot_tbl[\"zeropoint\"] *\n                            10**(-0.4*phot_tbl[\"mag\"]))\n                       ).to(u.erg*u.s**-1*u.cm**-2)\n\n\n\nAs the above formula was for the photometry measurements given, compute the photometry uncertainty using a similar formula:\n\nphot_tbl[\"lam_flam_err\"] = phot_tbl[\"lam_flam\"]*phot_tbl[\"magerr\"]*np.log(10)/2.5\n\n\n\nCheck the energy density values for each converted value below:\n\nphot_tbl[\"lam_flam\"]\n\n\n\n\n\nphot_tbl[\"lam_flam_err\"]\n\n\n\nRun the following cell to view the full photometry table and its values for each column (ex. zeropoint, upper limit):\n\nphot_tbl\n\n\n\n","type":"content","url":"/seds-in-firefly#convert-magnitudes-to-energy-densities","position":25},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Upload to Firefly"},"type":"lvl2","url":"/seds-in-firefly#upload-to-firefly","position":26},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Upload to Firefly"},"content":"\n\nThere are two ways to initialize a Firefly client from Python, depending on whether you’re running the notebook in JupyterLab or not. Assuming you have jupyter-firefly-extensions set up in your environment as explained \n\nhere, you can use make_lab_client() in JupyterLab, which will open the Firefly viewer in a new tab within the Lab. Otherwise, you can use make_client() in a Jupyter Notebook (or even a Python shell), which will open the Firefly viewer in a new web browser tab.\n\nYou also need a Firefly server to communicate with your Firefly Python client. In this notebook, we use a public Firefly server: the IRSA Viewer (\n\nhttps://​irsa​.ipac​.caltech​.edu​/irsaviewer). However, you can also run a local Firefly server via a \n\nFirefly Docker image and access it at http://localhost:8080/firefly. The URL of the Firefly server is read by both make_client() and make_lab_client() through the environment variable FIREFLY_URL. However, make_client() also allows you to pass the URL directly as the url parameter.\n\n# Uncomment when using within Jupyter Lab with jupyter_firefly_extensions installed\n# fc = FireflyClient.make_lab_client()\n\n# Uncomment for contexts other than above \nfc = FireflyClient.make_client(url=\"https://irsa.ipac.caltech.edu/irsaviewer\")\n\nfc.reinit_viewer() # to clean the state, if this cell ran earlier\n\n\n\n\n\nIn the event that there are problems with the tab opened above, run the below command to obtain a web link that can be opened in a browser directly:\n\nfc.display_url()\n\n\n\nWrite the photometry table to a FITS file:\n\ntblpath = \"./phot_tbl.fits\"\nphot_tbl.write(tblpath, format=\"fits\", overwrite=True)\n\n\n\nUpload the table (FITS file) to Firefly:\n\ntval = fc.upload_file(tblpath)\n\n\n\nShow the table in Firefly:\n\nfc.show_table(tval, tbl_id=\"photometry\", title=\"SED Photometry\")\n\n\n\nSpecify the columns and markers for the SED plot:\n\ntrace1 = dict(tbl_id=\"photometry\", x=\"tables::wavelength\", y=\"tables::lam_flam\", mode=\"markers\",\n            type=\"scatter\", error_y=dict(array=\"tables::lam_flam_err\"),\n            marker=dict(size=4))\ntrace_data = [trace1]\n\n\n\nSpecify the title and axes for the SED plot:\n\nlayout_s = dict(title=\"SED\", xaxis=dict(title=\"Wavelength (microns)\", type=\"log\"),\n                yaxis=dict(title=\"Energy density (erg/s/cm2)\", type=\"log\"))\n\n\n\nDisplay the SED plot in Firefly:\n\nfc.show_chart(layout=layout_s, data=trace_data)\n\n\n\n","type":"content","url":"/seds-in-firefly#upload-to-firefly","position":27},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Retrieve and Display Images"},"type":"lvl2","url":"/seds-in-firefly#retrieve-and-display-images","position":28},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"Retrieve and Display Images"},"content":"\n\nUse queries directed at Simple Image Access services to retrieve images:\n\nirsa_sia = pyvo.dal.SIAService(\"https://irsa.ipac.caltech.edu/cgi-bin/2MASS/IM/nph-im_sia?type=at&ds=asky&\")\n\n\n\n","type":"content","url":"/seds-in-firefly#retrieve-and-display-images","position":29},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Get 2MASS Images","lvl2":"Retrieve and Display Images"},"type":"lvl3","url":"/seds-in-firefly#get-2mass-images","position":30},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl3":"Get 2MASS Images","lvl2":"Retrieve and Display Images"},"content":"\n\ndef get_2mass_images(target, search_size=1*u.arcsec):\n    \"\"\"Retrieve 2MASS images\n\n    Parameters:\n    -----------\n    target: astropy.coordinates.SkyCoord\n        coordinates of the target(s)\n\n    search_size: astropy.units.Quantity\n        matching radius, defaults to 1*u.arcsec\n\n    Returns:\n    --------\n    j_filename: `str`\n         path to J-band image in FITS format\n\n    h_filename: `str`\n         path to H-band image in FITS format\n\n    k_filename: `str`\n         path to K-band image in FITS format\n    \"\"\"\n    im_table = irsa_sia.search(pos=target, size=search_size)\n    url_j = url_h = url_k = None\n    # Get URL for the first instance of each band\n    for i in range(len(im_table)):\n        if url_j is None and im_table[i][\"band\"] == \"J\":\n            url_j = im_table[i].getdataurl()\n        if url_h is None and im_table[i][\"band\"] == \"H\":\n            url_h = im_table[i].getdataurl()\n        if url_k is None and im_table[i][\"band\"] == \"K\":\n            url_k = im_table[i].getdataurl()\n    j_filename = download_file(url_j, cache=True)\n    h_filename = download_file(url_h, cache=True)\n    k_filename = download_file(url_k, cache=True)\n    return (j_filename, h_filename, k_filename)\n\n\n\n\n\nj_fname, h_fname, k_fname = get_2mass_images(target)\n\n\n\n\n\nfc.show_fits(fc.upload_file(j_fname), plot_id=\"2MASS J\", title=\"2MASS J\")\nfc.show_fits(fc.upload_file(h_fname), plot_id=\"2MASS H\", title=\"2MASS H\")\nfc.show_fits(fc.upload_file(k_fname), plot_id=\"2MASS K\", title=\"2MASS K\")\n\n\n\n\n\n\n\n\n\nfc.set_zoom(plot_id=\"2MASS J\", factor=4.0)\nfc.set_pan(plot_id=\"2MASS J\", x=target.ra.deg,\n           y=target.dec.deg, coord=\"j2000\")\nfc.set_zoom(plot_id=\"2MASS H\", factor=4.0)\nfc.set_pan(plot_id=\"2MASS H\", x=target.ra.deg,\n           y=target.dec.deg, coord=\"j2000\")\nfc.set_zoom(plot_id=\"2MASS K\", factor=4.0)\nfc.set_pan(plot_id=\"2MASS K\", x=target.ra.deg,\n           y=target.dec.deg, coord=\"j2000\")\n\n\n\nFor turning on “Align and Lock by WCS” in these images, use:\n\nfc.align_images(lock_match=True)\n\n\n\nNote that zooming or panning one image will do the same in the other images as well.\n\n\n\n","type":"content","url":"/seds-in-firefly#get-2mass-images","position":31},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"About This Notebook"},"type":"lvl2","url":"/seds-in-firefly#about-this-notebook","position":32},{"hierarchy":{"lvl1":"Using Firefly visualization tools in Python to vet SEDs","lvl2":"About This Notebook"},"content":"\n\nAuthor: David Shupe (IRSA Scientist) and Joyce Kim (IRSA Scientist) in conjunction with the IRSA Science Team\n\nUpdated: 2024-12-19\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/seds-in-firefly#about-this-notebook","position":33},{"hierarchy":{"lvl1":"IRSA cloud access introduction"},"type":"lvl1","url":"/cloud-access-intro","position":0},{"hierarchy":{"lvl1":"IRSA cloud access introduction"},"content":"This is the introductory tutorial demonstrating basic python access to the IRSA-curated images and catalogs available in AWS S3 cloud storage buckets.\n\nLearning Goals:\n\nFind datasets and connection information.\n\nBrowse buckets.\n\nFind an image and retrieve a cutout.\n\nNavigate a catalog and perform a basic query.\n\n","type":"content","url":"/cloud-access-intro","position":1},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"1. Cloud basics"},"type":"lvl2","url":"/cloud-access-intro#id-1-cloud-basics","position":2},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"1. Cloud basics"},"content":"AWS S3 is an \n\nobject store where the fundamental entities are “buckets” and “objects”.\nBuckets are containers for objects, and objects are blobs of data.\nUsers may be more familiar with \n\nfilesystem “files” and “directories”.\nFiles are analogous to objects, and the terms are often used interchangeably.\nWhile there is no directory-like structure in S3 itself, various layers on top of S3 have implemented functionality to mimic it.\nAs a result, users can generally interact using familiar filesystem logic, but may notice subtle differences.\n\nThe following S3 terms are also used in this notebook:\n\nKey: Unique ID for an object, relative to the bucket. Analogous to the full path to a file.\n\nPrefix: String of characters at the beginning of the object key. Analogous to the full path to a directory.\n\n","type":"content","url":"/cloud-access-intro#id-1-cloud-basics","position":3},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"1.1 General access","lvl2":"1. Cloud basics"},"type":"lvl3","url":"/cloud-access-intro#id-1-1-general-access","position":4},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"1.1 General access","lvl2":"1. Cloud basics"},"content":"Most of the common python methods used to read images and catalogs from a local disk can also be pointed at cloud storage buckets.\nThis includes methods like Astropy fits.open and Pandas read_parquet.\nThe cloud connection is handled by a separate library, usually \n\ns3fs, \n\nfsspec, or \n\npyarrow.fs.\n\nThe IRSA buckets are public and access is free.\nCredentials are not required.\nHowever, most tools will look for credentials by default and raise an error when none are found.\nTo access without credentials, users can make an anonymous connection, usually with a keyword argument such as anon=True.\nThis notebook demonstrates with the s3fs, astropy, and pyarrow libraries.\n\n","type":"content","url":"/cloud-access-intro#id-1-1-general-access","position":5},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"2. Find datasets and connection information"},"type":"lvl2","url":"/cloud-access-intro#id-2-find-datasets-and-connection-information","position":6},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"2. Find datasets and connection information"},"content":"A listing of the available datasets is maintained on \n\nIRSA’s Cloud Access page.\nIt can be used to substitute values for the following variables, which are used throughout this notebook and identified by all caps:\n\nCloud Access Variable\n\nDefinition\n\nBUCKET_NAME\n\nAWS S3 bucket name.\n\nBUCKET_REGION\n\nAWS region the bucket is in.\n\nIMAGES_PREFIX\n\nS3 prefix to the base of an image set.\n\nCATALOG_PREFIX\n\nS3 prefix to the base of a catalog.\n\nPARQUET_NAME\n\nPath to the base of the catalog’s Parquet dataset, relative to the prefix.\n\n","type":"content","url":"/cloud-access-intro#id-2-find-datasets-and-connection-information","position":7},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"3. Imports"},"type":"lvl2","url":"/cloud-access-intro#id-3-imports","position":8},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"3. Imports"},"content":"The following libraries will be used:\n\nastropy: perform image cutouts\n\nhpgeom: map sky location to catalog partition\n\nmatplotlib: view results\n\npandas: query Parquet datasets\n\npyarrow: work with Parquet datasets\n\npyvo: locate images in the cloud (pyvo>=1.5 required)\n\ns3fs: browse buckets\n\nLibraries are imported at the top of each section where used.\nThis cell will install them if needed:\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install astropy hpgeom pandas pyarrow 'pyvo>=1.5' s3fs matplotlib astroquery\n\n\n\n","type":"content","url":"/cloud-access-intro#id-3-imports","position":9},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"4. Browse buckets"},"type":"lvl2","url":"/cloud-access-intro#id-4-browse-buckets","position":10},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"4. Browse buckets"},"content":"Generally, it is not necessary to look through the buckets.\nThe Cloud Access page is the best way to understand what is available.\nHowever, browsing buckets can sometimes be useful.\n\nThe \n\ns3fs library offers a filesystem-like experience and is demonstrated below.\nBeware that these datasets can contain millions of files or more --\nif calls are taking a long time to return, try narrowing the search by adding additional levels to the prefix.\n\nUsers who want to interact with the buckets and objects more directly may instead prefer the \n\nAWS Boto3 library.\n\nimport s3fs\n\n\n\n\n\n# create an S3 client\ns3 = s3fs.S3FileSystem(anon=True)\n\n\n\nls the root directory of the Spitzer SEIP Super Mosaics image set:\n\nBUCKET_NAME = \"nasa-irsa-spitzer\"\nIMAGES_PREFIX = \"spitzer/seip/seip_science/images\"\n\ns3.ls(f\"{BUCKET_NAME}/{IMAGES_PREFIX}/\")\n\n\n\nUse wildcard (glob) matching to find mosaics.\nThis can be slow if looking through a large number of files, so we’ll add a few levels to the base prefix to reduce the number of files in the search.\nThe additional levels can be found using ls recursively or constructed using information on the Cloud Access page.\n\nsub_prefix = \"4/0019/40019821/9\"\nglob_pattern = \"**/*.mosaic.fits\"\n\ns3.glob(f\"{BUCKET_NAME}/{IMAGES_PREFIX}/{sub_prefix}/{glob_pattern}\")\n\n\n\n","type":"content","url":"/cloud-access-intro#id-4-browse-buckets","position":11},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"5. Find an image and retrieve a cutout"},"type":"lvl2","url":"/cloud-access-intro#id-5-find-an-image-and-retrieve-a-cutout","position":12},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"5. Find an image and retrieve a cutout"},"content":"A basic image cutout using \n\nAstropy is demonstrated in section 5.2.\nIt requires a bucket name and image key.\nSection 5.1 demonstrates how to query IRSA for this information using \n\nPyVO and a coordinate search.\n\nNote: There may be a delay between the time when a new dataset is made available in S3 and when IRSA’s image services are able to return the new cloud access information.\nIn this case, the IRSA URL that is returned by the service can be mapped to the cloud information.\nThis is explained further on the Cloud Access page, and also demonstrated in section 5.1 below.\n\nimport astropy.io\nimport json\nimport pyvo\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nfrom matplotlib import pyplot as plt\n\n\n\n","type":"content","url":"/cloud-access-intro#id-5-find-an-image-and-retrieve-a-cutout","position":13},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"5.1 Find an image using PyVO and a coordinate search","lvl2":"5. Find an image and retrieve a cutout"},"type":"lvl3","url":"/cloud-access-intro#id-5-1-find-an-image-using-pyvo-and-a-coordinate-search","position":14},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"5.1 Find an image using PyVO and a coordinate search","lvl2":"5. Find an image and retrieve a cutout"},"content":"\n\ncoords = SkyCoord(\"150.01d 2.2d\", frame=\"icrs\")\nsize = 0.01 * u.deg\n\n\n\nFor the full list of datasets that can be used with the collection parameter in the SIA search below, uncomment the next cell.\nNote that only some of these are currently available in cloud storage.\nTo request that additional datasets be made available in cloud storage, please contact IRSA’s \n\nHelp Desk.\n\n# from astroquery.ipac.irsa import Irsa\n\n# Irsa.list_collections()\n\n\n\nUse PyVO to execute a search for Spitzer SEIP Super Mosaics and find cloud access information:\n\nirsa_SIA = pyvo.dal.SIA2Service(\"https://irsa.ipac.caltech.edu/SIA\")\nseip_results = irsa_SIA.search((coords, size), collection=\"spitzer_seip\")\n\n# view cloud access info for the first few files overlapping the search area\nseip_results[\"cloud_access\"][:5]\n\n\n\nOption 1: Extract the cloud information from the “cloud_access” column for a single mosaic file:\n\n# find the first mosaic file in the results\n# use json to convert the string containing the cloud info to a dictionary\nmosaics = [i for i in seip_results[\"cloud_access\"] if \".mosaic.fits\" in i]\nseip_mosaic_cloud_info = json.loads(mosaics[0])\n\n# extract\nBUCKET_NAME = seip_mosaic_cloud_info[\"aws\"][\"bucket_name\"]\nimage_key = seip_mosaic_cloud_info[\"aws\"][\"key\"]\n\n\n\nOption 2: Construct the cloud information for a single mosaic file from the IRSA URL (useful if you know the dataset is in S3, but results in the “cloud_access” column were empty):\n\nBUCKET_NAME = \"nasa-irsa-spitzer\"\nIMAGES_PREFIX = \"spitzer/seip/seip_science/images\"\n\n# find the first mosaic URL in the results\nimage_url = [url for url in seip_results[\"access_url\"] if \".mosaic.fits\" in url][0]\n\n# construct the key by replacing the first part of the URL with the IMAGES_PREFIX\n# split the URL at the data product ID, the final level of the prefix\ndata_product_id = IMAGES_PREFIX.split(\"/\")[-1]\nkey_suffix = image_url.split(f\"/{data_product_id}/\")[-1]\nimage_key = f\"{IMAGES_PREFIX}/{key_suffix}\"\n\n\n\n","type":"content","url":"/cloud-access-intro#id-5-1-find-an-image-using-pyvo-and-a-coordinate-search","position":15},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"5.2 Retrieve a cutout using Astropy","lvl2":"5. Find an image and retrieve a cutout"},"type":"lvl3","url":"/cloud-access-intro#id-5-2-retrieve-a-cutout-using-astropy","position":16},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"5.2 Retrieve a cutout using Astropy","lvl2":"5. Find an image and retrieve a cutout"},"content":"Lazy-load data for better performance.\nThe relevant options are enabled by default when using astropy.io.fits.open with fsspec.\nIn addition, use the HDU section method in place of the usual data to avoid downloading the full data block.\n(See \n\nObtaining subsets from cloud-hosted FITS files.)\n\ns3_image_path = f\"s3://{BUCKET_NAME}/{image_key}\"\nwith astropy.io.fits.open(s3_image_path, fsspec_kwargs={\"anon\": True}) as hdul:\n    cutout = Cutout2D(hdul[0].section, position=coords, size=size, wcs=WCS(hdul[0].header))\n\n\n\nShow the cutout:\n\nplt.imshow(cutout.data, cmap=\"gray\")\n\n\n\n","type":"content","url":"/cloud-access-intro#id-5-2-retrieve-a-cutout-using-astropy","position":17},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"6. Navigate a catalog and perform a basic query"},"type":"lvl2","url":"/cloud-access-intro#id-6-navigate-a-catalog-and-perform-a-basic-query","position":18},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"6. Navigate a catalog and perform a basic query"},"content":"The catalogs are in Apache Parquet format and partitioned spatially using HEALPix order k=5.\nParquet datasets can be queried directly using SQL-like logic by applying filters during the file reads.\nFor queries that have a spatial constraint, including a filter on the partitioning column can greatly improve performance.\n\nSection 6.1 demonstrates how to view the catalog schema using \n\nPyArrow to find the column names, etc. needed for constructing queries.\nA basic spatial query using \n\nPandas is shown in section 6.2 and includes finding the relevant partitions using \n\nHPGeom.\n\nimport hpgeom\nimport pandas as pd\nimport pyarrow.dataset\nimport pyarrow.fs\nfrom matplotlib import colors\n\n\n\n\n\nBUCKET_NAME = \"nasa-irsa-wise\"\nBUCKET_REGION = \"us-west-2\"\n\nCATALOG_PREFIX = \"wise/allwise/catalogs/p3as_psd/healpix_k5\"\nPARQUET_NAME = \"wise-allwise.parquet\"\n\nparquet_root = f\"{BUCKET_NAME}/{CATALOG_PREFIX}/{PARQUET_NAME}\"\n\nfs = pyarrow.fs.S3FileSystem(region=BUCKET_REGION, anonymous=True)\n\n\n\n","type":"content","url":"/cloud-access-intro#id-6-navigate-a-catalog-and-perform-a-basic-query","position":19},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"6.1 View the schema using PyArrow","lvl2":"6. Navigate a catalog and perform a basic query"},"type":"lvl3","url":"/cloud-access-intro#id-6-1-view-the-schema-using-pyarrow","position":20},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"6.1 View the schema using PyArrow","lvl2":"6. Navigate a catalog and perform a basic query"},"content":"\n\n# load the schema from the \"_common_metadata\" file\ns3_schema_path = f\"{parquet_root}/_common_metadata\"\nschema = pyarrow.dataset.parquet_dataset(s3_schema_path, filesystem=fs).schema\n\n# the full schema can be quite large since catalogs often have hundreds of columns\n# but if you do want to look at the entire schema, uncomment the next line\n# schema\n\n\n\nSearch through the column names to find the HEALPix partitioning columns:\n\n[name for name in schema.names if \"healpix\" in name]\n# the result shows that both orders k=0 and k=5 are available, but typical use cases only need k=5\n\n\n\nLook at a specific column (“ext_flg” will be used below to query for likely stars):\n\n# this will display basic information like name and type\nschema.field(\"ext_flg\")\n\n\n\n\n\n# units and descriptions are in the field's metadata attribute\nschema.field(\"ext_flg\").metadata\n\n\n\n","type":"content","url":"/cloud-access-intro#id-6-1-view-the-schema-using-pyarrow","position":21},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"6.2 Perform a basic spatial search with Pandas","lvl2":"6. Navigate a catalog and perform a basic query"},"type":"lvl3","url":"/cloud-access-intro#id-6-2-perform-a-basic-spatial-search-with-pandas","position":22},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl3":"6.2 Perform a basic spatial search with Pandas","lvl2":"6. Navigate a catalog and perform a basic query"},"content":"Query the AllWISE catalog for likely stars within an RA/Dec polygon.\n\nSpatial limits roughly covering the Taurus L1495 star-forming region:\n\nra_min, ra_max, dec_min, dec_max = 62, 66, 25, 29  # deg\n\npolygon_corners = [(ra_min, dec_min), (ra_min, dec_max), (ra_max, dec_max), (ra_max, dec_min)]\ncorners = list(zip(*polygon_corners))  # [(ra values), (dec values)]\n\n\n\nFind the partitions (HEALPix pixel indexes) that overlap the polygon:\n\nk = 5\npolygon_pixels = hpgeom.query_polygon(\n    a=corners[0], b=corners[1], nside=hpgeom.order_to_nside(k), inclusive=True\n)\n\n\n\nQuery:\n\nresults_df = pd.read_parquet(\n    parquet_root,\n    filesystem=fs,\n    # columns to be returned. similar to a SQL SELECT clause.\n    columns=[\"designation\", \"w1mpro\", \"w2mpro\", \"w3mpro\", \"w4mpro\", f\"healpix_k{k}\"],\n    # row filters. similar to a SQL WHERE clause.\n    # supported operators: ==, !=, <, >, <=, >=, in, not in\n    # tuple conditions are joined by AND (for OR, use a list of lists)\n    filters=[\n        (\"ext_flg\", \"==\", 0),\n        (\"ra\", \">\", ra_min),\n        (\"ra\", \"<\", ra_max),\n        (\"dec\", \">\", dec_min),\n        (\"dec\", \"<\", dec_max),\n        # include filter on partition column for efficiency. similar to a database index.\n        (f\"healpix_k{k}\", \"in\", polygon_pixels),\n    ],\n)\n\n\n\nView results on a color-color diagram:\n\nresults_df[\"W1-W2\"] = results_df[\"w1mpro\"] - results_df[\"w2mpro\"]\nresults_df[\"W2-W3\"] = results_df[\"w2mpro\"] - results_df[\"w3mpro\"]\nresults_df.plot.hexbin(\"W2-W3\", \"W1-W2\", norm=colors.LogNorm(vmin=1, vmax=500))\n\n\n\n\n\n","type":"content","url":"/cloud-access-intro#id-6-2-perform-a-basic-spatial-search-with-pandas","position":23},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"About this notebook"},"type":"lvl2","url":"/cloud-access-intro#about-this-notebook","position":24},{"hierarchy":{"lvl1":"IRSA cloud access introduction","lvl2":"About this notebook"},"content":"Author: Troy Raen (IRSA Developer) in conjunction with Brigitta Sipőcz, Jessica Krick and the IPAC Science Platform team\n\nUpdated: 2024-07-29\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/cloud-access-intro#about-this-notebook","position":25},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23"},"type":"lvl1","url":"/irsa-hats-with-lsdb","position":0},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23"},"content":"","type":"content","url":"/irsa-hats-with-lsdb","position":1},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"Learning Goals"},"type":"lvl2","url":"/irsa-hats-with-lsdb#learning-goals","position":2},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will learn how to:\n\nUse the lsdb library to access IRSA HATS Collections from the cloud.\n\nDefine spatial, column, and row filters to read only a portion of large HATS catalogs.\n\nCrossmatch catalogs using lsdb and visualize the results.\n\nPerform index searches on HATS catalogs using lsdb.\n\n","type":"content","url":"/irsa-hats-with-lsdb#learning-goals","position":3},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"Introduction"},"type":"lvl2","url":"/irsa-hats-with-lsdb#introduction","position":4},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"Introduction"},"content":"IRSA provides several astronomical catalogs in spatially partitioned \n\nParquet format, enabling efficient access and scalable analysis of large datasets.\nAmong these are the \n\nHATS (Hierarchical Adaptive Tiling Scheme) collections, which organize catalog data using adaptive \n\nHEALPix-based partitioning for parallelizable queries and crossmatching.\n\nFor more details on HATS collections, partitioning, and schema organization, see the IRSA documentation on \n\nParquet catalogs.\n\nThis notebook demonstrates how to access and analyze HATS collections using the \n\nlsdb Python library, which makes it convenient to work with these large datasets.\nWe will explore the following IRSA HATS collections in this tutorial:\n\nEuclid Q1 Merged Objects: 14 Euclid Q1 tables, including MER (multi-wavelength mosaics) final catalog, photometric redshift catalogs, and spectroscopic catalogs joined on MER Object ID.\n\nZTF DR23 Objects Table: catalog of PSF-fit photometry detections extracted from ZTF reference images, including “collapsed-lightcurve” metrics.\n\nZTF DR23 Lightcurves: catalog of PSF-fit photometry detections extracted from single-exposure images at the locations of Objects Table detections.\n\nWe will use lsdb to leverage HATS partitioning for performing fast spatial queries on a relatively large area of sky and for crossmatching sources between Euclid and ZTF to identify variable galaxy candidates and visualize their light curves.\n\n","type":"content","url":"/irsa-hats-with-lsdb#introduction","position":5},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"Imports"},"type":"lvl2","url":"/irsa-hats-with-lsdb#imports","position":6},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"Imports"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install s3fs \"lsdb>=0.6.6,<0.8\" pyarrow pandas numpy astropy dask matplotlib\n\n\n\n\n\nimport s3fs\nimport lsdb\nimport pyarrow.parquet as pq\nimport numpy as np\nimport pandas as pd\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nimport os\nimport matplotlib.pyplot as plt\nfrom dask.distributed import Client\n\n\n\n\n\npd.set_option(\"display.max_colwidth\", None) # schema descriptions can be long\npd.set_option(\"display.min_rows\", 18) # to show more rows when truncated view\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#imports","position":7},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"1. Locate HATS collections in the cloud"},"type":"lvl2","url":"/irsa-hats-with-lsdb#id-1-locate-hats-collections-in-the-cloud","position":8},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"1. Locate HATS collections in the cloud"},"content":"From IRSA’s \n\ncloud data access page, let’s identify the bucket and path prefix for the HATS Collections for Euclid Q1 and ZTF DR23:\n\neuclid_q1_bucket = \"nasa-irsa-euclid-q1\"\neuclid_q1_hats_prefix = \"contributed/q1/merged_objects/hats\"\n\nztf_bucket = \"ipac-irsa-ztf\"\nztf_hats_prefix = \"contributed/dr23/objects/hats\"\nztf_lc_hats_prefix = \"contributed/dr23/lc/hats\"\n\n\n\ns3fs provides a filesystem-like Python interface for AWS S3 buckets.\nFirst, we create an S3 client:\n\ns3 = s3fs.S3FileSystem(anon=True)\n\n\n\nUsing the S3 client, we can now list the contents of the Euclid Q1 HATS collection:\n\ns3.ls(f\"{euclid_q1_bucket}/{euclid_q1_hats_prefix}\")\n\n\n\nIn this collection, you can see collection properties, catalog, index table, and margin cache in order.\nYou can explore more directories to see how this HATS collection follows the directory structure described in IRSA’s documentation on \n\nHATS partitioning and HATS Collections.\n\nAs per the documentation, the Parquet file containing the schema for this euclid_q1_merged_objects-hats catalog is stored in dataset/_common_metadata directory.\nYou can find the catalog schema through other sources as well but we use the _common_metadata file here because it contains column metadata (units and descriptions) which the other schemas don’t have.\nLet’s save its path for later use:\n\neuclid_q1_schema_path = \"euclid_q1_merged_objects-hats/dataset/_common_metadata\" # euclid_q1_merged_objects-hats is the catalog name we identified above\n\n\n\nSimilarly, we can list the contents of the ZTF DR23 Objects HATS collection and save the path to its schema file:\n\ns3.ls(f\"{ztf_bucket}/{ztf_hats_prefix}\")\n\n\n\n\n\nztf_schema_path = \"ztf_dr23_objects-hats/dataset/_common_metadata\" # ztf_dr23_objects-hats is the catalog name we identified above\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-1-locate-hats-collections-in-the-cloud","position":9},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"2. Open the HATS catalogs"},"type":"lvl2","url":"/irsa-hats-with-lsdb#id-2-open-the-hats-catalogs","position":10},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"2. Open the HATS catalogs"},"content":"Now we can use \n\nlsdb to open the HATS catalogs we identified above, starting with the Euclid Q1 catalog.\nNote that we can simply pass the HATS collection path and it will automatically identify the catalog inside it and make use of the ancillary data (index table and margin cache) when needed.\nAlso, we prefix the path with s3:// to indicate that it is an S3 path.\n\neuclid_catalog = lsdb.open_catalog(f\"s3://{euclid_q1_bucket}/{euclid_q1_hats_prefix}\")\neuclid_catalog\n\n\n\nFew things to note here:\n\nThis catalog is opened \n\nlazily, i.e., no data is read from the S3 bucket into memory yet.\n\nSince we did not specify any columns to select from this very wide catalog, we get 7 default columns out of 1593 available columns!\n\nWe see some of the HEALPix orders and pixels at which this catalog is partitioned.\n\nLet’s plot the sky coverage of this catalog:\n\neuclid_catalog.plot_pixels()\n\n\n\n\n\nSince HATS uses adaptive tile-based partitioning, we see higher order HEALPix tiles in regions with higher source density and lower order tiles in regions with lower source density.\n\nNow let’s open the ZTF DR23 Objects catalog in a similar manner and view its sky coverage:\n\nztf_catalog = lsdb.open_catalog(f\"s3://{ztf_bucket}/{ztf_hats_prefix}\")\nztf_catalog\n\n\n\n\n\nztf_catalog.plot_pixels()\n\n\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-2-open-the-hats-catalogs","position":11},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"type":"lvl2","url":"/irsa-hats-with-lsdb#id-3-define-filters-for-querying-euclid-q1-catalog","position":12},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"content":"","type":"content","url":"/irsa-hats-with-lsdb#id-3-define-filters-for-querying-euclid-q1-catalog","position":13},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.1 Define a spatial filter","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-3-1-define-a-spatial-filter","position":14},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.1 Define a spatial filter","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"content":"From the above two plots, it’s clear that the ZTF DR23 Objects catalog covers a much larger area of the sky compared to the Euclid Q1 catalog.\nSo for cross-matching, we can safely pick the Euclid Deep Field North (EDF-N) region that is covered by both catalogs.\nFrom the \n\nEuclid user guide at IRSA Table 1, we identify the following for the EDF-N region:\n\neuclid_DF_N_center = SkyCoord('17:58:55.9 +66:01:03.7', unit=('hourangle', 'deg'))\neuclid_DF_N_center\n\n\n\n\n\n# euclid_DF_N_radius = 3 * u.deg  # ceil(sqrt(22.9 / pi)) approximate radius to cover almost entire EDF-N\neuclid_DF_N_radius = 0.5 * u.deg  # smaller radius to reduce execution time for this tutorial\neuclid_DF_N_radius\n\n\n\nNow, using lsdb, we define a cone \n\nsearch object for this region that can be passed as a spatial filter when querying the catalogs:\n\nspatial_filter = lsdb.ConeSearch(\n    ra=euclid_DF_N_center.ra.deg,\n    dec=euclid_DF_N_center.dec.deg,\n    radius_arcsec=euclid_DF_N_radius.to(u.arcsec).value\n)\n\n\n\nWe can overplot this spatial filter on the Euclid catalog’s sky coverage to verify (note we limited the field of view so that we can see the cone better):\n\neuclid_catalog.plot_pixels(fov=(8 * u.deg, 6 * u.deg), center=euclid_DF_N_center)\nspatial_filter.plot(fc=\"#00000000\", ec=\"red\");\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-3-1-define-a-spatial-filter","position":15},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.2 Identify columns of interest from the schema","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-3-2-identify-columns-of-interest-from-the-schema","position":16},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.2 Identify columns of interest from the schema","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"content":"Before we can define column and row filters for querying any HATS catalog, we need to know what columns are available in that catalog.\nSo let’s inspect the schema of the Euclid Q1 catalog.\n\nUsing pyarrow parquet, we can \n\nread the schema from the _common_metadata parquet file we identified above (note that we again prefixed the path with s3:// to indicate that it is an S3 path):\n\neuclid_schema = pq.read_schema(\n    f\"s3://{euclid_q1_bucket}/{euclid_q1_hats_prefix}/{euclid_q1_schema_path}\",\n    filesystem=s3\n)\ntype(euclid_schema)\n\n\n\nSince the schema of this catalog is a pyarrow object, let’s convert it to a pandas DataFrame for easier inspection:\n\ndef pq_schema_to_df(schema):\n    \"\"\"Convert a PyArrow schema to a Pandas DataFrame.\"\"\"\n    return pd.DataFrame(\n        [\n            (\n                field.name,\n                str(field.type),\n                field.metadata.get(b\"unit\", b\"\").decode(),\n                field.metadata.get(b\"description\", b\"\").decode()\n            )\n            for field in schema # schema is an iterable of pyarrow fields\n        ],\n        columns=[\"name\", \"type\", \"unit\", \"description\"]\n    )\n\neuclid_schema_df = pq_schema_to_df(euclid_schema)\nlen(euclid_schema_df)\n\n\n\nAs we saw above, the Euclid HATS catalog contains 1594 columns in its schema!\n\nThe columns at the beginning of this schema (such as “tileid”, “objectid”, “ra”, “dec”) define positions and IDs.\nThese can be useful for our column filters — the columns we want to SELECT when querying the catalog.\n\nWe can also filter the schema DataFrame by name, unit, type, etc., to identify columns most relevant for our row filters — WHERE rows satisfy conditions on column values for our query.\nFor example, let’s explore the columns that are part of the PHZ (photometric redshift) catalog to identify photometric redshifts and source types:\n\neuclid_schema_df[\n    euclid_schema_df[\"name\"].str.startswith(\"phz_\") # phz_ prefix is for PHZ catalog columns in this merged catalog\n    & euclid_schema_df[\"type\"].str.contains(\"int\") # to see flag type columns\n]\n\n\n\n“phz_phz_median” gives us the median photometric redshift of sources, while “phz_phz_classification” can be used to filter the Euclid catalog to only galaxy-type sources.\n\nYou can explore the schema DataFrame further to identify any other columns of interest.\nIt is also useful to go through the description of Euclid Q1 data products and papers that contain different PHZ, MER, etc. catalog tables that are merged in this HATS catalog, which are linked in the \n\nEuclid Q1 Merged Objects HATS Catalog: Introduction.\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-3-2-identify-columns-of-interest-from-the-schema","position":17},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.3 Define column filters","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-3-3-define-column-filters","position":18},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.3 Define column filters","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"content":"\n\nFor this tutorial, the following columns are most relevant to us:\n\neuclid_columns = euclid_schema_df['name'].iloc[:7].tolist() # the positional and ID columns\neuclid_columns.extend([\n    'phz_phz_median', # median of the photometric redshift PDF\n])\n\n\n\nThe selected columns with their metadata are:\n\neuclid_schema_df[euclid_schema_df[\"name\"].isin(euclid_columns)]\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-3-3-define-column-filters","position":19},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.4 Define row filters","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-3-4-define-row-filters","position":20},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"3.4 Define row filters","lvl2":"3. Define filters for querying Euclid Q1 catalog"},"content":"\n\nTo identify the galaxies with quality redshifts in this catalog, the following filters can be applied to the rows (from \n\nTucci et al. 2025 section 5.3):\n\nNote that these filters are defined as lists of lists, where each inner list represents a single condition in the format [column_name, operator, value] and the conditions are combined using logical AND.\nMore on filters that lsdb supports can be found in its \n\ndocumentation.\n\ndef magnitude_to_flux(magnitude: float) -> float:\n    \"\"\"Convert magnitude to flux following the MER Photometry Cookbook.\"\"\"\n    zeropoint = 23.9\n    flux = 10 ** ((magnitude - zeropoint) / -2.5)\n    return flux\n\ngalaxy_filters = [\n    ['phz_phz_classification', '==', 2], # 2 is for galaxies\n    ['mer_vis_det', '==', 1], # No NIR-only objects\n    ['mer_spurious_flag', '==', 0], # not spurious\n    ['mer_flux_detection_total', '>', magnitude_to_flux(24.5)],  # I < 24.5\n    # lsdb doesn't support column arithmetic in filters yet, so can rather apply the S/N > 5 filter after loading the data\n    # ['mer_flux_detection_total', '/', 'mer_flux_detection_error_total', '>', 5], # I band S/N > 5\n]\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-3-4-define-row-filters","position":21},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"4. Define filters for querying ZTF DR23 Objects catalog"},"type":"lvl2","url":"/irsa-hats-with-lsdb#id-4-define-filters-for-querying-ztf-dr23-objects-catalog","position":22},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"4. Define filters for querying ZTF DR23 Objects catalog"},"content":"Since we are interested in cross-matching ZTF DR23 Objects with the Euclid Q1 catalog, we will use the same spatial filter defined above.\n\nWe can define column and row filters for the ZTF catalog based on its schema, similar to the previous section:\n\nztf_schema = pq.read_schema(\n    f\"s3://{ztf_bucket}/{ztf_hats_prefix}/{ztf_schema_path}\",\n    filesystem=s3\n)\nztf_schema_df = pq_schema_to_df(ztf_schema)\n\n\n\nYou can filter the schema further by units, type, etc. to identify other columns of interest.\nIt’s also useful to go through the \n\nZTF DR23 release notes and \n\nexplanatory supplement at IRSA for more details on column selections and caveats.\n\nFor this tutorial, the following columns are most relevant to us:\n\nztf_columns = ztf_schema_df[\"name\"].tolist()[:6]\nztf_columns.extend([\n    'fid', 'filtercode',\n    'ngoodobsrel',\n    'chisq', 'magrms', 'meanmag', 'medianabsdev'\n])\n\n\n\nThe rationale for selecting these columns is as follows:\n\nztf_schema_df[ztf_schema_df.name.isin(ztf_columns)]\n\n\n\nFor a quality cut, we apply the following filter on the number of good epochs (from \n\nCoughlin et al. (2021)) section 2):\n\nquality_filters = [\n    [\"ngoodobsrel\", \">\", 50]\n]\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-4-define-filters-for-querying-ztf-dr23-objects-catalog","position":23},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"type":"lvl2","url":"/irsa-hats-with-lsdb#id-5-crossmatch-euclid-q1-x-ztf-dr23-objects","position":24},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"content":"Now that we have defined the spatial, column, and row filters for both catalogs, we can use lsdb to perform the crossmatch.\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-5-crossmatch-euclid-q1-x-ztf-dr23-objects","position":25},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"5.1 Create crossmatch query using the filters","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-5-1-create-crossmatch-query-using-the-filters","position":26},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"5.1 Create crossmatch query using the filters","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"content":"Following lsdb’s documentation on \n\nperforming filters, we open the Euclid Q1 catalog with the defined spatial, column, and row filters:\n\neuclid_cone = lsdb.open_catalog(\n    f\"s3://{euclid_q1_bucket}/{euclid_q1_hats_prefix}\",\n    columns=euclid_columns,\n    search_filter=spatial_filter,\n    filters=galaxy_filters\n)\neuclid_cone\n\n\n\nNotice how the number of partitions reduced from 114 (in section 2) to only a few partitions that are within the spatial filter.\nThis allows LSDB to avoid loading unused parts of the catalog.\nAlso, we see only the columns we selected in the column filters.\n\nNow, similarly, we open the ZTF DR23 Objects catalog with its defined filters:\n\nztf_cone = lsdb.open_catalog(\n    f\"s3://{ztf_bucket}/{ztf_hats_prefix}\",\n    columns=ztf_columns,\n    search_filter=spatial_filter,\n    filters=quality_filters\n)\nztf_cone\n\n\n\nWe see a similar reduction in the number of partitions for this catalog as well, and our column selection is applied.\n\nNow we can use lsdb’s \n\ncrossmatch functionality to plan a crossmatch between these two filtered catalogs.\nNote that the order of crossmatch matters: lsdb’s algorithm takes each object in the left catalog and finds the closest spatial match from the right catalog.\nSince Euclid is denser than ZTF, we put it on the left side of the crossmatch to maximize the number of matches.\nFor more details on the parameters, refer to the documentation linked.\n\nNote\n\nSince ZTF objects are defined per band, setting n_neighbors=1 means this is only going to return data in one ZTF band for each Euclid object. This is done for simplicity of analysing the crossmatched results in subsequent sections but you can increase n_neighbors to get data in multiple ZTF bands.\n\neuclid_x_ztf = euclid_cone.crossmatch(\n    ztf_cone,\n    suffixes=(\"_euclid\", \"_ztf\"), # to distinguish columns from the two catalogs\n    n_neighbors=1, # default is 1 too, can be tweaked\n    radius_arcsec=1  # default is 1 arcsec too, can be tweaked\n)\neuclid_x_ztf\n\n\n\n\n\nNote how the resulting crossmatched catalog contains columns from both catalogs with the suffixes we passed.\nAs we noticed when opening the catalog, this created a crossmatch catalog object lazily but did not execute the crossmatch.\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-5-1-create-crossmatch-query-using-the-filters","position":27},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"5.2 Execute the query and load data","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-5-2-execute-the-query-and-load-data","position":28},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"5.2 Execute the query and load data","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"content":"Now we can run the crossmatch query we have been planning so far by using the compute() method of the crossmatch catalog object, which will perform all the tasks and return the result as a DataFrame.\nlsdb utilizes \n\nDask to parallelize the tasks on different partitions, so we need to create a Dask client to manage the resources (and track progress) for the scope of this computation:\n\ndef get_nworkers(catalog):\n    return min(os.cpu_count(), catalog.npartitions + 1)\n\nwith Client(n_workers=get_nworkers(euclid_x_ztf),\n            threads_per_worker=2,\n            memory_limit='auto') as client:\n    print(f\"This may take more than a few minutes to complete. You can monitor progress in Dask dashboard at {client.dashboard_link}\")\n    euclid_x_ztf_df = euclid_x_ztf.compute() # this will load the data into memory finally\n\n\n\n\n\neuclid_x_ztf_df\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-5-2-execute-the-query-and-load-data","position":29},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"5.3 Identify objects of interest from the crossmatch","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"type":"lvl3","url":"/irsa-hats-with-lsdb#id-5-3-identify-objects-of-interest-from-the-crossmatch","position":30},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl3":"5.3 Identify objects of interest from the crossmatch","lvl2":"5. Crossmatch: Euclid Q1 x ZTF DR23 Objects"},"content":"\n\nCheck the number of unique Euclid and ZTF sources in the crossmatched catalog:\n\neuclid_x_ztf_df.shape[0], euclid_x_ztf_df['object_id_euclid'].nunique(), euclid_x_ztf_df['oid_ztf'].nunique()\n\n\n\nThis means there is one unique Euclid source for each row in the crossmatched catalog as expected (since we put Euclid on the left side of the crossmatch).\nBut for ZTF, this is not true as some ZTF objects have multiple Euclid matches since ZTF has lower resolution than Euclid.\n\nNow let’s plot some variability metrics from ZTF against Euclid redshift to see if there are any trends.\nWe will use hexbin plots to visualize the density of sources in each panel:\n\nz = euclid_x_ztf_df[\"phz_phz_median_euclid\"].to_numpy() # x-axis\nmetrics = [ # y-axes\n    (\"magrms_ztf\",        \"ZTF mag RMS\"),\n    (\"chisq_ztf\",         \"ZTF χ²\"),\n    (\"medianabsdev_ztf\",  \"ZTF MAD\"),\n    (\"ngoodobsrel_ztf\",   \"ZTF # good obs\"),\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6), constrained_layout=True, sharex=True)\naxes = axes.ravel()\n\nxmin, xmax = 0, 2.5\ngridsize = 48  # resolution: larger => finer grid\n\nfor i, (col, ylabel) in enumerate(metrics):\n    ax = axes[i]\n    y = euclid_x_ztf_df[col].to_numpy()\n\n    # clip y to robust range (1–99th percentile) for visibility\n    y_lo, y_hi = np.nanpercentile(y, [1, 99])\n\n    hb = ax.hexbin(\n        z, y,\n        gridsize=gridsize,\n        extent=(xmin, xmax, y_lo, y_hi),\n        mincnt=1,\n        bins='log'  # logarithmic color scale of counts\n    )\n\n    # ax.set_xlim(xmin, xmax)\n    # ax.set_ylim(y_lo, y_hi)\n    ax.set_ylabel(ylabel)\n\n    # x-label only on bottom row to reduce clutter\n    if i // 2 == 1:\n        ax.set_xlabel(\"Euclid redshift\")\n\n\n# Shared colorbar across all panels (uses last hexbin as mappable)\ncbar = fig.colorbar(hb, ax=axes[:-1], shrink=0.9, label=\"Galaxy sources count (log)\")\n\nfig.suptitle(\"Euclid redshift vs ZTF variability metrics\", y=1.04)\nplt.show()\n\n\n\nThere’s no strong variability trend for higher redshift galaxies (z > 1) as the distributions of mag RMS, MAD, and Chi-squared are fairly stable with redshift.\nThis is likely because of the limited sensitivity of ZTF, as there are clearly fewer good observations as redshift increases.\n\nDespite this, we can still select some high-variability galaxy sources from the low redshift (z < 1) range for further analysis.\nLet’s focus only on Chi-squared (measure of significance) and RMS magnitude (measure of variability amplitude; similar to MAD) metrics:\n\neuclid_x_ztf_df['chisq_ztf'].describe()\n\n\n\n\n\nchisq_threshold = euclid_x_ztf_df['chisq_ztf'].quantile(0.95)\nchisq_threshold\n\n\n\n\n\neuclid_x_ztf_df['magrms_ztf'].describe()\n\n\n\n\n\nmagrms_threshold = euclid_x_ztf_df['magrms_ztf'].quantile(0.95)\nmagrms_threshold\n\n\n\n\n\nvariable_galaxies = euclid_x_ztf_df.query(\n    f\"chisq_ztf >= {chisq_threshold} & magrms_ztf >= {magrms_threshold}\"\n    ).sort_values(\"chisq_ztf\", ascending=False) # sort by significant variability\n\n\n\nLet’s inspect the top variable galaxies that we can use for plotting their light curves:\n\ncols_of_interest = [\"oid_ztf\", \"fid_ztf\", \"filtercode_ztf\", \"phz_phz_median_euclid\", \"magrms_ztf\", \"chisq_ztf\"]\ntop_variable_galaxies = variable_galaxies.head(3)\ntop_variable_galaxies[cols_of_interest]\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-5-3-identify-objects-of-interest-from-the-crossmatch","position":31},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"6. Load ZTF lightcurves by object IDs"},"type":"lvl2","url":"/irsa-hats-with-lsdb#id-6-load-ztf-lightcurves-by-object-ids","position":32},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"6. Load ZTF lightcurves by object IDs"},"content":"\n\nLet’s open the ZTF light curves HATS catalog.\nNote that we are using the same spatial filter as before.\nWe do not specify any column filters, because the default columns already contain the light curve data we need.\nWe also do not apply any row filters, since we will directly filter rows by object IDs.\n\nztf_lc_catalog = lsdb.open_catalog(f\"s3://{ztf_bucket}/{ztf_lc_hats_prefix}\",\n                                   search_filter=spatial_filter\n                                   )\nztf_lc_catalog\n\n\n\nWe can inspect the catalog’s collection properties and find which columns have ancillary index tables (as explained in lsdb’s documentation on \n\nusing index tables).\n\nztf_lc_idx_column = list(ztf_lc_catalog.hc_collection.all_indexes.keys())[0]\nztf_lc_idx_column\n\n\n\nLet’s just extract the object IDs of the top variable galaxies we identified above:\n\nztf_oids = top_variable_galaxies['oid_ztf'].tolist()\nztf_oids\n\n\n\nNow we can use these IDs to filter the ZTF light curves catalog by the id_search method:\n\nztf_lcs = ztf_lc_catalog.id_search(values={ztf_lc_idx_column: ztf_oids})\nztf_lcs\n\n\n\n\n\nAs earlier, this creates a lazy catalog object with the partition(s) that contains our IDs.\nWe can load the light curves data into a DataFrame by using the compute() method:\n\nztf_lcs_df = ztf_lcs.compute() # ID search runs out of memory if we try to parallelize it with Dask client\nztf_lcs_df\n\n\n\nLet’s merge this light curves DataFrame with the crossmatched catalog (with variability filters and sorting) so that we can use collapsed light curves metrics for annotating the plots:\n\nmerged_ztf_lcs_df = ztf_lcs_df.merge(\n    top_variable_galaxies[cols_of_interest].set_index('oid_ztf'),\n    left_on='objectid', right_index=True\n).set_index('objectid', drop=False)\n\n# Sort by the order of top_variable_galaxies\nmerged_ztf_lcs_df = merged_ztf_lcs_df.loc[ztf_oids]\nmerged_ztf_lcs_df\n\n\n\nNow we can finally plot the light curves of these variable galaxy candidates.\nWe need to filter out bad epochs using the catflags column as explained in the \n\nZTF DR23 explanatory supplement section 13.6.\nThen, we will plot the light curves in separate panels for each object:\n\nfig, axs = plt.subplots(len(merged_ztf_lcs_df), 1, figsize=(10, 4 * len(merged_ztf_lcs_df)), constrained_layout=True)\n\nif len(merged_ztf_lcs_df) == 1:\n    axs = [axs]\n\nfor ax, (_, row) in zip(axs, merged_ztf_lcs_df.iterrows()):\n    lc = row['lightcurve'].query(\"catflags == 0\")  # remove bad epochs\n    lc_label = (f'ZTF DR23 Object {row[\"objectid\"]} in band {row[\"filtercode_ztf\"][1]}\\n'\n                f'(z={row[\"phz_phz_median_euclid\"]:.2f}, RMS mag={row[\"magrms_ztf\"]:.3f}, Chi-sq={row[\"chisq_ztf\"]:.3f})')\n    pts = ax.plot(lc['hmjd'], lc['mag'], '.', markersize=4, label=lc_label, zorder=3)\n    ax.errorbar(\n        lc['hmjd'], lc['mag'], yerr=lc['magerr'],\n        fmt='none', ecolor=pts[0].get_color(), elinewidth=0.8, capsize=0, alpha=0.3, zorder=2\n    )\n    ax.set_ylabel('Magnitude')\n    ax.set_xlabel('HMJD')\n    ax.invert_yaxis()\n    ax.set_title(lc_label, fontsize=10)\n\nfig.suptitle(\"Lightcurves of Variable Galaxy Candidates\", fontsize=14, y=1.02)\nfig.set_constrained_layout_pads(h_pad=0.15)\nplt.show()\n\n\n\n","type":"content","url":"/irsa-hats-with-lsdb#id-6-load-ztf-lightcurves-by-object-ids","position":33},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"About this notebook"},"type":"lvl2","url":"/irsa-hats-with-lsdb#about-this-notebook","position":34},{"hierarchy":{"lvl1":"Access HATS Collections Using LSDB: Euclid Q1 and ZTF DR23","lvl2":"About this notebook"},"content":"Author: Jaladh Singhal, Troy Raen, Jessica Krick, Brigitta Sipőcz, Vandana Desai, and the IRSA Data Science Team\n\nUpdated: 2025-10-02\n\nContact: the \n\nIRSA Helpdesk with questions or to report problems.","type":"content","url":"/irsa-hats-with-lsdb#about-this-notebook","position":35},{"hierarchy":{"lvl1":"Techniques & Tools Tutorial Notebooks"},"type":"lvl1","url":"/techniques","position":0},{"hierarchy":{"lvl1":"Techniques & Tools Tutorial Notebooks"},"content":"This section gathers tutorials that go beyond mission specific data access and analysis to explore broader methods, workflows, and utilities applicable across missions and datasets.\nThis collection includes guidance on cloud-based data access, use of powerful big data tools such as lsdb, interactive visualization tools, and computational techniques like parallel image convolution.\n\nCloud Access — Access and work with data served by IRSA from AWS S3 cloud storage.\n\nHATS with LSDB - Use LSDB for user-friendly cross matching and querying of HATS Collections.\n\nSED Visualization - Explore and interactively visualize multi-wavelength SEDs using Firefly.\n\nParallelization - Apply parallelization techniques to speed up operations on large astronomical images.","type":"content","url":"/techniques","position":1},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects"},"type":"lvl1","url":"/neowise-light-curve-demo","position":0},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects"},"content":"","type":"content","url":"/neowise-light-curve-demo","position":1},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Learning Goals"},"type":"lvl2","url":"/neowise-light-curve-demo#learning-goals","position":2},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nConstruct a TAP query to download the necessary data and visualize it via the web browser with an instantiated Firefly environment.\n\nPlot light curves from NEOWISE data using the Firefly Python API.\n\nOverlay a catalog of data onto a HiPS image.\n\n","type":"content","url":"/neowise-light-curve-demo#learning-goals","position":3},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Introduction"},"type":"lvl2","url":"/neowise-light-curve-demo#introduction","position":4},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Introduction"},"content":"This tutorial demonstrates how to plot light curves from NEOWISE data while also flaunting the many useful capabilities of the Firefly Python API. Using the ‘Known Solar System Object Possible Association List’ catalog from the NEOWISE-R database, we can easily compose a light curve of the faint asteroid 558 Carmen and show its observed positions in the sky solely through the firefly_client package. Minor planet light curves are important in determining their size, spectral class, rotation period and many other properties.\n\nFirefly is an astronomical data access and visualization written by Caltech/IPAC-IRSA. The visualization provides user with an integrated experience with brushing and linking capabilities among images, catalogs, and plots. Firefly is used in IRSA GUIs to query and visualize data from missions such as WISE, Spitzer, SOFIA, ZTF, PTF, etc. and a large number of highly-used contributed data products from a diverse set of astrophysics projects.\n\nThe firefly_client package provides a lightweight client class that includes a Python interface to Firefly’s Javascript API.\n\nFor documentation on the firefly client visit \n\nhttps://​caltech​-ipac​.github​.io​/firefly​_client/.\n\n","type":"content","url":"/neowise-light-curve-demo#introduction","position":5},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Imports"},"type":"lvl2","url":"/neowise-light-curve-demo#imports","position":6},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Imports"},"content":"firefly_client FireflyClient - Python API to Firefly for displaying tables, images and charts\n\nastropy.utils.data for downloading the catalog data via TAP query\n\nurllib.parse for converting regular query string to url-safe string\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install firefly_client astropy\n\n\n\n\n\nfrom firefly_client import FireflyClient\nimport astropy.utils.data\nimport urllib.parse\n\n\n\n","type":"content","url":"/neowise-light-curve-demo#imports","position":7},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Instantiate the Firefly client"},"type":"lvl2","url":"/neowise-light-curve-demo#instantiate-the-firefly-client","position":8},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Instantiate the Firefly client"},"content":"There are two ways to initialize a Firefly client from Python, depending on whether you’re running the notebook in JupyterLab or not. Assuming you have jupyter-firefly-extensions set up in your environment as explained \n\nhere, you can use make_lab_client() in JupyterLab, which will open the Firefly viewer in a new tab within the Lab. Otherwise, you can use make_client() in a Jupyter Notebook (or even a Python shell), which will open the Firefly viewer in a new web browser tab.\n\nYou also need a Firefly server to communicate with your Firefly Python client. In this notebook, we use a public Firefly server: the IRSA Viewer (\n\nhttps://​irsa​.ipac​.caltech​.edu​/irsaviewer). However, you can also run a local Firefly server via a \n\nFirefly Docker image and access it at http://localhost:8080/firefly. The URL of the Firefly server is read by both make_client() and make_lab_client() through the environment variable FIREFLY_URL. However, make_client() also allows you to pass the URL directly as the url parameter.\n\n# Uncomment when using within Jupyter Lab with jupyter_firefly_extensions installed\n# fc = FireflyClient.make_lab_client()\n\n# Uncomment for contexts other than above \nfc = FireflyClient.make_client(url=\"https://irsa.ipac.caltech.edu/irsaviewer\")\n\nfc.reinit_viewer() # to clean the state, if this cell ran earlier\n\n\n\n\n\n","type":"content","url":"/neowise-light-curve-demo#instantiate-the-firefly-client","position":9},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Construct a TAP Query and display the retrieved table in Firefly"},"type":"lvl2","url":"/neowise-light-curve-demo#construct-a-tap-query-and-display-the-retrieved-table-in-firefly","position":10},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Construct a TAP Query and display the retrieved table in Firefly"},"content":"TAP search the ‘Known Solar System Object Possible Association List’ catalog from the NEOWISE-R database. The specific target we are looking for is minor planet 558 Carmen. We can query this target using a TAP search through IRSA; the table_url is broken down as follows:\n\nWe want to search the data through IRSA, which supports TAP querying, and we want it streamed directly to us via a synchronous search: “https://\n\nirsa​.ipac​.caltech​.edu​/TAP​/sync”\n\nNext, we want to structure query to only retrieve (558) Carmen data from the NEOWISE-R ‘Known Solar System Object Possible Association List’ catalog. The table name of the catalog can be found using \n\nIRSAViewer and clicking the VO TAP Search tab and changing the ‘Project’ to neowiser. We query all columns of data and we search the target by its object id, which is its name, and use the ‘like’ condition to only write (558) with a wildcard as shown in the cell below.\n\nConstruction of the query can be found in the \n\nIRSA TAP documentation page.\n\nBASE_URL = \"https://irsa.ipac.caltech.edu/TAP/sync\"\nQUERY = \"\"\"\nSELECT *\nFROM neowiser_p1ba_mch AS n\nWHERE n.objid LIKE '(558)%'\n\"\"\"\ntable_url = f\"{BASE_URL}?QUERY={urllib.parse.quote_plus(QUERY)}\"\ntable_url\n\n\n\nNow, we can request the necessary data from the catalog and display the data as a table in the Firefly client, using the \n\nshow_table method.\n\nAlternatively, we can download data from the catalog using \n\nastropy.utils.data.download_file and upload it to the Firefly client shown in the cell below the first method.\n\nfc.show_table(table_url, tbl_id='tableneo', title='558 Carmen NeoWise Catalog', page_size=50)\n\n\n\n\n\n# tablename = astropy.utils.data.download_file(table_url, timeout=120, cache=True)\n# file = fc.upload_file(tablename)\n# fc.show_table(file, tbl_id='tableneo', title='558 Carmen Catalog', page_size=50)\n\n\n\nNote that along with the table, firefly also displays the coverage and chart associated with the table. It overlays colored squares for each row of the table onto a HiPS image, because the table contains recognizable celestial coordinates. It also creates a scatter plot of ra and dec from the table.\n\n","type":"content","url":"/neowise-light-curve-demo#construct-a-tap-query-and-display-the-retrieved-table-in-firefly","position":11},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Plot Light Curves in Firefly"},"type":"lvl2","url":"/neowise-light-curve-demo#plot-light-curves-in-firefly","position":12},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Plot Light Curves in Firefly"},"content":"After retrieving the data and displaying it in the client, we can now create a light curve by plotting the Modified Julian Date (‘mjd’) in the abscissa and the magnitude from band W1 (‘w1mpro’) in the ordinate. We also flip the ordinate to accurately display magnitude.\n\nfc.show_xyplot(tbl_id='tableneo', xCol='mjd', yCol='w1mpro', yOptions='flip')\n\n\n\n","type":"content","url":"/neowise-light-curve-demo#plot-light-curves-in-firefly","position":13},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Overlay the catalog on a HiPS image"},"type":"lvl2","url":"/neowise-light-curve-demo#overlay-the-catalog-on-a-hips-image","position":14},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Overlay the catalog on a HiPS image"},"content":"Finally, we can overlay the catalog of data in the table onto a HiPS image of our choice, using the method \n\nshow_hips. However, this method requires target coordinates for the object you want to analyze.\n\ntarget='229.851396;-9.720647;EQ_J2000'\nviewer_id = 'hipsDiv'\nhips_url = 'http://alasky.u-strasbg.fr/AllWISE/RGB-W4-W2-W1'\n\nfc.show_hips(viewer_id=viewer_id, plot_id='aHipsID1-1', hips_root_url = hips_url,\n                          Title='HiPS-WISE', WorldPt=target)\n\n\n\n","type":"content","url":"/neowise-light-curve-demo#overlay-the-catalog-on-a-hips-image","position":15},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Summary"},"type":"lvl2","url":"/neowise-light-curve-demo#summary","position":16},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"Summary"},"content":"Firefly allows you to visualize data for specific targets. In conjuction with Astropy, one can manipulate a catalog of observations to display a light curve in an instantiated Firefly enviroment on their web browser.\n\nWe import all necessary modules to create a Firefly client and to download the catalog of data for our target.\n\nWe start the client in our web browser to appropiately display our tables, plots and images.\n\nWe use the TAP schema to display the data for our target — \n\n558 Carmen — via a table and visualize such data through charts.\n\nWe finally overlay the catalog onto a HiPS image to dynamically view where our target has been observed in space.\n\n","type":"content","url":"/neowise-light-curve-demo#summary","position":17},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"About This Notebook"},"type":"lvl2","url":"/neowise-light-curve-demo#about-this-notebook","position":18},{"hierarchy":{"lvl1":"Using Firefly visualization tools to understand the light curves of Solar System objects","lvl2":"About This Notebook"},"content":"\n\nAuthor: Eric Bratton II (IRSA Scientist) in conjunction with the IRSA Science Team\n\nUpdated: 2024-12-19\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/neowise-light-curve-demo#about-this-notebook","position":19},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table"},"type":"lvl1","url":"/neowise-source-table-lightcurves","position":0},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table"},"content":"An executed version of this notebook can be seen on\n\n\nIRSA’s website.\n\nLearning Goals:\n\nSearch the NEOWISE Single-exposure Source Table (Parquet version) for the light curves of a\nset of targets with RA/Dec coordinates.\n\nWrite a pyarrow dataset filter and use it to load the NEOWISE detections near each target (rough cut).\n\nMatch targets to detections using an astropy cone search (precise cut).\n\nParallelize this.\n\nPlot the light curves.\n\n","type":"content","url":"/neowise-source-table-lightcurves","position":1},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"1. Introduction"},"type":"lvl2","url":"/neowise-source-table-lightcurves#id-1-introduction","position":2},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"1. Introduction"},"content":"This notebook loads light curves from the\n\n\nNEOWISE Single-exposure Source Table\nfor a sample of about 2000 cataclysmic variables from \n\nDownes et al. (2001).\nThe NEOWISE Single-exposure Source Table is a very large catalog -- 11 years and 42 terabytes in total\nwith 145 columns and 200 billion rows.\nWhen searching this catalog, it is important to consider the requirements of your use case and\nthe format of this dataset.\nThis notebook applies the techniques developed in\n\n\nStrategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet.\nThis is a fully-worked example that demonstrates the important steps, but note that this is a\nrelatively small use case for the Parquet version of the dataset.\n\nThe specific strategy we employ is:\n\nChoose a cone search radius that determines which NEOWISE source detections to associate\nwith each target.\n\nLoad the sample of CV targets.\n\nCalculate the indexes of all HEALPix order k=5 pixels within the radius of each target.\nThese are the dataset partitions that need to be searched.\n\nParallelize over the partitions using multiprocessing.Pool.\nFor each pixel:\n\nConstruct a dataset filter for NEOWISE sources in the vicinity of the targets in the partition.\n\nLoad data, applying the filter. In our case, the number of rows loaded will be fairly small.\n\nDo a cone search to match sources with targets in the partition.\n\nReturn the results.\n\nConcatenate the cone search results, groupby target ID, and sort by time to construct the light curves.\n\nThe efficiency of this method will increase with the number of rows needed from each partition.\nFor example, a cone search radius of 1 arcsec will require about 10 CPUs, 65G RAM, and\n50 minutes to load the data from all 11 NEOWISE years.\nIncreasing the radius to 10 arcsec will return about 2.5x more rows using roughly the same resources.\nIncreasing the target sample size can result in similar efficiency gains.\nTo try out this notebook with fewer resources, use a subset of NEOWISE years.\nUsing one year is expected to require about 5 CPUs, 20G RAM, and 10 minutes.\nThese estimates are based on testing in science platform environments.\nYour numbers will vary based on many factors including compute power, bandwidth, and physical distance from the data.\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-1-introduction","position":3},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"2. Imports"},"type":"lvl2","url":"/neowise-source-table-lightcurves#id-2-imports","position":4},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"2. Imports"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install astropy astroquery hpgeom matplotlib pandas pyarrow pyvo\n\n\n\n\n\nimport multiprocessing  # parallelization\n\nimport astroquery.vizier  # fetch the sample of CV targets\nimport hpgeom  # HEALPix math\nimport numpy as np  # math\nimport pandas as pd  # manipulate tabular data\nimport pyarrow.compute  # construct dataset filters\nimport pyarrow.dataset  # load and query the NEOWISE dataset\nimport pyarrow.fs  # interact with the S3 bucket storing the NEOWISE catalog\nimport pyvo  # TAP service for the Vizier query\nfrom astropy import units as u  # manipulate astropy quantities\nfrom astropy.coordinates import SkyCoord  # manipulate sky coordinates\nfrom matplotlib import pyplot as plt  # plot light curves\n\n# copy-on-write will become the default in pandas 3.0 and is generally more performant\npd.options.mode.copy_on_write = True\n\n\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-2-imports","position":5},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"3. Setup"},"type":"lvl2","url":"/neowise-source-table-lightcurves#id-3-setup","position":6},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"3. Setup"},"content":"\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-3-setup","position":7},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl3":"3.1 Define variables","lvl2":"3. Setup"},"type":"lvl3","url":"/neowise-source-table-lightcurves#id-3-1-define-variables","position":8},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl3":"3.1 Define variables","lvl2":"3. Setup"},"content":"First, choose which NEOWISE years to include.\nReal use cases are likely to require all ten years but it can be helpful to start with\nfewer while exploring to make things run faster.\n\n# all years => about 11 CPU, 65G RAM, and 50 minutes runtime\nYEARS = [f\"year{yr}\" for yr in range(1, 12)] + [\"addendum\"]\n\n# To try out a smaller version of the notebook,\n# uncomment the next line and choose your own subset of years.\n# YEARS = [10]  # one year => about 5 CPU, 20G RAM, and 10 minutes runtime\n\n\n\n\n\n# sets of columns that we'll need\nFLUX_COLUMNS = [\"w1flux\", \"w2flux\"]\nLIGHTCURVE_COLUMNS = [\"mjd\"] + FLUX_COLUMNS\nCOLUMN_SUBSET = [\"cntr\", \"ra\", \"dec\"] + LIGHTCURVE_COLUMNS\n\n# cone-search radius defining which NEOWISE sources are associated with each target object\nMATCH_RADIUS = 1 * u.arcsec\n\n\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-3-1-define-variables","position":9},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl3":"3.2 Load NEOWISE metadata","lvl2":"3. Setup"},"type":"lvl3","url":"/neowise-source-table-lightcurves#id-3-2-load-neowise-metadata","position":10},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl3":"3.2 Load NEOWISE metadata","lvl2":"3. Setup"},"content":"\n\nThe metadata contains column names, schema, and row-group statistics for every file in the dataset.\nWe’ll load it as a pyarrow dataset.\n\n# This catalog is so big that even the metadata is big.\n# Expect this cell to take about 30 seconds per year.\n\n# This information can be found at https://irsa.ipac.caltech.edu/cloud_access/.\nbucket = \"nasa-irsa-wise\"\nbase_prefix = \"wise/neowiser/catalogs/p1bs_psd/healpix_k5\"\nmetadata_path = (\n    lambda yr: f\"{bucket}/{base_prefix}/{yr}/neowiser-healpix_k5-{yr}.parquet/_metadata\"\n)\nfs = pyarrow.fs.S3FileSystem(region=\"us-west-2\", anonymous=True)\n\n# list of datasets, one per year\nyear_datasets = [\n    pyarrow.dataset.parquet_dataset(metadata_path(yr), filesystem=fs, partitioning=\"hive\")\n    for yr in YEARS\n]\n\n# unified dataset, all years\nneowise_ds = pyarrow.dataset.dataset(year_datasets)\n\n\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-3-2-load-neowise-metadata","position":11},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"4. Define functions to filter and load data"},"type":"lvl2","url":"/neowise-source-table-lightcurves#id-4-define-functions-to-filter-and-load-data","position":12},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"4. Define functions to filter and load data"},"content":"\n\nThese functions will be used in the next section.\nDefining them here in the notebook is useful for demonstration and should work seamlessly on Linux, which includes most science platforms.\nMac and Windows users should see the note at the end of the notebook.\nHowever, note that this use case is likely too large for a laptop and may perform poorly and/or crash if attempted.\n\n# If you have your own list of target objects, replace this function to load your sample.\ndef load_targets_Downes2001(radius=1 * u.arcsec):\n    \"\"\"Load a sample of targets and return a pandas DataFrame.\n\n    References:\n    - Downes et al., 2001 ([2001PASP..113..764D](https://ui.adsabs.harvard.edu/abs/2001PASP..113..764D/abstract)).\n    - https://cdsarc.cds.unistra.fr/ftp/V/123A/ReadMe\n\n    Parameters\n    ----------\n    radius : astropy.Quantity (optional)\n        Radius for the cone search around each target. This is used to determine which partition(s)\n        need to be searched for a given target. Use the same radius here as in the rest of the notebook.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The loaded targets with the following columns:\n            - uid: Unique identifier of the target.\n            - GCVS: Name in the General Catalogue of Variable Stars if it exists, else the constellation name.\n            - RAJ2000: Right Ascension of the target in J2000 coordinates.\n            - DEJ2000: Declination of the target in J2000 coordinates.\n            - healpix_k5: HEALPix pixel index at order k=5.\n    \"\"\"\n    astroquery.vizier.Vizier.ROW_LIMIT = -1\n    # https://cdsarc.cds.unistra.fr/vizier/notebook.gml?source=V/123A\n    # https://cdsarc.cds.unistra.fr/ftp/V/123A/ReadMe\n    CATALOGUE = \"V/123A\"\n    voresource = pyvo.registry.search(ivoid=f\"ivo://CDS.VizieR/{CATALOGUE}\")[0]\n    tap_service = voresource.get_service(\"tap\")\n\n    # Query Vizier and load targets to a dataframe.\n    cv_columns = [\"uid\", \"GCVS\", \"RAJ2000\", \"DEJ2000\"]\n    cvs_records = tap_service.run_sync(\n        f'SELECT {\",\".join(cv_columns)} from \"{CATALOGUE}/cv\"'\n    )\n    cvs_df = cvs_records.to_table().to_pandas()\n\n    # Add a new column containing a list of all order k HEALPix pixels that overlap with\n    # the CV's position plus search radius.\n    cvs_df[\"healpix_k5\"] = [\n        hpgeom.query_circle(\n            a=cv.RAJ2000,\n            b=cv.DEJ2000,\n            radius=radius.to_value(\"deg\"),\n            nside=hpgeom.order_to_nside(order=5),\n            nest=True,\n            inclusive=True,\n        )\n        for cv in cvs_df.itertuples()\n    ]\n    # Explode the lists of pixels so the dataframe has one row per target per pixel.\n    # Targets near a pixel boundary will now have more than one row.\n    # Later, we'll search each pixel separately for NEOWISE detections and then\n    # concatenate the matches for each target to produce complete light curves.\n    cvs_df = cvs_df.explode(\"healpix_k5\", ignore_index=True)\n\n    return cvs_df\n\n\n\n\n\n# This is the main function.\ndef load_lightcurves_one_partition(targets_group):\n    \"\"\"Load lightcurves from a single partition.\n\n    Parameters\n    ----------\n    targets_group : tuple\n        Tuple of pixel index and sub-DataFrame (result of DataFrame groupby operation).\n\n    Returns\n    -------\n    pd.DataFrame\n        The lightcurves for targets found in this partition.\n    \"\"\"\n    # These global variables will be set when the worker is initialized.\n    global _neowise_ds\n    global _columns\n    global _radius\n\n    # Get row filters that will limit the amount of data loaded from this partition.\n    # It is important for these filters to be efficient for the specific use case.\n    filters = _construct_dataset_filters(targets_group=targets_group, radius=_radius)\n\n    # Load this slice of the dataset to a pyarrow Table.\n    pixel_tbl = _neowise_ds.to_table(columns=_columns, filter=filters)\n\n    # Associate NEOWISE detections with targets to get the light curves.\n    lightcurves_df = _cone_search(\n        targets_group=targets_group, pixel_tbl=pixel_tbl, radius=_radius\n    )\n\n    return lightcurves_df\n\n\n\n\n\n# The filters returned by this function need to be efficient for the specific use case.\ndef _construct_dataset_filters(*, targets_group, radius, scale_factor=100):\n    \"\"\"Construct dataset filters for a box search around all targets in the partition.\n\n    Parameters\n    ----------\n    targets_group : tuple\n        Tuple of pixel index and sub-DataFrame (result of DataFrame groupby operation).\n    radius : astropy.Quantity\n        The radius used for constructing the RA and Dec filters.\n    scale_factor : int (optional)\n        Factor by which the radius will be multiplied to ensure that the box encloses\n        all relevant detections.\n\n    Returns\n    -------\n    filters : pyarrow.compute.Expression\n        The constructed filters based on the given inputs.\n    \"\"\"\n    pixel, locations_df = targets_group\n\n    # Start with a filter for the partition. This is the most important one because\n    # it tells the Parquet reader to just skip all the other partitions.\n    filters = pyarrow.compute.field(\"healpix_k5\") == pixel\n\n    # Add box search filters. For our CV sample, one box encompassing all targets in\n    # the partition should be sufficient. Make a different choice if you use a different\n    # sample and find that this loads more data than you want to handle at once.\n    buffer_dist = scale_factor * radius.to_value(\"deg\")\n    for coord, target_coord in zip([\"ra\", \"dec\"], [\"RAJ2000\", \"DEJ2000\"]):\n        coord_fld = pyarrow.compute.field(coord)\n\n        # Add a filter for coordinate lower limit.\n        coord_min = locations_df[target_coord].min()\n        filters = filters & (coord_fld > coord_min - buffer_dist)\n\n        # Add a filter for coordinate upper limit.\n        coord_max = locations_df[target_coord].max()\n        filters = filters & (coord_fld < coord_max + buffer_dist)\n\n    # Add your own additional requirements here, like magnitude limits or quality cuts.\n    # See the AllWISE notebook for more filter examples and links to pyarrow documentation.\n    # We'll add a filter for sources not affected by contamination or confusion.\n    filters = filters & pyarrow.compute.equal(pyarrow.compute.field(\"cc_flags\"), \"0000\")\n\n    return filters\n\n\n\n\n\ndef _cone_search(*, targets_group, pixel_tbl, radius):\n    \"\"\"Perform a cone search to select NEOWISE detections belonging to each target object.\n\n    Parameters\n    ----------\n    targets_group : tuple\n        Tuple of pixel index and sub-DataFrame (result of DataFrame groupby operation).\n    pixel_tbl : pyarrow.Table\n        Table of NEOWISE data for a single pixel\n    radius : astropy.Quantity\n        Cone search radius.\n\n    Returns\n    -------\n    match_df : pd.DataFrame\n        A dataframe with all matched sources.\n    \"\"\"\n    _, targets_df = targets_group\n\n    # Cone search using astropy to select NEOWISE detections belonging to each object.\n    pixel_skycoords = SkyCoord(ra=pixel_tbl[\"ra\"] * u.deg, dec=pixel_tbl[\"dec\"] * u.deg)\n    targets_skycoords = SkyCoord(targets_df[\"RAJ2000\"], targets_df[\"DEJ2000\"], unit=u.deg)\n    targets_ilocs, pixel_ilocs, _, _ = pixel_skycoords.search_around_sky(\n        targets_skycoords, radius\n    )\n\n    # Create a dataframe with all matched source detections.\n    match_df = pixel_tbl.take(pixel_ilocs).to_pandas()\n\n    # Add the target IDs by joining with targets_df.\n    match_df[\"targets_ilocs\"] = targets_ilocs\n    match_df = match_df.set_index(\"targets_ilocs\").join(targets_df.reset_index().uid)\n\n    return match_df\n\n\n\n\n\n# This function will be called once for each worker in the pool.\ndef init_worker(neowise_ds, columns, radius):\n    \"\"\"Set global variables '_neowise_ds', '_columns', and '_radius'.\n\n    These variables will be the same for every call to 'load_lightcurves_one_partition'\n    and will be set once for each worker. It is important to pass 'neowise_ds' this\n    way because of its size and the way it will be used. (For the other two, it makes\n    little difference whether we use this method or pass them directly as function\n    arguments to 'load_lightcurves_one_partition'.)\n\n    Parameters\n    ----------\n    neowise_ds : pyarrow.dataset.Dataset\n        NEOWISE metadata loaded as a PyArrow dataset.\n    columns : list\n        Columns to include in the output DataFrame of light curves.\n    radius : astropy.Quantity\n        Cone search radius.\n    \"\"\"\n    global _neowise_ds\n    _neowise_ds = neowise_ds\n    global _columns\n    _columns = columns\n    global _radius\n    _radius = radius\n\n\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-4-define-functions-to-filter-and-load-data","position":13},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"5. Load light curves"},"type":"lvl2","url":"/neowise-source-table-lightcurves#id-5-load-light-curves","position":14},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"5. Load light curves"},"content":"\n\nLoad the target objects’ coordinates and other info.\n\ntargets_df = load_targets_Downes2001(radius=MATCH_RADIUS)\ntargets_df.head()\n\n\n\nSearch the NEOWISE Source Table for all targets (positional matches) and load the light curves.\nPartitions are searched in parallel.\nFor targets located near partition boundaries, relevant partitions will be searched\nindependently for the given target and the results will be concatenated.\nIf searching all NEOWISE years, this may take 45 minutes or more.\n\n# Group targets by partition. 'load_lightcurves_one_partition' will be called once per group.\ntargets_groups = targets_df.groupby(\"healpix_k5\")\n# Arguments for 'init_worker'.\ninit_args = (neowise_ds, COLUMN_SUBSET, MATCH_RADIUS)\n\n# Start a multiprocessing pool and load the target light curves in parallel.\n# About 1900 unique pixels in targets_df, 8 workers, 48 chunksize => ~5 chunks per worker.\nnworkers = 8\nchunksize = 48\nwith multiprocessing.Pool(nworkers, initializer=init_worker, initargs=init_args) as pool:\n    lightcurves = []\n    for lightcurves_df in pool.imap_unordered(\n        load_lightcurves_one_partition, targets_groups, chunksize=chunksize\n    ):\n        lightcurves.append(lightcurves_df)\nneowise_lightcurves_df = pd.concat(lightcurves).sort_values(\"mjd\").reset_index(drop=True)\n\n\n\n\n\nneowise_lightcurves_df.head()\n\n\n\n","type":"content","url":"/neowise-source-table-lightcurves#id-5-load-light-curves","position":15},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"6. Plot NEOWISE light curves"},"type":"lvl2","url":"/neowise-source-table-lightcurves#id-6-plot-neowise-light-curves","position":16},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"6. Plot NEOWISE light curves"},"content":"The light curves will have large gaps due to the observing cadence, so we’ll plot each\n“epoch” separately to see them better.\n\n# get the light curves of the target with the most data\ntarget_uid = neowise_lightcurves_df.groupby(\"uid\").mjd.count().sort_values().index[-1]\ntarget_df = neowise_lightcurves_df.loc[neowise_lightcurves_df.uid == target_uid]\n\n# list of indexes that separate epochs (arbitrarily at delta mjd > 30)\nepoch_idxs = target_df.loc[target_df.mjd.diff() > 30].index.to_list()\nepoch_idxs = epoch_idxs + [target_df.index[-1]]  # add the final index\n\n# make the figure\nncols = 4\nnrows = int(np.ceil(len(epoch_idxs) / ncols))\nfig, axs = plt.subplots(nrows, ncols, sharey=True, figsize=(3 * ncols, 2.5 * nrows))\naxs = axs.flatten()\nidx0 = target_df.index[0]\nfor i, (idx1, ax) in enumerate(zip(epoch_idxs, axs)):\n    epoch_df = target_df.loc[idx0 : idx1 - 1, LIGHTCURVE_COLUMNS].set_index(\"mjd\")\n    for col in FLUX_COLUMNS:\n        ax.plot(epoch_df[col], \".\", markersize=3, label=col)\n    ax.set_title(f\"epoch {i}\")\n    ax.xaxis.set_ticks(  # space by 10\n        range(int((ax.get_xlim()[0] + 10) / 10) * 10, int(ax.get_xlim()[1]), 10)\n    )\n    idx0 = idx1\naxs[0].legend()\nfig.supxlabel(\"MJD\")\nfig.supylabel(\"RAW FLUX\")\nfig.suptitle(f\"NEOWISE light curves for target CV {target_uid}\")\nfig.tight_layout()\nplt.show(block=False)\n\n\n\n\n\n[*] Note to Mac and Windows users:\n\nYou will need to copy the functions and imports from this notebook into a separate ‘.py’ file and then import them in order to use the multiprocessing pool for parallelization.\nIn addition, you may need to load neowise_ds separately for each child process (i.e., worker) by adding that code to the init_worker function instead of passing it in as an argument.\nThis has to do with differences in what does / does not get copied into the child processes on different platforms.","type":"content","url":"/neowise-source-table-lightcurves#id-6-plot-neowise-light-curves","position":17},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"About this notebook"},"type":"lvl2","url":"/neowise-source-table-lightcurves#about-this-notebook","position":18},{"hierarchy":{"lvl1":"Make Light Curves from NEOWISE Single-exposure Source Table","lvl2":"About this notebook"},"content":"Author: Troy Raen (IRSA Developer) and the IPAC Science Platform team\n\nUpdated: 2025-03-07\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/neowise-source-table-lightcurves#about-this-notebook","position":19},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet"},"type":"lvl1","url":"/neowise-source-table-strategies","position":0},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet"},"content":"An executed version of this notebook can be seen on\n\n\nIRSA’s website.\n\nThis notebook discusses strategies for working with the Apache Parquet version of the\n\n\nNEOWISE Single-exposure Source Table\nand provides the basic code needed for each approach.\nThis is a very large catalog -- 11 years and 42 terabytes in total with 145 columns and 200 billion rows.\nMost of the work shown in this notebook is how to efficiently deal with so much data.\n\nLearning Goals:\n\nIdentify use cases that will benefit most from using this Parquet format.\n\nUnderstand how this dataset is organized and three different ways to efficiently\nslice it in order to obtain a subset small enough to load into memory and process.\n\nFeel prepared to apply these methods to a new use case and determine efficient filtering and slicing options.\n\n","type":"content","url":"/neowise-source-table-strategies","position":1},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"1. Introduction"},"type":"lvl2","url":"/neowise-source-table-strategies#id-1-introduction","position":2},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"1. Introduction"},"content":"\n\nThe NEOWISE Single-exposure Source Table comprises 11 years of data.\nEach year on its own would be considered “large” compared to astronomy catalogs produced\ncontemporaneously, so working with the full dataset requires extra consideration.\nIn this Parquet version, each year is stored as an independent Parquet dataset.\nThis tutorial shows how to combine them and work with all years as one.\nThe data are partitioned by HEALPix (\n\nGórski et al., 2005) order k=5.\nHEALPix is a tessellation of the sky, so partitioning the dataset this way makes it especially\nefficient for spatial queries.\nIn addition, the access methods shown below are expected to perform well when parallelized.\n\nThe terms “partition”, “filter”, and “slice” all relate to subsets of data.\nIn this notebook we’ll use them with definitions that overlap but are not identical, as follows.\nA “partition” includes all data observed in a single HEALPix pixel.\nThis data is grouped together in the Parquet files.\nA “filter” is a set of criteria defined by the user and applied when reading data so that only the desired rows are loaded.\nWe’ll use it exclusively to refer to a PyArrow filter.\nThe criteria can include partitions and/or any other column in the dataset.\nA “slice” is a generic subset of data.\nThere are a few ways to obtain a slice; one is by applying a filter.\n\nThis notebook is expected to require about 2 CPUs and 50G RAM and to complete in about 10 minutes.\nThese estimates are based on testing in science platform environments.\nYour numbers will vary based on many factors including compute power, bandwidth, and physical distance from the data.\nThe required RAM and runtime can be reduced by limiting the number of NEOWISE years loaded.\n\n","type":"content","url":"/neowise-source-table-strategies#id-1-introduction","position":3},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"1.1 When to use the Parquet version","lvl2":"1. Introduction"},"type":"lvl3","url":"/neowise-source-table-strategies#id-1-1-when-to-use-the-parquet-version","position":4},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"1.1 When to use the Parquet version","lvl2":"1. Introduction"},"content":"IRSA provides large catalogs in file formats (e.g., Parquet) primarily to support use cases that require bulk access.\nThe Parquet version of the NEOWISE Source Table, coupled with the methods demonstrated in this tutorial,\nare expected to be the most efficient option for large-ish use cases like classifying, clustering, or\nbuilding light curves for many objects.\nIn general, the efficiency will increase with the number of rows required from each slice because the slice\nonly needs to be searched once regardless of the number of rows that are actually loaded.\nIn addition, this access route tends to perform well when parallelized.\nNote that these use cases (including this notebook) are often too large for a laptop and may perform\npoorly and/or crash if attempted.\n\nFor small-ish use cases like searching for a handful of objects, other access routes like\nPyVO and TAP queries will be faster.\n\nConsider using this tutorial if either of the following is true:\n\nYour use case is large enough that you are considering parallelizing your code to speed it up.\n\nYour sample size is large enough that loading the data using a different method is likely to\ntake hours, days, or longer.\n\n","type":"content","url":"/neowise-source-table-strategies#id-1-1-when-to-use-the-parquet-version","position":5},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"1.2 Recommended approach","lvl2":"1. Introduction"},"type":"lvl3","url":"/neowise-source-table-strategies#id-1-2-recommended-approach","position":6},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"1.2 Recommended approach","lvl2":"1. Introduction"},"content":"The basic process is:\n\nLoad the catalog metadata as a PyArrow dataset.\n\nDecide how to slice the dataset (e.g., by year, partition, and/or file) depending on your use case.\n\nIterate and/or parallelize over the slices. For each slice:\n\nUse the PyArrow dataset to load data of interest, applying row filters during the read.\n\nProcess the data as you like (e.g., cluster, classify, etc.).\n\nWrite your results to disk and/or return them for further processing.\n\n(Optional) Concatenate your results and continue processing.\n\nThis notebook covers steps 1 through 3.1 and indicates where to insert your own code to proceed with 3.2.\nHere we iterate over slices, but the same code can be parallelized using any multi-processing framework.\nA fully-worked example is shown in the light curve notebook linked below.\n\n","type":"content","url":"/neowise-source-table-strategies#id-1-2-recommended-approach","position":7},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"1.3 See also","lvl2":"1. Introduction"},"type":"lvl3","url":"/neowise-source-table-strategies#id-1-3-see-also","position":8},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"1.3 See also","lvl2":"1. Introduction"},"content":"IRSA cloud access introduction\n\nAnalyzing cloud-hosted AllWISE Source Catalog in Parquet format\n\nMake Light Curves from NEOWISE Single-exposure Source Table\n\n","type":"content","url":"/neowise-source-table-strategies#id-1-3-see-also","position":9},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"2. Imports"},"type":"lvl2","url":"/neowise-source-table-strategies#id-2-imports","position":10},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"2. Imports"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install hpgeom pandas pyarrow\n\n\n\n\n\nimport re  # parse strings\nimport sys  # check size of loaded data\n\nimport hpgeom  # HEALPix math\nimport pandas as pd  # store and manipulate table data\nimport pyarrow.compute  # construct dataset filters\nimport pyarrow.dataset  # load and query the NEOWISE dataset\nimport pyarrow.fs  # interact with the S3 bucket storing the NEOWISE catalog\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#id-2-imports","position":11},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"3. Setup"},"type":"lvl2","url":"/neowise-source-table-strategies#id-3-setup","position":12},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"3. Setup"},"content":"","type":"content","url":"/neowise-source-table-strategies#id-3-setup","position":13},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"3.1 Define variables and helper functions","lvl2":"3. Setup"},"type":"lvl3","url":"/neowise-source-table-strategies#id-3-1-define-variables-and-helper-functions","position":14},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"3.1 Define variables and helper functions","lvl2":"3. Setup"},"content":"\n\nChoose which NEOWISE years to include.\nExpect the notebook to require about 4G RAM and 1 minute of runtime per year.\n\n# All NEOWISE years => about 40G RAM and 10 minutes runtime\nYEARS = [f\"year{yr}\" for yr in range(1, 12)] + [\"addendum\"]\n\n# To reduce the needed RAM or runtime, uncomment the next line and choose your own years.\n# Years 1 and 8 are needed for the median_file and biggest_file (defined below).\n# YEARS = [1, 8]\n\n\n\nColumn and partition variables:\n\n# subset of columns to load\nflux_columns = [\"w1flux\", \"w1sigflux\", \"w2flux\", \"w2sigflux\"]\nCOLUMN_SUBSET = [\"cntr\", \"source_id\", \"ra\", \"dec\"] + flux_columns\n\n# partitioning info. do not change these values.\nK = 5  # healpix order of the catalog partitioning\nKCOLUMN = \"healpix_k5\"  # partitioning column name\nKFIELD = pyarrow.compute.field(KCOLUMN)  # pyarrow compute field, to be used in filters\n\n\n\nPaths:\n\n# We're going to look at several different files, so make a function to return the path.\ndef neowise_path(year, file=\"_metadata\"):\n    \"\"\"Return the path to a file. Default is \"_metadata\" file of the given year's dataset.\n\n    Parameters\n    ----------\n    year : int\n        NEOWISE year for which the path is being generated.\n    file : str\n        The name of the file to the returned path.\n\n    Returns\n    -------\n    str\n        The path to the file.\n    \"\"\"\n    # This information can be found at https://irsa.ipac.caltech.edu/cloud_access/.\n    bucket = \"nasa-irsa-wise\"\n    base_prefix = \"wise/neowiser/catalogs/p1bs_psd/healpix_k5\"\n    root_dir = f\"{bucket}/{base_prefix}/{year}/neowiser-healpix_k5-{year}.parquet\"\n    return f\"{root_dir}/{file}\"\n\n\n\nSome representative partitions and files (see dataset stats in the Appendix for how we determine these values):\n\n# pixel index of the median partition and the biggest partition by number of rows\nmedian_part = 11_831\nbiggest_part = 8_277\n\n# path to the median file and the biggest file by file size on disk (see Appendix)\nmedian_file = neowise_path(\"year8\", \"healpix_k0=1/healpix_k5=1986/part0.snappy.parquet\")\nbiggest_file = neowise_path(\"year1\", \"healpix_k0=2/healpix_k5=2551/part0.snappy.parquet\")\n\n\n\nConvenience function for displaying a table size:\n\n# We'll use this function throughout the notebook to see how big different tables are.\ndef print_table_size(table, pixel_index=None):\n    \"\"\"Prints the shape (rows x columns) and size (GiB) of the given table.\n\n    Parameters\n    ----------\n    table : pyarrow.Table\n        The table for which to print the size.\n    pixel_index : int or str or None\n        The pixel index corresponding to the partition this table was loaded from.\n    \"\"\"\n    if pixel_index is not None:\n        print(f\"pixel index: {pixel_index}\")\n    print(f\"table shape: {table.num_rows:,} rows x {table.num_columns} columns\")\n    print(f\"table size: {sys.getsizeof(table) / 1024**3:.2f} GiB\")\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#id-3-1-define-variables-and-helper-functions","position":15},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"3.2 Load NEOWISE metadata as a pyarrow dataset","lvl2":"3. Setup"},"type":"lvl3","url":"/neowise-source-table-strategies#id-3-2-load-neowise-metadata-as-a-pyarrow-dataset","position":16},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"3.2 Load NEOWISE metadata as a pyarrow dataset","lvl2":"3. Setup"},"content":"\n\nThe metadata contains column names, schema, and row-group statistics for every file in the dataset.\nLater, we will use this pyarrow dataset object to slice and query the catalog in several different ways.\n\n# This catalog is so big that even the metadata is big.\n# Expect this cell to take about 30 seconds per year.\nfs = pyarrow.fs.S3FileSystem(region=\"us-west-2\", anonymous=True)\n\n# list of datasets, one per year\nyear_datasets = [\n    pyarrow.dataset.parquet_dataset(neowise_path(yr), filesystem=fs, partitioning=\"hive\")\n    for yr in YEARS\n]\n\n# unified dataset, all years\nneowise_ds = pyarrow.dataset.dataset(year_datasets)\n\n\n\nneowise_ds is a \n\nUnionDataset.\nAll methods demonstrated for pyarrow datasets in the AllWISE demo notebook can be used with\nneowise_ds and will be applied to all years as if they were a single dataset.\nIn addition, a separate \n\nDataset\nfor each year is stored in the list attribute neowise_ds.children (== year_datasets, loaded above),\nand the same methods can be applied to them individually.\n\n","type":"content","url":"/neowise-source-table-strategies#id-3-2-load-neowise-metadata-as-a-pyarrow-dataset","position":17},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"4. Example: Slice by partition"},"type":"lvl2","url":"/neowise-source-table-strategies#id-4-example-slice-by-partition","position":18},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"4. Example: Slice by partition"},"content":"\n\nThis example shows how to load data from each partition separately.\nThe actual “slicing” is done by applying a filter to the pyarrow dataset neowise_ds.\nConstructing filters was discussed in the AllWISE notebook linked above.\n\n# number of order K pixels covering the full sky\nnpixels = hpgeom.nside_to_npixel(hpgeom.order_to_nside(order=K))\n\n# iterate over all partitions\nfor pix in range(npixels):\n\n    # slice and load to get all rows in this partition, subset of columns\n    pixel_tbl = neowise_ds.to_table(filter=(KFIELD == pix), columns=COLUMN_SUBSET)\n\n    # insert your code here to continue processing\n\n    # we'll just print the table size to get a sense of how much data has been loaded\n    print_table_size(table=pixel_tbl, pixel_index=pix)\n\n    # when done, you may want to delete pixel_tbl to free the memory\n    del pixel_tbl\n    # we'll stop after one partition\n    break\n\n\n\npixel_tbl is a (pyarrow) \n\nTable\ncontaining all NEOWISE sources with an ra/dec falling within HEALPix order 5 pixel pix.\nUse pixel_tbl.to_pandas() to convert the table to a pandas dataframe.\n\nHow big are the partitions? (see Appendix for details)\n\n# median partition\nmedian_part_tbl = neowise_ds.to_table(\n    filter=(KFIELD == median_part), columns=COLUMN_SUBSET\n)\nprint_table_size(table=median_part_tbl, pixel_index=median_part)\n\n\n\nOften only a few columns are needed for processing, so most partitions will fit comfortably in memory.\n(The recommended maximum for an in-memory table/dataframe is typically 1GB, but there is\nno strict upper limit -- performance will depend on the compute resources available.)\n\nHowever, beware that the largest partitions are quite large:\n\n# biggest partition\n# this is very large, so we'll restrict the number of columns to one\nbiggest_part_tbl = neowise_ds.to_table(\n    filter=(KFIELD == biggest_part), columns=COLUMN_SUBSET[:1]\n)\nprint_table_size(table=biggest_part_tbl, pixel_index=biggest_part)\n\n# Additional filters can be included to reduce the number of rows if desired.\n# Another option is to load individual files.\n\n\n\n\n\n# cleanup\ndel median_part_tbl\ndel biggest_part_tbl\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#id-4-example-slice-by-partition","position":19},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"5. Example: Slice by file"},"type":"lvl2","url":"/neowise-source-table-strategies#id-5-example-slice-by-file","position":20},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"5. Example: Slice by file"},"content":"\n\nIf you don’t need data for all years at the same time, you may want to load individual files.\nGenerally, there is 1 file per partition per year, but a few partitions are as large as 6+ files per year.\nMost of the files are about 0.3GB (compressed on disk) but about 1% are > 1GB.\nThus it should be reasonable to load at least a subset of columns for every row in a file.\n\nThe actual “slicing” here is done by using neowise_ds to access a\ndataset \n\nFragment\n(frag in the code below) which represents a single file.\n\n# slice by file and iterate\nfor frag in neowise_ds.get_fragments():\n    # load the slice to get every row in the file, subset of columns\n    file_tbl = frag.to_table(columns=COLUMN_SUBSET)\n\n    # insert your code here to continue processing the file as desired\n\n    # if you need to see which file this is, parse the path\n    print(f\"file path: {frag.path}\")\n    # let's see how much data this loaded\n    print_table_size(table=file_tbl)\n\n    # again, we'll stop after one\n    del file_tbl\n    break\n\n\n\nThis can be combined with the previous example to iterate over the files in a single partition\n(left as an exercise for the reader).\n\nHow big are the files?\n\n# median file\nmedian_file_frag = [\n    frag for frag in neowise_ds.get_fragments() if frag.path == median_file\n][0]\nmedian_file_tbl = median_file_frag.to_table(columns=COLUMN_SUBSET)\nprint_table_size(table=median_file_tbl)\n\n\n\n\n\n# biggest file\nbiggest_file_frag = [\n    frag for frag in neowise_ds.get_fragments() if frag.path == biggest_file\n][0]\nbiggest_file_tbl = biggest_file_frag.to_table(columns=COLUMN_SUBSET)\nprint_table_size(table=biggest_file_tbl)\n\n\n\n\n\n# cleanup\ndel median_file_tbl\ndel biggest_file_tbl\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#id-5-example-slice-by-file","position":21},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"6. Example: Slice by year"},"type":"lvl2","url":"/neowise-source-table-strategies#id-6-example-slice-by-year","position":22},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"6. Example: Slice by year"},"content":"\n\nIf you want to handle the years independently you can work with the per-year datasets.\nWe actually created these “slices” in the Setup section with year_datasets, and that\nsame list is now accessible in neowise_ds.children used below.\nAny of the techniques shown in this notebook or those listed under “See also” can also\nbe applied to the per-year datasets.\n\n# slice by year and iterate. zip with YEARS so that we know which slice this is.\nfor year, year_ds in zip(YEARS, neowise_ds.children):\n    # insert your code here to process year_ds as desired.\n    # filter and load, iterate over partitions or files, etc.\n\n    # we'll just look at some basic metadata.\n    num_rows = sum(frag.metadata.num_rows for frag in year_ds.get_fragments())\n    num_files = len(year_ds.files)\n    print(f\"NEOWISE {year} dataset: {num_rows:,} rows in {num_files:,} files\")\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#id-6-example-slice-by-year","position":23},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"Appendix"},"type":"lvl2","url":"/neowise-source-table-strategies#appendix","position":24},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"Appendix"},"content":"\n\n","type":"content","url":"/neowise-source-table-strategies#appendix","position":25},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"A.1 Considerations when extending to specific use cases","lvl2":"Appendix"},"type":"lvl3","url":"/neowise-source-table-strategies#a-1-considerations-when-extending-to-specific-use-cases","position":26},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"A.1 Considerations when extending to specific use cases","lvl2":"Appendix"},"content":"Because the catalog is so large, you will need to carefully consider your specific problem and\ndetermine how to slice and filter the data most efficiently.\nThere is no one right answer; it will depend on the use case.","type":"content","url":"/neowise-source-table-strategies#a-1-considerations-when-extending-to-specific-use-cases","position":27},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.1.1 Filtering","lvl3":"A.1 Considerations when extending to specific use cases","lvl2":"Appendix"},"type":"lvl4","url":"/neowise-source-table-strategies#a-1-1-filtering","position":28},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.1.1 Filtering","lvl3":"A.1 Considerations when extending to specific use cases","lvl2":"Appendix"},"content":"Filter out as much data as possible as early as possible. Ideas to consider are:\n\nWith the Parquet file format, you can apply filters during the read to avoid loading\nrows that you don’t need.\n\nPandas (not demonstrated here) supports basic filters.\n\nPyArrow (demonstrated here) also supports complex filters which allow you to compare\nvalues between columns and/or construct new columns on the fly (e.g., subtracting\nmagnitude columns to construct a new color column, as done in the AllWISE notebook).\n\nQueries (i.e., loading data by applying filters) will be much more efficient when they\ninclude a filter on the partitioning column (‘healpix_k5’; demonstrated above).\n\nNotice both that this is essentially equivalent to slicing by partition and that\nyou can filter for more than one partition at a time.\n\nThis is highly recommended even if your use case doesn’t explicitly care about it.\nExceptions include situations where you’re working with individual files and when\nit’s impractical or counterproductive for the science.\n\nYou should also include filters specific to your use case if possible.\n\nExceptions: Sometimes it’s not easy to write a dataset filter for the query.\nA cone search is a common example.\nIn principal it could be written as a PyArrow dataset filter, but in practice the correct\nformula is much too complicated.\nIn this case, it’s easier to write dataset filters for broad RA and Dec limits and then\ndo the actual cone search using astropy.\nThis approach is still quite performant (see the NEOWISE light curves notebook).","type":"content","url":"/neowise-source-table-strategies#a-1-1-filtering","position":29},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.1.2 Slicing","lvl3":"A.1 Considerations when extending to specific use cases","lvl2":"Appendix"},"type":"lvl4","url":"/neowise-source-table-strategies#a-1-2-slicing","position":30},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.1.2 Slicing","lvl3":"A.1 Considerations when extending to specific use cases","lvl2":"Appendix"},"content":"Slice the dataset in some way(s), then iterate and/or parallelize over the slices. Ideas to consider are:\n\nChoose your slices so that you can:\n\nRun your processing code on one slice independently. For example, if your code must\nsee all the data for some target object (RA and Dec) at the same time, you may\nslice the dataset by partition, but don’t slice it by year.\n\nLoad all data in the slice into memory at once (after applying your filters during the read).\nThis notebook shows how to determine how big a slice of data is in order to guide this decision.\n\nBy default, slice by partition. If this is too much data, you may also want to slice\nby year and/or file.\n\nIf you have enough memory to load more than one slice simultaneously, parallelize over\nthe slices to speed up your code.","type":"content","url":"/neowise-source-table-strategies#a-1-2-slicing","position":31},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"A.2 Inspect dataset stats","lvl2":"Appendix"},"type":"lvl3","url":"/neowise-source-table-strategies#a-2-inspect-dataset-stats","position":32},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl3":"A.2 Inspect dataset stats","lvl2":"Appendix"},"content":"\n\nWhen deciding how to slice and filter the dataset, it can be useful to understand\ndataset statistics like partition and file sizes.\n\ndef pixel_index_from_path(path, k_column=KCOLUMN):\n    \"\"\"Parse the path and return the partition pixel index.\n\n    Parameters\n    ----------\n    path : str\n        The path to parse.\n    k_column : str (optional)\n        Name of the partitioning column.\n\n    Returns\n    -------\n    int\n        The partition pixel index parsed from the path.\n    \"\"\"\n    pattern = rf\"({k_column}=)([0-9]+)\"  # matches strings like \"healpix_k5=1124\"\n    return int(re.search(pattern, path).group(2))  # pixel index, e.g., 1124\n\n\n\n\n\n# load some file statistics to a dataframe\nfile_stats = pd.DataFrame(\n    columns=[\"path\", KCOLUMN, \"numrows\"],\n    data=[\n        (frag.path, pixel_index_from_path(frag.path), frag.metadata.num_rows)\n        for frag in neowise_ds.get_fragments()\n    ],\n)\n\n\n\n\n\nfile_stats.sample(5)\n\n\n\n\n\nfile_stats.describe()\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#a-2-inspect-dataset-stats","position":33},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.2.1 Dataset statistics per file","lvl3":"A.2 Inspect dataset stats","lvl2":"Appendix"},"type":"lvl4","url":"/neowise-source-table-strategies#a-2-1-dataset-statistics-per-file","position":34},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.2.1 Dataset statistics per file","lvl3":"A.2 Inspect dataset stats","lvl2":"Appendix"},"content":"\n\n# visualize distribution of file sizes (number of rows)\nax = file_stats.numrows.hist(log=True)\nax.set_xlabel(\"Number of rows\")\nax.set_ylabel(\"Number of files\")\n\n\n\n\n\n# largest file\nfile_stats.loc[file_stats.numrows == file_stats.numrows.max()].head(1)\n\n\n\n\n\n# median file\nfile_stats.sort_values(\"numrows\").iloc[len(file_stats.index) // 2]\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#a-2-1-dataset-statistics-per-file","position":35},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.2.2 Dataset statistics per partition","lvl3":"A.2 Inspect dataset stats","lvl2":"Appendix"},"type":"lvl4","url":"/neowise-source-table-strategies#a-2-2-dataset-statistics-per-partition","position":36},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl4":"A.2.2 Dataset statistics per partition","lvl3":"A.2 Inspect dataset stats","lvl2":"Appendix"},"content":"\n\n# get stats per partition\nk_groups = file_stats[[KCOLUMN, \"numrows\"]].groupby(KCOLUMN)\nper_part = k_groups.sum()\nper_part[\"numfiles\"] = k_groups.count()\n\n\n\n\n\nper_part.sample(5)\n\n\n\n\n\nper_part.describe()\n\n\n\n\n\n# visualize number of rows per partition\nper_part.numrows.plot(\n    logy=True, xlabel=f\"{KCOLUMN} pixel index\", ylabel=\"Number of rows per partition\"\n)\n\n\n\n\n\n# largest partition\nper_part.loc[per_part.numrows == per_part.numrows.max()]\n\n\n\n\n\n# median partition\nper_part.sort_values(\"numrows\").iloc[len(per_part.index) // 2]\n\n\n\n","type":"content","url":"/neowise-source-table-strategies#a-2-2-dataset-statistics-per-partition","position":37},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"About this notebook"},"type":"lvl2","url":"/neowise-source-table-strategies#about-this-notebook","position":38},{"hierarchy":{"lvl1":"Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet","lvl2":"About this notebook"},"content":"Author: Troy Raen (IRSA Developer) and the IPAC Science Platform team\n\nUpdated: 2025-03-07\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/neowise-source-table-strategies#about-this-notebook","position":39},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images"},"type":"lvl1","url":"/sia-allwise-atlas","position":0},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images"},"content":"This notebook tutorial demonstrates the process of querying IRSA’s Simple Image Access (SIA) service for AllWISE Atlas images, making a cutout image (thumbnail), and displaying the cutout.\n\n","type":"content","url":"/sia-allwise-atlas","position":1},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Learning Goals"},"type":"lvl2","url":"/sia-allwise-atlas#learning-goals","position":2},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Learning Goals"},"content":"By the end of this tutorial, you will:\n\nLearn how to search the NASA Astronomical Virtual Observatory Directory web portal for a service that provides access to IRSA’s WISE AllWISE Atlas (L3a) coadded images.\n\nUse the Python pyvo package to identify which of IRSA’s AllWISE Atlas images cover a specified coordinate.\n\nDownload one of the identified images.\n\nCreate and display a cutout of the downloaded image.\n\n","type":"content","url":"/sia-allwise-atlas#learning-goals","position":3},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Introduction"},"type":"lvl2","url":"/sia-allwise-atlas#introduction","position":4},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Introduction"},"content":"The AllWISE program builds upon the work of the successful Wide-field Infrared Survey Explorer mission \n\n(WISE; Wright et al. 2010) by combining data from the WISE cryogenic and NEOWISE \n\n(Mainzer et al. 2011 ApJ, 731, 53) post-cryogenic survey phases to form the a comprehensive view of the full mid-infrared sky. The AllWISE Images Atlas is comprised of 18,240 4-band calibrated 1.56°x1.56° FITS images, depth-of-coverage and noise maps, and image metadata produced by coadding nearly 7.9 million Single-exposure images from all survey phases. For more information about the WISE mission, see:\n\nhttps://​irsa​.ipac​.caltech​.edu​/Missions​/wise​.html\n\nThe \n\nNASA/IPAC Infrared Science Archive (IRSA) at Caltech is the archive for AllWISE images and catalogs. The AllWISE Atlas images that are the subject of this tutorial are made accessible via the \n\nInternational Virtual Observatory Alliance (IVOA) \n\nSimple Image Access (SIA) protocol. IRSA’s AllWISE SIA service is registered in the NASA Astronomical Virtual Observatory (NAVO) \n\nDirectory. Based on the registered information, the Python package \n\npyvo can be used to query the SEIP SIA service for a list of images that meet specified criteria, and standard Python libraries can be used to download and manipulate the images.\nOther datasets at IRSA are available through other SIA services:\n\nhttps://​irsa​.ipac​.caltech​.edu​/docs​/program​_interface​/api​_images​.html\n\n","type":"content","url":"/sia-allwise-atlas#introduction","position":5},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Imports"},"type":"lvl2","url":"/sia-allwise-atlas#imports","position":6},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Imports"},"content":"pyvo for querying IRSA’s AllWISE Atlas SIA service\n\nastropy.coordinates for defining coordinates\n\nastropy.nddata for creating an image cutout\n\nastropy.wcs for interpreting the World Coordinate System header keywords of a fits file\n\nastropy.units for attaching units to numbers passed to the SIA service\n\nmatplotlib.pyplot for plotting\n\nastropy.utils.data for downloading files\n\nastropy.io to manipulate FITS files\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install matplotlib astropy pyvo\n\n\n\n\n\nimport pyvo as vo\nfrom astropy.coordinates import SkyCoord\nfrom astropy.nddata import Cutout2D\nfrom astropy.wcs import WCS\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nfrom astropy.utils.data import download_file\nfrom astropy.io import fits\n\n\n\n","type":"content","url":"/sia-allwise-atlas#imports","position":7},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 1 - Setup"},"type":"lvl2","url":"/sia-allwise-atlas#section-1-setup","position":8},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 1 - Setup"},"content":"\n\nSet images to display in the notebook\n\n%matplotlib inline\n\n\n\nDefine coordinates of a bright star\n\nra = 314.30417\ndec = 77.595559\npos = SkyCoord(ra=ra, dec=dec, unit='deg')\n\n\n\n","type":"content","url":"/sia-allwise-atlas#section-1-setup","position":9},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 2 - Lookup and define a service for AllWISE Atlas images"},"type":"lvl2","url":"/sia-allwise-atlas#section-2-lookup-and-define-a-service-for-allwise-atlas-images","position":10},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 2 - Lookup and define a service for AllWISE Atlas images"},"content":"\n\nStart at STScI VAO Registry at \n\nhttps://​vao​.stsci​.edu​/keyword​-search/\n\nLimit by Publisher “NASA/IPAC Infrared Science Archive” and Capability Type “Simple Image Access Protocol” then search on “AllWISE Atlas”\n\nLocate the SIA2 URL \n\nhttps://​irsa​.ipac​.caltech​.edu​/ibe​/sia​/wise​/allwise​/p3am​_cdd?\n\nallwise_service = vo.dal.SIAService(\"https://irsa.ipac.caltech.edu/ibe/sia/wise/allwise/p3am_cdd?\")\n\n\n\n","type":"content","url":"/sia-allwise-atlas#section-2-lookup-and-define-a-service-for-allwise-atlas-images","position":11},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 3 - Search the service"},"type":"lvl2","url":"/sia-allwise-atlas#section-3-search-the-service","position":12},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 3 - Search the service"},"content":"\n\nSearch for images covering within 1 arcsecond of the star\n\nim_table = allwise_service.search(pos=pos, size=1.0*u.arcsec)\n\n\n\nInspect the table that is returned\n\nim_table\n\n\n\n\n\nim_table.to_table().colnames\n\n\n\n\n\nim_table.to_table()['sia_bp_id']\n\n\n\n","type":"content","url":"/sia-allwise-atlas#section-3-search-the-service","position":13},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 4 - Locate and download an image of interest"},"type":"lvl2","url":"/sia-allwise-atlas#section-4-locate-and-download-an-image-of-interest","position":14},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 4 - Locate and download an image of interest"},"content":"\n\nLet’s search the image results for the W3 band image.\n\nfor i in range(len(im_table)):\n    if im_table[i]['sia_bp_id'] == 'W3':\n        break\nprint(im_table[i].getdataurl())\n\n\n\nDownload the image and open it in Astropy\n\nfname = download_file(im_table[i].getdataurl(), cache=True)\nimage1 = fits.open(fname)\n\n\n\n","type":"content","url":"/sia-allwise-atlas#section-4-locate-and-download-an-image-of-interest","position":15},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 5 - Extract a cutout and plot it"},"type":"lvl2","url":"/sia-allwise-atlas#section-5-extract-a-cutout-and-plot-it","position":16},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Section 5 - Extract a cutout and plot it"},"content":"\n\nwcs = WCS(image1[0].header)\n\n\n\n\n\ncutout = Cutout2D(image1[0].data, pos, (60, 60), wcs=wcs)\nwcs = cutout.wcs\n\n\n\n\n\nfig = plt.figure()\n\nax = fig.add_subplot(1, 1, 1, projection=wcs)\nax.imshow(cutout.data, cmap='gray_r', origin='lower',\n          vmax = 1000)\nax.scatter(ra, dec, transform=ax.get_transform('fk5'), s=500, edgecolor='red', facecolor='none')\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/sia-allwise-atlas#section-5-extract-a-cutout-and-plot-it","position":17},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"About this notebook"},"type":"lvl2","url":"/sia-allwise-atlas#about-this-notebook","position":18},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"About this notebook"},"content":"\n\nAuthor: David Shupe, IRSA Scientist, and the IRSA Science Team\n\nUpdated: 2022-02-14\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.\n\n","type":"content","url":"/sia-allwise-atlas#about-this-notebook","position":19},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Citations"},"type":"lvl2","url":"/sia-allwise-atlas#citations","position":20},{"hierarchy":{"lvl1":"Searching for AllWISE Atlas Images","lvl2":"Citations"},"content":"\n\nIf you use astropy for published research, please cite the authors. Follow these links for more information about citing astropy:\n\nCiting astropy\n\nPlease include the following in any published material that makes use of the WISE data products:\n\n“This publication makes use of data products from the Wide-field Infrared Survey Explorer, which is a joint project of the University of California, Los Angeles, and the Jet Propulsion Laboratory/California Institute of Technology, funded by the National Aeronautics and Space Administration.”\n\nPlease also cite the dataset Digital Object Identifier (DOI): \n\nWISE team (2020)\n\n\n\n","type":"content","url":"/sia-allwise-atlas#citations","position":21},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format"},"type":"lvl1","url":"/wise-allwise-catalog-demo","position":0},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format"},"content":"","type":"content","url":"/wise-allwise-catalog-demo","position":1},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Learning Goals"},"type":"lvl2","url":"/wise-allwise-catalog-demo#learning-goals","position":2},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Learning Goals"},"content":"Learn how to load data from the AllWISE parquet catalog that is partitioned by HEALPix pixel index at order 5.\n\nLearn efficient methods for performing common types of searches. This includes:\n\nquery/load using pandas, applying basic filters\n\nquery/load using pyarrow, applying advanced filters that combine and/or compare columns\n\nperform nearest neighbor searches using pyarrow and astropy\n\n","type":"content","url":"/wise-allwise-catalog-demo#learning-goals","position":3},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Introduction"},"type":"lvl2","url":"/wise-allwise-catalog-demo#introduction","position":4},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Introduction"},"content":"This notebook demonstrates access to the \n\nHEALPix-partitioned (order 5), \n\nApache Parquet version of the \n\nAllWISE Source Catalog.\nThe catalog is available through the \n\nAWS Open Data program, as part of the \n\nNASA Open-Source Science Initiative.\nAccess is free and no special permissions or credentials are required.\n\nParquet is convenient for large astronomical catalogs in part because the storage format supports efficient database-style queries on the files themselves, without having to load the catalog into a database (or into memory) first.\nThe AllWISE catalog is fairly large at 340 GB.\nThe examples below demonstrate different methods that can be used to query the catalog, filtering the data and loading only the results.\nEach method accesses the parquet files a bit differently and is useful for different types of queries.\n\n","type":"content","url":"/wise-allwise-catalog-demo#introduction","position":5},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Imports"},"type":"lvl2","url":"/wise-allwise-catalog-demo#imports","position":6},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Imports"},"content":"\n\n# Uncomment the next line to install dependencies if needed.\n# !pip install 'pandas>=1.5.2' 'pyarrow>=10.0.1' matplotlib hpgeom astropy\n\n\n\n\n\nimport sys\n\nimport hpgeom as hp\nimport pandas as pd\nimport pyarrow.compute as pc\nimport pyarrow.dataset as ds\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\nfrom matplotlib import colors\nfrom matplotlib import pyplot as plt\nfrom pyarrow.fs import S3FileSystem\n\n\n\n","type":"content","url":"/wise-allwise-catalog-demo#imports","position":7},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Setup catalog paths and query filters"},"type":"lvl2","url":"/wise-allwise-catalog-demo#setup-catalog-paths-and-query-filters","position":8},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Setup catalog paths and query filters"},"content":"\n\nThis AllWISE catalog is stored in an \n\nAWS S3 cloud storage bucket.\nTo connect to an S3 bucket we just need to point the reader at S3 instead of the local filesystem.\n(Here, a “reader” is a python library that reads parquet files.)\nWe’ll use \n\npyarrow​.fs​.S3FileSystem for this because it is recognized by every reader in examples below, and we’re already using pyarrow.\n(\n\ns3fs is another common option.)\nTo access without credentials, we’ll use the keyword argument anonymous=True.\nMore information about accessing S3 buckets can be found at \n\nIRSA cloud access introduction.\n\nbucket = \"nasa-irsa-wise\"\nfolder = \"wise/allwise/catalogs/p3as_psd/healpix_k5\"\nparquet_root = f\"{bucket}/{folder}/wise-allwise.parquet\"\n\nfs = S3FileSystem(region=\"us-west-2\", anonymous=True)  # the bucket is in region us-west-2\n\n\n\nThese limits will be used to query the catalog using specific filters created in examples below.\nThe Schema Access section (below) shows how to access column names and other schema information.\n\nw1mpro_min = 10.0\nra_min, ra_max = 15, 25  # deg\ndec_min, dec_max = 62, 72  # deg\npolygon_corners = [(ra_min, dec_min), (ra_min, dec_max), (ra_max, dec_max), (ra_max, dec_min)]\nradius = 5 * u.arcmin.to(u.deg)\n\n\n\nThe catalog is partitioned by HEALPix pixel index at order 5.\nQueries can be most efficient when a filter on the partition column is included, since the reader can skip those partitions entirely.\nThus, for queries that include ra/dec constraints, we can usually speed up load times significantly by including a constraint on the HEALPix order 5 pixel index.\nThe required pixel indexes can be calculated using \n\nhpgeom, as demonstrated here.\n\norder = 5  # the catalog is partitioned by HEALPix pixel index at order 5\nnside = hp.order_to_nside(order)  # number of order 5 pixels along one side of a base pixel\n\n\n\n\n\n# polygon search: get the set of pixel indexes that overlap the ra/dec polygon\npolygon_pixels = hp.query_polygon(\n    nside=nside,\n    a=[corner[0] for corner in polygon_corners],  # ra values\n    b=[corner[1] for corner in polygon_corners],  # dec values\n    nest=True,  # catalog uses nested ordering scheme for pixel index\n    inclusive=True,  # return all pixels that overlap with the polygon, and maybe a few more\n)\n\nprint(f\"polygon_pixels contains {len(polygon_pixels)} of a possible {hp.nside_to_npixel(nside)} pixels\")\n\n\n\n\n\n# cone search: get the set of pixel indexes that overlap a 5' circle around the ra/dec min\ncone_pixels = hp.query_circle(\n    nside=nside,\n    a=ra_min,\n    b=dec_min,\n    radius=radius,\n    nest=True,  # catalog uses nested ordering scheme for pixel index\n    inclusive=True,  # return all pixels that overlap with the disk, and maybe a few more\n)\n\n# this can reduce the number of partitions the reader needs to look at from 12288 down to 2\nprint(f\"cone_pixels contains {len(cone_pixels)} of a possible {hp.nside_to_npixel(nside)} pixels\")\n\n\n\n","type":"content","url":"/wise-allwise-catalog-demo#setup-catalog-paths-and-query-filters","position":9},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Example 1:  Pandas with basic filters (magnitude limit and ra/dec polygon)"},"type":"lvl2","url":"/wise-allwise-catalog-demo#example-1-pandas-with-basic-filters-magnitude-limit-and-ra-dec-polygon","position":10},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Example 1:  Pandas with basic filters (magnitude limit and ra/dec polygon)"},"content":"\n\nLoad using \n\npandas.read_parquet.\nFilter for magnitudes above our w1mpro limit and a sky-area limited to the ra/dec polygon.\n\nPandas actually uses either pyarrow or fastparquet to interact with parquet files.\nWe’ll choose pyarrow (the default).\nFor filter options, see the filters arg description in \n\nParquetDataset.\n\n# expect this to take 40-90 seconds\npandas_df = pd.read_parquet(\n    parquet_root,\n    filesystem=fs,\n    # columns to be returned. similar to a SQL SELECT clause.\n    columns=[\"designation\", \"ra\", \"dec\", \"w1mpro\", \"healpix_k5\"],\n    # row filters. similar to a SQL WHERE clause.\n    # tuple conditions are joined by AND (for OR, use a list of lists)\n    # supported operators: ==, !=, <, >, <=, >=, in, not in\n    filters=[\n        (\"w1mpro\", \">\", w1mpro_min),\n        (\"ra\", \">\", ra_min),\n        (\"ra\", \"<\", ra_max),\n        (\"dec\", \">\", dec_min),\n        (\"dec\", \"<\", dec_max),\n        # include filter on partition column for most efficient loading\n        (\"healpix_k5\", \"in\", polygon_pixels),\n    ],\n)\n\npandas_df.describe()\n\n\n\n\n\n# Delete pandas_df to save memory. This is useful when running on a machine with a small amount of RAM.\ndel pandas_df\n\n\n\n","type":"content","url":"/wise-allwise-catalog-demo#example-1-pandas-with-basic-filters-magnitude-limit-and-ra-dec-polygon","position":11},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Example 2:  Pyarrow with advanced filters (color-color cuts for AGN)"},"type":"lvl2","url":"/wise-allwise-catalog-demo#example-2-pyarrow-with-advanced-filters-color-color-cuts-for-agn","position":12},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Example 2:  Pyarrow with advanced filters (color-color cuts for AGN)"},"content":"\n\nLoad using \n\npyarrow​.dataset​.parquet​_dataset and convert to pandas.\nUseful for:\n\nadvanced filters that combine, compare, and/or create new columns. (this example)\n\nspeed. This method is more efficient at “discovering” the dataset than pandas. It also provides a persistent dataset object that can be reused for future queries, where pandas must re-discover the dataset every time. (this example and Example 3)\n\nThis example filters the catalog for AGN by making cuts in color-color space using the selection limits from \n\nMateos et al. (2012).\nThis is a more complicated filter than in Example 1 (it requires both constructing new columns and comparing values between columns) but this load is generally faster, demonstrating the efficiency of this method.\n\nFor basic info about the columns and filter arguments, see \n\nScanner.\nThe construction of columns/filters is more involved than before because they must be passed as \n\nExpressions, and all operations must be done using pyarrow.compute \n\nfunctions and fields.\nThis is demonstrated below.\nNote that the catalog uses a file naming scheme called “hive”, which the reader uses to identify partitions.\nIn other examples this is recognized automatically, but here we must pass it explicitly.\n\n# define new columns for colors W1-W2 and W3-W4\nw1w2 = pc.subtract(pc.field(\"w1mpro\"), pc.field(\"w2mpro\"))\nw3w4 = pc.subtract(pc.field(\"w3mpro\"), pc.field(\"w4mpro\"))\n\n# define the AGN locus, as in Mateos et al., 2012\nlocus = pc.multiply(pc.scalar(0.5), w3w4)\n\n\n\n\n\n# expect this to take 20-60 seconds.\n# notice this is generally faster than example 1 using pandas even though\n# this filter is much more complicated, highlighting the efficiency of this method.\n\n# load catalog metadata as a pyarrow dataset\npyarrow_ds = ds.parquet_dataset(f\"{parquet_root}/_metadata\", filesystem=fs, partitioning=\"hive\")\n\n# query for AGN using selection limits from Mateos et al., 2012\npyarrow_df = pyarrow_ds.to_table(\n    # column filter. similar to a SQL SELECT clause.\n    columns={\n        \"w1w2\": w1w2,\n        \"w3w4\": w3w4,\n        \"cntr\": pc.field(\"cntr\"),\n        \"ra\": pc.field(\"ra\"),\n        \"dec\": pc.field(\"dec\"),\n        \"healpix_k5\": pc.field(\"healpix_k5\"),\n    },\n    # row filter. similar to a SQL WHERE clause.\n    filter=(\n        # color-color cuts\n        (w1w2 < pc.add(locus, pc.scalar(0.979)))\n        & (w1w2 > pc.subtract(locus, pc.scalar(0.405)))\n        & (w3w4 > pc.scalar(1.76))\n        # to do an all-sky search, comment out the rest of the filter. expect it to take 30-60 min\n        # same ra/dec polygon as before\n        & (pc.field(\"ra\") > ra_min)\n        & (pc.field(\"ra\") < ra_max)\n        & (pc.field(\"dec\") > dec_min)\n        & (pc.field(\"dec\") < dec_max)\n        # same partition-column filter as before\n        & (pc.field(\"healpix_k5\").isin(polygon_pixels))\n    ),\n).to_pandas()\n\n\n\n\n\nlen(pyarrow_df.index)\n\n\n\n\n\ncolorbar_norm = colors.LogNorm(vmin=1, vmax=10)  # for an all-sky search, use vmax=100_000\npyarrow_df.plot.hexbin(\"w3w4\", \"w1w2\", norm=colorbar_norm)\n\n\n\n\n\n# Delete variables to save memory. This is useful when running on a machine with a small amount of RAM.\ndel pyarrow_ds\ndel pyarrow_df\n\n\n\n","type":"content","url":"/wise-allwise-catalog-demo#example-2-pyarrow-with-advanced-filters-color-color-cuts-for-agn","position":13},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Example 3:  Nearest-neighbor search (using pyarrow and astropy)"},"type":"lvl2","url":"/wise-allwise-catalog-demo#example-3-nearest-neighbor-search-using-pyarrow-and-astropy","position":14},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Example 3:  Nearest-neighbor search (using pyarrow and astropy)"},"content":"\n\nNearest-neighbor searches and cone searches generally use the on-sky separation distance to determine the matches.\nIt would be cumbersome to construct the new column and filter on it using the methods shown above because the separation distance is a fairly complicated function of ra and dec.\nHowever, we can get pretty fast results by filtering down to the HEALPix pixels that cover the region, loading all the data in those partitions, and then using astropy to compute the separations and find the matches.\n\nHere, we’ll search for the 3 nearest neighbors to each of the 4 corners of our ra/dec polygon.\nWe’ll load the data with pyarrow because because it makes this query significantly faster than with pandas (see explanation in Example 2).\n(Note that astropy can also read parquet, but it can only read a single file at a time and so is less convenient.)\nWe’ll use \n\nastropy​.coordinates​.SkyCoord​.match​_to​_catalog​_sky to do the actual nearest neighbor search.\n\n# construct dictionary of pixels covering a cone around each polygon corner\n# we did this once before but now we want all 4 corners\ncorner_cone_pixels = {\n    (ra, dec): hp.query_circle(nside=nside, a=ra, b=dec, radius=radius, nest=True, inclusive=True)\n    for (ra, dec) in polygon_corners\n}\ncorner_cone_pixels\n\n\n\nFind the 3 nearest neighbors of each corner:\n\n# expect this to take 30-60 seconds\nidcol = \"cntr\"\nneighbor_ids = []  # store the allwise source id (cntr) of each neighbor\ncorner_neighbors = {}  # record neighbor info for each corner (for reference. not actually used)\n\n# use same pyarrow dataset as before, but get it again to include the load time in this cell\npyarrow_ds = ds.parquet_dataset(f\"{parquet_root}/_metadata\", filesystem=fs, partitioning=\"hive\")\n\nfor (ra, dec), cone_pixels in corner_cone_pixels.items():\n\n    # load data from all pixels/partitions/files covering this corner\n    src_tbl = pyarrow_ds.to_table(\n        columns=[idcol, \"ra\", \"dec\"], filter=(pc.field(\"healpix_k5\").isin(cone_pixels))\n    )\n\n    # get list of 3 nearest neighbors to this corner\n    corner = SkyCoord(ra=ra * u.degree, dec=dec * u.degree)\n    allwise_sources = SkyCoord(ra=src_tbl[\"ra\"] * u.degree, dec=src_tbl[\"dec\"] * u.degree)\n    neighbors = [corner.match_to_catalog_sky(allwise_sources, nthneighbor=i) for i in range(1, 4)]\n\n    # get the allwise source ids. record/report some info\n    corner_neighbors[(ra, dec)] = []\n    for n, (idx, sep, _) in enumerate(neighbors):\n        srcid = src_tbl[idcol][idx.item()].as_py()\n        neighbor_ids.append(srcid)\n        corner_neighbors[(ra, dec)].append((srcid, sep))\n        print(f\"neighbor {n+1}, corner ({ra}, {dec}): {srcid} at on-sky dist {sep.to('arcsec')}\")\n\nneighbor_ids\n\n\n\nLoad all the data (all columns) for all nearest neighbors:\n\n# expect this to take 15-25 seconds\ncone_pixels = [pixel for cone_pixels in corner_cone_pixels.values() for pixel in cone_pixels]\nneighbors_df = pyarrow_ds.to_table(\n    filter=((pc.field(idcol).isin(neighbor_ids)) & (pc.field(\"healpix_k5\").isin(cone_pixels)))\n).to_pandas()\n\nneighbors_df\n\n\n\n\n\n# Delete variables to save memory. This is useful when running on a machine with a small amount of RAM.\ndel pyarrow_ds\ndel neighbors_df\n\n\n\n","type":"content","url":"/wise-allwise-catalog-demo#example-3-nearest-neighbor-search-using-pyarrow-and-astropy","position":15},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Schema Access"},"type":"lvl2","url":"/wise-allwise-catalog-demo#schema-access","position":16},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"Schema Access"},"content":"\n\nThe schema can be viewed \n\nonline and also accessed from the parquet catalog itself.\nThe example below loads it using the _common_metadata file and \n\npyarrow​.dataset​.parquet​_dataset.\nNote that the schema can also be accessed from the metadata in the footer of each Parquet file and from the _metadata file, but the method used here is generally faster and easier.\nIn addition, this _common_metadata file has extra information (units and descriptions) stored in the custom metadata of each column.\n\nschema = ds.parquet_dataset(f\"{parquet_root}/_common_metadata\", filesystem=fs).schema\n\n\n\n\n\n# access individual columns by name or index\nfld = schema.field(1)  # equivalently: fld = schema.field(\"ra\")\n\n# basic column information\nprint(fld.name)\nprint(fld.type)\nprint(fld.nullable)\n\n# units and descriptions are in the `metadata` attribute, always as bytestrings\nprint(fld.metadata)\nprint(fld.metadata[b\"units\"].decode())  # use decode to get a regular string\nprint(fld.metadata[b\"description\"].decode())\n\n\n\n\n\n# full list of column names\nschema.names\n\n\n\n","type":"content","url":"/wise-allwise-catalog-demo#schema-access","position":17},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"About this notebook"},"type":"lvl2","url":"/wise-allwise-catalog-demo#about-this-notebook","position":18},{"hierarchy":{"lvl1":"Analyzing cloud-hosted AllWISE Source Catalog in Parquet format","lvl2":"About this notebook"},"content":"Author: Troy Raen (IRSA Developer) in conjunction with David Shupe, Jessica Krick and the IPAC Science Platform team\n\nUpdated: 2023-07-27\n\nContact: \n\nthe IRSA Helpdesk with questions or reporting problems.","type":"content","url":"/wise-allwise-catalog-demo#about-this-notebook","position":19},{"hierarchy":{"lvl1":"WISE Tutorial Notebooks"},"type":"lvl1","url":"/wise","position":0},{"hierarchy":{"lvl1":"WISE Tutorial Notebooks"},"content":"The Wide-field Infrared Survey Explorer (\n\nWISE) is a NASA infrared space telescope launched in December 2009 that performed a sensitive all-sky survey at 3.4, 4.6, 12, and 22 µm, cataloging hundreds of millions of stars, galaxies, and Solar System objects and enabling discoveries of cool brown dwarfs and luminous infrared galaxies.\nAfter exhausting its cryogen, the mission was repurposed as NEOWISE in 2013 to detect and characterize near-Earth asteroids and comets using the remaining infrared channels until the mission concluded in August 2024.\n\nWISE and NEOWISE data are released publicly through the NASA/IPAC Infrared Science Archive (IRSA), including calibrated images, source catalogs, and single-exposure source tables that together enable multi-epoch photometry, light curves, and motion studies for a wide range of astrophysical and Solar System applications.\nSuccessive NEOWISE data releases, issued with annual updates, provided users with increasingly deep coverage and time-domain information across the infrared sky.\n\nAllWISE Images - Retrieve coadded images and make coordinate-based cutouts.\n\nAllWISE Catalog - Querying, filter, and work with a HEALPix-partitioned Parquet catalog using the AllWISE dataset.\n\nNEOWISE Firefly - Visualize and analyze light curves of Solar System objects using Firefly.\n\nNEOWISE Strategies - Use efficient strategies for accessing and handling the very large Parquet dataset.\n\nNEOWISE Light Curves - Build multi-epoch photometric light curves for given coordinates.","type":"content","url":"/wise","position":1}]}