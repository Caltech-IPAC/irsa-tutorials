{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12dff187",
   "metadata": {},
   "source": [
    "# Parallelizing Image Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b82c94",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "- Employ three parallelization libraries to speed up a serial process.\n",
    "- Calculate the speedup of the different approaches shown.\n",
    "- Evaluate which library is suited to your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68071c81",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows how to speed up an image convolution task using these three libraries:\n",
    "\n",
    "* Ray: an open-source unified compute framework that makes it easy to scale AI and Python workloads.\n",
    "* Multiprocessing: part of the standard library; supports spawning processes using an API similar to the threading module; offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads.\n",
    "* Dask: developed to natively scale computational packages like numpy, pandas and scikit-learn, and the surrounding ecosystem, to multi-core machines and distributed clusters when datasets exceed memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372d622",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* _multiprocessing.Pool_ for multiprocessing using the standard library\n",
    "* _time_ for timing the processes\n",
    "* _dask.distributed.Client_ for making a local Dask cluster\n",
    "* _numpy_ and _scipy.signal_ for numerical work\n",
    "* _psutil_ for finding the available processors on your machine\n",
    "* _ray_ for scaling up Python tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import psutil\n",
    "import scipy.signal\n",
    "\n",
    "try:\n",
    "    import ray\n",
    "except ImportError:\n",
    "    !pip install ray\n",
    "    import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e835099",
   "metadata": {},
   "source": [
    "## Find the cpus available\n",
    "\n",
    "Find and print the number of cpus \n",
    "(taken from https://towardsdatascience.com/10x-faster-parallel-python-without-python-multiprocessing-e5017c93cce1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = psutil.cpu_count(logical=True)\n",
    "print(num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cbaf00",
   "metadata": {},
   "source": [
    "## Process serially using a conventional loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5996487",
   "metadata": {},
   "source": [
    "Use `scipy.signal` to convolve two 2-dimensional arrays and return a 5x5 downsampled result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58333413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fconv(image, random_filter):\n",
    "    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [np.random.normal(size=(4, 4)) for _ in range(num_cpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da837434",
   "metadata": {},
   "source": [
    "Process 100 iterations serially, then extrapolate to num_cpus*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "num_iter = 100\n",
    "image = np.zeros((3000, 3000))\n",
    "for i in range(num_iter):\n",
    "    result = fconv(image, filters[i % num_cpus])\n",
    "duration_conv = time.time() - start\n",
    "print(\"(scaled) conventional duration for {:d} iterations = {:.1f} seconds\"\n",
    "      .format(num_cpus*num_iter, duration_conv*num_cpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce470a0b",
   "metadata": {},
   "source": [
    "## Process in parallel using Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38439e7",
   "metadata": {},
   "source": [
    "[Documentation for ray](https://docs.ray.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b0c8c",
   "metadata": {},
   "source": [
    "The warning raised by `ray.init` only affects shared object usage, which is not an issue for this tutorial. It may harm performance in other scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748437fb",
   "metadata": {},
   "source": [
    "Use `scipy.signal` to convolve two 2-dimensional arrays and return a 5x5 downsampled result. To use Ray, we decorate the function that is doing the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fray(image, random_filter):\n",
    "    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0e975",
   "metadata": {},
   "source": [
    "In the following loop, `ray.put` places the image into shared memory. The call to `ray.get` retrieves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "image = np.zeros((3000, 3000))\n",
    "for _ in range(100):\n",
    "    image_id = ray.put(image)\n",
    "    ray.get([fray.remote(image_id, filters[i]) for i in range(num_cpus)])\n",
    "duration_ray = time.time() - start\n",
    "print(\"Ray duration = {:.1f}, speedup = {:.2f}\"\n",
    "      .format(duration_ray, duration_conv*num_cpus / duration_ray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56263597",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac47c1e",
   "metadata": {},
   "source": [
    "## Process in parallel using multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ffc23",
   "metadata": {},
   "source": [
    "[Documentation for multiprocessing](https://docs.python.org/3/library/multiprocessing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0a568",
   "metadata": {},
   "source": [
    "Use `scipy.signal` to convolve two 2-dimensional arrays and return a 5x5 downsampled result. The call to the function has a slightly different form than that for the serial loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmp(args):\n",
    "    image, random_filter = args\n",
    "    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc2d5d",
   "metadata": {},
   "source": [
    "Use a multiprocessing pool with the number of cpus we found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c92055",
   "metadata": {},
   "source": [
    "Using `pool.map` is the closest analog in multiprocessing to the Ray API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "image = np.zeros((3000, 3000))\n",
    "for _ in range(100):\n",
    "    pool.map(fmp, zip(num_cpus * [image], filters))\n",
    "duration_mp = time.time() - start\n",
    "print(\"Multiprocessing duration = {:.1f}, speedup = {:.2f}\"\n",
    "      .format(duration_mp, duration_conv*num_cpus / duration_mp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fbd15",
   "metadata": {},
   "source": [
    "## Process using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b306b6a",
   "metadata": {},
   "source": [
    "[Documentation for Dask](https://www.dask.org/get-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a2868",
   "metadata": {},
   "source": [
    "Define a Dask distributed client with number of workers set to the number of cpus we found earlier, and with one thread per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=num_cpus, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b10512",
   "metadata": {},
   "source": [
    "Dask recommends scattering the large inputs across the workers, though this makes little difference in execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "image = np.zeros((3000, 3000))\n",
    "for _ in range(100):\n",
    "    for j in range(num_cpus):\n",
    "        big_future = client.scatter((image, filters[j % num_cpus]))\n",
    "        future = client.submit(fconv, big_future)\n",
    "duration_dask = time.time() - start\n",
    "print(\"Dask duration = {:.1f}, speedup = {:.2f}\"\n",
    "      .format(duration_dask, duration_conv*num_cpus / duration_dask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e529e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099cf70",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51903b0",
   "metadata": {},
   "source": [
    "* Ray is the most effective at speeding up the convolution workload by fully utilizing all available processes\n",
    "* Multiprocessing is second in effectiveness\n",
    "* Dask delivers the least speedup; perhaps due to having only six processes on the dask.distributed client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f982d2c",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook was developed by David Shupe (shupe@ipac.caltech.edu) in conjunction with Jessica Krick and the IRSA Science Platform team at IPAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900a730",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "If you use these software packages in your work, please use the following citations:\n",
    "\n",
    "* Dask: Dask Development Team (2016). Dask: Library for dynamic task scheduling. URL https://dask.org\n",
    "* Ray: The Ray Development Team. URL https://docs.ray.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de92100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.2"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:clonenv]",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   26,
   36,
   47,
   61,
   68,
   71,
   75,
   79,
   84,
   86,
   90,
   99,
   103,
   107,
   111,
   113,
   117,
   121,
   125,
   136,
   138,
   142,
   146,
   150,
   154,
   158,
   160,
   164,
   172,
   176,
   180,
   184,
   188,
   190,
   194,
   206,
   208,
   212,
   218,
   224,
   233
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}