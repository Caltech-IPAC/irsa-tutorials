{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f181b5",
   "metadata": {},
   "source": [
    "An executed version of this notebook can be seen on\n",
    "[IRSA's website](https://irsa.ipac.caltech.edu/docs/notebooks/neowise-source-table-lightcurves.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4fdee",
   "metadata": {},
   "source": [
    "# Make Light Curves from NEOWISE Single-exposure Source Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa27cfc",
   "metadata": {},
   "source": [
    "Learning Goals:\n",
    "\n",
    "- Search the NEOWISE Single-exposure Source Table (Parquet version) for the light curves of a\n",
    "  set of targets with RA/Dec coordinates.\n",
    "  - Write a pyarrow dataset filter and use it to load the NEOWISE detections near each target (rough cut).\n",
    "  - Match targets to detections using an astropy cone search (precise cut).\n",
    "  - Parallelize this.\n",
    "- Plot the light curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523bc95",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook loads light curves from the\n",
    "[NEOWISE](https://irsa.ipac.caltech.edu/Missions/wise.html) Single-exposure Source Table\n",
    "for a sample of about 2000 cataclysmic variables from [Downes et al. (2001)](https://doi.org/10.1086/320802).\n",
    "The NEOWISE Single-exposure Source Table is a very large catalog -- 11 years and 42 terabytes in total\n",
    "with 145 columns and 200 billion rows.\n",
    "When searching this catalog, it is important to consider the requirements of your use case and\n",
    "the format of this dataset.\n",
    "This notebook applies the techniques developed in\n",
    "[Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet](https://irsa.ipac.caltech.edu/docs/notebooks/neowise-source-table-strategies.html).\n",
    "This is a fully-worked example that demonstrates the important steps, but note that this is a\n",
    "relatively small use case for the Parquet version of the dataset.\n",
    "\n",
    "The specific strategy we employ is:\n",
    "\n",
    "- Choose a cone search radius that determines which NEOWISE source detections to associate\n",
    "  with each target.\n",
    "- Load the sample of CV targets.\n",
    "- Calculate the indexes of all HEALPix order k=5 pixels within the radius of each target.\n",
    "  These are the dataset partitions that need to be searched.\n",
    "- Parallelize over the partitions using `multiprocessing.Pool`.\n",
    "  For each pixel:\n",
    "  - Construct a dataset filter for NEOWISE sources in the vicinity of the targets in the partition.\n",
    "  - Load data, applying the filter. In our case, the number of rows loaded will be fairly small.\n",
    "  - Do a cone search to match sources with targets in the partition.\n",
    "  - Return the results.\n",
    "- Concatenate the cone search results, groupby target ID, and sort by time to construct the light curves.\n",
    "\n",
    "The efficiency of this method will increase with the number of rows needed from each partition.\n",
    "For example, a cone search radius of 1 arcsec will require about 10 CPUs, 65G RAM, and\n",
    "50 minutes to load the data from all 11 NEOWISE years.\n",
    "Increasing the radius to 10 arcsec will return about 2.5x more rows using roughly the same resources.\n",
    "Increasing the target sample size can result in similar efficiency gains.\n",
    "To try out this notebook with fewer resources, use a subset of NEOWISE years.\n",
    "Using one year is expected to require about 5 CPUs, 20G RAM, and 10 minutes.\n",
    "These estimates are based on testing in science platform environments.\n",
    "Your numbers will vary based on many factors including compute power, bandwidth, and physical distance from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db111a",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to install dependencies if needed.\n",
    "# !pip install astropy astroquery hpgeom matplotlib pandas pyarrow pyvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing  # parallelization\n",
    "\n",
    "import astroquery.vizier  # fetch the sample of CV targets\n",
    "import hpgeom  # HEALPix math\n",
    "import numpy as np  # math\n",
    "import pandas as pd  # manipulate tabular data\n",
    "import pyarrow.compute  # construct dataset filters\n",
    "import pyarrow.dataset  # load and query the NEOWISE dataset\n",
    "import pyarrow.fs  # interact with the S3 bucket storing the NEOWISE catalog\n",
    "import pyvo  # TAP service for the Vizier query\n",
    "from astropy import units as u  # manipulate astropy quantities\n",
    "from astropy.coordinates import SkyCoord  # manipulate sky coordinates\n",
    "from matplotlib import pyplot as plt  # plot light curves\n",
    "\n",
    "# copy-on-write will become the default in pandas 3.0 and is generally more performant\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173744fa",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56de48",
   "metadata": {},
   "source": [
    "### 3.1 Define variables\n",
    "\n",
    "First, choose which NEOWISE years to include.\n",
    "Real use cases are likely to require all ten years but it can be helpful to start with\n",
    "fewer while exploring to make things run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all years => about 11 CPU, 65G RAM, and 50 minutes runtime\n",
    "YEARS = [f\"year{yr}\" for yr in range(1, 12)] + [\"addendum\"]\n",
    "\n",
    "# To try out a smaller version of the notebook,\n",
    "# uncomment the next line and choose your own subset of years.\n",
    "# YEARS = [10]  # one year => about 5 CPU, 20G RAM, and 10 minutes runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets of columns that we'll need\n",
    "FLUX_COLUMNS = [\"w1flux\", \"w2flux\"]\n",
    "LIGHTCURVE_COLUMNS = [\"mjd\"] + FLUX_COLUMNS\n",
    "COLUMN_SUBSET = [\"cntr\", \"ra\", \"dec\"] + LIGHTCURVE_COLUMNS\n",
    "\n",
    "# cone-search radius defining which NEOWISE sources are associated with each target object\n",
    "MATCH_RADIUS = 1 * u.arcsec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94f23f",
   "metadata": {},
   "source": [
    "### 3.2 Load NEOWISE metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f719db2",
   "metadata": {},
   "source": [
    "The metadata contains column names, schema, and row-group statistics for every file in the dataset.\n",
    "We'll load it as a pyarrow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This catalog is so big that even the metadata is big.\n",
    "# Expect this cell to take about 30 seconds per year.\n",
    "\n",
    "# This information can be found at https://irsa.ipac.caltech.edu/cloud_access/.\n",
    "bucket = \"nasa-irsa-wise\"\n",
    "base_prefix = \"wise/neowiser/catalogs/p1bs_psd/healpix_k5\"\n",
    "metadata_path = (\n",
    "    lambda yr: f\"{bucket}/{base_prefix}/{yr}/neowiser-healpix_k5-{yr}.parquet/_metadata\"\n",
    ")\n",
    "fs = pyarrow.fs.S3FileSystem(region=\"us-west-2\", anonymous=True)\n",
    "\n",
    "# list of datasets, one per year\n",
    "year_datasets = [\n",
    "    pyarrow.dataset.parquet_dataset(metadata_path(yr), filesystem=fs, partitioning=\"hive\")\n",
    "    for yr in YEARS\n",
    "]\n",
    "\n",
    "# unified dataset, all years\n",
    "neowise_ds = pyarrow.dataset.dataset(year_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffd3ea",
   "metadata": {},
   "source": [
    "## 4. Define functions to filter and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e54c0",
   "metadata": {},
   "source": [
    "These functions will be used in the next section.\n",
    "Defining them here in the notebook is useful for demonstration and should work seamlessly on Linux, which includes most science platforms.\n",
    "Mac and Windows users should see the note at the end of the notebook.\n",
    "However, note that this use case is likely too large for a laptop and may perform poorly and/or crash if attempted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44518b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have your own list of target objects, replace this function to load your sample.\n",
    "def load_targets_Downes2001(radius=1 * u.arcsec):\n",
    "    \"\"\"Load a sample of targets and return a pandas DataFrame.\n",
    "\n",
    "    References:\n",
    "    - Downes et al., 2001 ([2001PASP..113..764D](https://ui.adsabs.harvard.edu/abs/2001PASP..113..764D/abstract)).\n",
    "    - https://cdsarc.cds.unistra.fr/ftp/V/123A/ReadMe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radius : astropy.Quantity (optional)\n",
    "        Radius for the cone search around each target. This is used to determine which partition(s)\n",
    "        need to be searched for a given target. Use the same radius here as in the rest of the notebook.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The loaded targets with the following columns:\n",
    "            - uid: Unique identifier of the target.\n",
    "            - GCVS: Name in the General Catalogue of Variable Stars if it exists, else the constellation name.\n",
    "            - RAJ2000: Right Ascension of the target in J2000 coordinates.\n",
    "            - DEJ2000: Declination of the target in J2000 coordinates.\n",
    "            - healpix_k5: HEALPix pixel index at order k=5.\n",
    "    \"\"\"\n",
    "    astroquery.vizier.Vizier.ROW_LIMIT = -1\n",
    "    # https://cdsarc.cds.unistra.fr/vizier/notebook.gml?source=V/123A\n",
    "    # https://cdsarc.cds.unistra.fr/ftp/V/123A/ReadMe\n",
    "    CATALOGUE = \"V/123A\"\n",
    "    voresource = pyvo.registry.search(ivoid=f\"ivo://CDS.VizieR/{CATALOGUE}\")[0]\n",
    "    tap_service = voresource.get_service(\"tap\")\n",
    "\n",
    "    # Query Vizier and load targets to a dataframe.\n",
    "    cv_columns = [\"uid\", \"GCVS\", \"RAJ2000\", \"DEJ2000\"]\n",
    "    cvs_records = tap_service.run_sync(\n",
    "        f'SELECT {\",\".join(cv_columns)} from \"{CATALOGUE}/cv\"'\n",
    "    )\n",
    "    cvs_df = cvs_records.to_table().to_pandas()\n",
    "\n",
    "    # Add a new column containing a list of all order k HEALPix pixels that overlap with\n",
    "    # the CV's position plus search radius.\n",
    "    cvs_df[\"healpix_k5\"] = [\n",
    "        hpgeom.query_circle(\n",
    "            a=cv.RAJ2000,\n",
    "            b=cv.DEJ2000,\n",
    "            radius=radius.to_value(\"deg\"),\n",
    "            nside=hpgeom.order_to_nside(order=5),\n",
    "            nest=True,\n",
    "            inclusive=True,\n",
    "        )\n",
    "        for cv in cvs_df.itertuples()\n",
    "    ]\n",
    "    # Explode the lists of pixels so the dataframe has one row per target per pixel.\n",
    "    # Targets near a pixel boundary will now have more than one row.\n",
    "    # Later, we'll search each pixel separately for NEOWISE detections and then\n",
    "    # concatenate the matches for each target to produce complete light curves.\n",
    "    cvs_df = cvs_df.explode(\"healpix_k5\", ignore_index=True)\n",
    "\n",
    "    return cvs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af026135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function.\n",
    "def load_lightcurves_one_partition(targets_group):\n",
    "    \"\"\"Load lightcurves from a single partition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    targets_group : tuple\n",
    "        Tuple of pixel index and sub-DataFrame (result of DataFrame groupby operation).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The lightcurves for targets found in this partition.\n",
    "    \"\"\"\n",
    "    # These global variables will be set when the worker is initialized.\n",
    "    global _neowise_ds\n",
    "    global _columns\n",
    "    global _radius\n",
    "\n",
    "    # Get row filters that will limit the amount of data loaded from this partition.\n",
    "    # It is important for these filters to be efficient for the specific use case.\n",
    "    filters = _construct_dataset_filters(targets_group=targets_group, radius=_radius)\n",
    "\n",
    "    # Load this slice of the dataset to a pyarrow Table.\n",
    "    pixel_tbl = _neowise_ds.to_table(columns=_columns, filter=filters)\n",
    "\n",
    "    # Associate NEOWISE detections with targets to get the light curves.\n",
    "    lightcurves_df = _cone_search(\n",
    "        targets_group=targets_group, pixel_tbl=pixel_tbl, radius=_radius\n",
    "    )\n",
    "\n",
    "    return lightcurves_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012319af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filters returned by this function need to be efficient for the specific use case.\n",
    "def _construct_dataset_filters(*, targets_group, radius, scale_factor=100):\n",
    "    \"\"\"Construct dataset filters for a box search around all targets in the partition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    targets_group : tuple\n",
    "        Tuple of pixel index and sub-DataFrame (result of DataFrame groupby operation).\n",
    "    radius : astropy.Quantity\n",
    "        The radius used for constructing the RA and Dec filters.\n",
    "    scale_factor : int (optional)\n",
    "        Factor by which the radius will be multiplied to ensure that the box encloses\n",
    "        all relevant detections.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filters : pyarrow.compute.Expression\n",
    "        The constructed filters based on the given inputs.\n",
    "    \"\"\"\n",
    "    pixel, locations_df = targets_group\n",
    "\n",
    "    # Start with a filter for the partition. This is the most important one because\n",
    "    # it tells the Parquet reader to just skip all the other partitions.\n",
    "    filters = pyarrow.compute.field(\"healpix_k5\") == pixel\n",
    "\n",
    "    # Add box search filters. For our CV sample, one box encompassing all targets in\n",
    "    # the partition should be sufficient. Make a different choice if you use a different\n",
    "    # sample and find that this loads more data than you want to handle at once.\n",
    "    buffer_dist = scale_factor * radius.to_value(\"deg\")\n",
    "    for coord, target_coord in zip([\"ra\", \"dec\"], [\"RAJ2000\", \"DEJ2000\"]):\n",
    "        coord_fld = pyarrow.compute.field(coord)\n",
    "\n",
    "        # Add a filter for coordinate lower limit.\n",
    "        coord_min = locations_df[target_coord].min()\n",
    "        filters = filters & (coord_fld > coord_min - buffer_dist)\n",
    "\n",
    "        # Add a filter for coordinate upper limit.\n",
    "        coord_max = locations_df[target_coord].max()\n",
    "        filters = filters & (coord_fld < coord_max + buffer_dist)\n",
    "\n",
    "    # Add your own additional requirements here, like magnitude limits or quality cuts.\n",
    "    # See the AllWISE notebook for more filter examples and links to pyarrow documentation.\n",
    "    # We'll add a filter for sources not affected by contamination or confusion.\n",
    "    filters = filters & pyarrow.compute.equal(pyarrow.compute.field(\"cc_flags\"), \"0000\")\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cone_search(*, targets_group, pixel_tbl, radius):\n",
    "    \"\"\"Perform a cone search to select NEOWISE detections belonging to each target object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    targets_group : tuple\n",
    "        Tuple of pixel index and sub-DataFrame (result of DataFrame groupby operation).\n",
    "    pixel_tbl : pyarrow.Table\n",
    "        Table of NEOWISE data for a single pixel\n",
    "    radius : astropy.Quantity\n",
    "        Cone search radius.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    match_df : pd.DataFrame\n",
    "        A dataframe with all matched sources.\n",
    "    \"\"\"\n",
    "    _, targets_df = targets_group\n",
    "\n",
    "    # Cone search using astropy to select NEOWISE detections belonging to each object.\n",
    "    pixel_skycoords = SkyCoord(ra=pixel_tbl[\"ra\"] * u.deg, dec=pixel_tbl[\"dec\"] * u.deg)\n",
    "    targets_skycoords = SkyCoord(targets_df[\"RAJ2000\"], targets_df[\"DEJ2000\"], unit=u.deg)\n",
    "    targets_ilocs, pixel_ilocs, _, _ = pixel_skycoords.search_around_sky(\n",
    "        targets_skycoords, radius\n",
    "    )\n",
    "\n",
    "    # Create a dataframe with all matched source detections.\n",
    "    match_df = pixel_tbl.take(pixel_ilocs).to_pandas()\n",
    "\n",
    "    # Add the target IDs by joining with targets_df.\n",
    "    match_df[\"targets_ilocs\"] = targets_ilocs\n",
    "    match_df = match_df.set_index(\"targets_ilocs\").join(targets_df.reset_index().uid)\n",
    "\n",
    "    return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256177c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be called once for each worker in the pool.\n",
    "def init_worker(neowise_ds, columns, radius):\n",
    "    \"\"\"Set global variables '_neowise_ds', '_columns', and '_radius'.\n",
    "\n",
    "    These variables will be the same for every call to 'load_lightcurves_one_partition'\n",
    "    and will be set once for each worker. It is important to pass 'neowise_ds' this\n",
    "    way because of its size and the way it will be used. (For the other two, it makes\n",
    "    little difference whether we use this method or pass them directly as function\n",
    "    arguments to 'load_lightcurves_one_partition'.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neowise_ds : pyarrow.dataset.Dataset\n",
    "        NEOWISE metadata loaded as a PyArrow dataset.\n",
    "    columns : list\n",
    "        Columns to include in the output DataFrame of light curves.\n",
    "    radius : astropy.Quantity\n",
    "        Cone search radius.\n",
    "    \"\"\"\n",
    "    global _neowise_ds\n",
    "    _neowise_ds = neowise_ds\n",
    "    global _columns\n",
    "    _columns = columns\n",
    "    global _radius\n",
    "    _radius = radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef395e",
   "metadata": {},
   "source": [
    "## 5. Load light curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf0dd3",
   "metadata": {},
   "source": [
    "Load the target objects' coordinates and other info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da27416",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = load_targets_Downes2001(radius=MATCH_RADIUS)\n",
    "targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a3a4a",
   "metadata": {},
   "source": [
    "Search the NEOWISE Source Table for all targets (positional matches) and load the light curves.\n",
    "Partitions are searched in parallel.\n",
    "For targets located near partition boundaries, relevant partitions will be searched\n",
    "independently for the given target and the results will be concatenated.\n",
    "If searching all NEOWISE years, this may take 45 minutes or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group targets by partition. 'load_lightcurves_one_partition' will be called once per group.\n",
    "targets_groups = targets_df.groupby(\"healpix_k5\")\n",
    "# Arguments for 'init_worker'.\n",
    "init_args = (neowise_ds, COLUMN_SUBSET, MATCH_RADIUS)\n",
    "\n",
    "# Start a multiprocessing pool and load the target light curves in parallel.\n",
    "# About 1900 unique pixels in targets_df, 8 workers, 48 chunksize => ~5 chunks per worker.\n",
    "nworkers = 8\n",
    "chunksize = 48\n",
    "with multiprocessing.Pool(nworkers, initializer=init_worker, initargs=init_args) as pool:\n",
    "    lightcurves = []\n",
    "    for lightcurves_df in pool.imap_unordered(\n",
    "        load_lightcurves_one_partition, targets_groups, chunksize=chunksize\n",
    "    ):\n",
    "        lightcurves.append(lightcurves_df)\n",
    "neowise_lightcurves_df = pd.concat(lightcurves).sort_values(\"mjd\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "neowise_lightcurves_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eec8e4",
   "metadata": {},
   "source": [
    "## 6. Plot NEOWISE light curves\n",
    "\n",
    "The light curves will have large gaps due to the observing cadence, so we'll plot each\n",
    "\"epoch\" separately to see them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0092d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the light curves of the target with the most data\n",
    "target_uid = neowise_lightcurves_df.groupby(\"uid\").mjd.count().sort_values().index[-1]\n",
    "target_df = neowise_lightcurves_df.loc[neowise_lightcurves_df.uid == target_uid]\n",
    "\n",
    "# list of indexes that separate epochs (arbitrarily at delta mjd > 30)\n",
    "epoch_idxs = target_df.loc[target_df.mjd.diff() > 30].index.to_list()\n",
    "epoch_idxs = epoch_idxs + [target_df.index[-1]]  # add the final index\n",
    "\n",
    "# make the figure\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(len(epoch_idxs) / ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, sharey=True, figsize=(3 * ncols, 2.5 * nrows))\n",
    "axs = axs.flatten()\n",
    "idx0 = target_df.index[0]\n",
    "for i, (idx1, ax) in enumerate(zip(epoch_idxs, axs)):\n",
    "    epoch_df = target_df.loc[idx0 : idx1 - 1, LIGHTCURVE_COLUMNS].set_index(\"mjd\")\n",
    "    for col in FLUX_COLUMNS:\n",
    "        ax.plot(epoch_df[col], \".\", markersize=3, label=col)\n",
    "    ax.set_title(f\"epoch {i}\")\n",
    "    ax.xaxis.set_ticks(  # space by 10\n",
    "        range(int((ax.get_xlim()[0] + 10) / 10) * 10, int(ax.get_xlim()[1]), 10)\n",
    "    )\n",
    "    idx0 = idx1\n",
    "axs[0].legend()\n",
    "fig.supxlabel(\"MJD\")\n",
    "fig.supylabel(\"RAW FLUX\")\n",
    "fig.suptitle(f\"NEOWISE light curves for target CV {target_uid}\")\n",
    "fig.tight_layout()\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6566352",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[*] Note to Mac and Windows users:\n",
    "\n",
    "You will need to copy the functions and imports from this notebook into a separate '.py' file and then import them in order to use the multiprocessing pool for parallelization.\n",
    "In addition, you may need to load `neowise_ds` separately for each child process (i.e., worker) by adding that code to the `init_worker` function instead of passing it in as an argument.\n",
    "This has to do with differences in what does / does not get copied into the child processes on different platforms.\n",
    "\n",
    "***\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "**Author:** Troy Raen (IRSA Developer) and the IPAC Science Platform team\n",
    "\n",
    "**Updated:** 2025-03-07\n",
    "\n",
    "**Contact:** [the IRSA Helpdesk](https://irsa.ipac.caltech.edu/docs/help_desk.html) with questions or reporting problems."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "science_demo",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   17,
   21,
   32,
   73,
   77,
   82,
   99,
   103,
   111,
   120,
   128,
   132,
   137,
   157,
   161,
   168,
   229,
   264,
   313,
   350,
   377,
   381,
   385,
   388,
   396,
   415,
   417,
   424,
   454
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}