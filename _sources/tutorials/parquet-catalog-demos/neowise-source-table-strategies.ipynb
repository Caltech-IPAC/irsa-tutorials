{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80840337",
   "metadata": {},
   "source": [
    "An executed version of this notebook can be seen on\n",
    "[IRSA's website](https://irsa.ipac.caltech.edu/docs/notebooks/neowise-source-table-strategies.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c182ab",
   "metadata": {},
   "source": [
    "# Strategies to Efficiently Work with NEOWISE Single-exposure Source Table in Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bf4b1",
   "metadata": {},
   "source": [
    "This notebook discusses strategies for working with the Apache Parquet version of the\n",
    "[NEOWISE](https://irsa.ipac.caltech.edu/Missions/wise.html) Single-exposure Source Table\n",
    "and provides the basic code needed for each approach.\n",
    "This is a very large catalog -- 11 years and 42 terabytes in total with 145 columns and 200 billion rows.\n",
    "Most of the work shown in this notebook is how to efficiently deal with so much data.\n",
    "\n",
    "Learning Goals:\n",
    "\n",
    "- Identify use cases that will benefit most from using this Parquet format.\n",
    "- Understand how this dataset is organized and three different ways to efficiently\n",
    "  slice it in order to obtain a subset small enough to load into memory and process.\n",
    "- Feel prepared to apply these methods to a new use case and determine efficient filtering and slicing options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c669d",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412b92f",
   "metadata": {},
   "source": [
    "The NEOWISE Single-exposure Source Table comprises 11 years of data.\n",
    "Each year on its own would be considered \"large\" compared to astronomy catalogs produced\n",
    "contemporaneously, so working with the full dataset requires extra consideration.\n",
    "In this Parquet version, each year is stored as an independent Parquet dataset.\n",
    "This tutorial shows how to combine them and work with all years as one.\n",
    "The data are partitioned by HEALPix ([GÃ³rski et al., 2005](https://doi.org/10.1086/427976)) order k=5.\n",
    "HEALPix is a tessellation of the sky, so partitioning the dataset this way makes it especially\n",
    "efficient for spatial queries.\n",
    "In addition, the access methods shown below are expected to perform well when parallelized.\n",
    "\n",
    "The terms \"partition\", \"filter\", and \"slice\" all relate to subsets of data.\n",
    "In this notebook we'll use them with definitions that overlap but are not identical, as follows.\n",
    "A \"partition\" includes all data observed in a single HEALPix pixel.\n",
    "This data is grouped together in the Parquet files.\n",
    "A \"filter\" is a set of criteria defined by the user and applied when reading data so that only the desired rows are loaded.\n",
    "We'll use it exclusively to refer to a PyArrow `filter`.\n",
    "The criteria can include partitions and/or any other column in the dataset.\n",
    "A \"slice\" is a generic subset of data.\n",
    "There are a few ways to obtain a slice; one is by applying a filter.\n",
    "\n",
    "This notebook is expected to require about 2 CPUs and 50G RAM and to complete in about 10 minutes.\n",
    "These estimates are based on testing in science platform environments.\n",
    "Your numbers will vary based on many factors including compute power, bandwidth, and physical distance from the data.\n",
    "The required RAM and runtime can be reduced by limiting the number of NEOWISE years loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d666bd",
   "metadata": {},
   "source": [
    "### 1.1 When to use the Parquet version\n",
    "\n",
    "IRSA provides large catalogs in file formats (e.g., Parquet) primarily to support use cases that require bulk access.\n",
    "The Parquet version of the NEOWISE Source Table, coupled with the methods demonstrated in this tutorial,\n",
    "are expected to be the most efficient option for large-ish use cases like classifying, clustering, or\n",
    "building light curves for many objects.\n",
    "In general, the efficiency will increase with the number of rows required from each slice because the slice\n",
    "only needs to be searched once regardless of the number of rows that are actually loaded.\n",
    "In addition, this access route tends to perform well when parallelized.\n",
    "Note that these use cases (including this notebook) are often too large for a laptop and may perform\n",
    "poorly and/or crash if attempted.\n",
    "\n",
    "For small-ish use cases like searching for a handful of objects, other access routes like\n",
    "PyVO and TAP queries will be faster.\n",
    "\n",
    "Consider using this tutorial if either of the following is true:\n",
    "\n",
    "- Your use case is large enough that you are considering parallelizing your code to speed it up.\n",
    "- Your sample size is large enough that loading the data using a different method is likely to\n",
    "  take hours, days, or longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e956c9",
   "metadata": {},
   "source": [
    "### 1.2 Recommended approach\n",
    "\n",
    "The basic process is:\n",
    "\n",
    "1. Load the catalog metadata as a PyArrow dataset.\n",
    "2. Decide how to slice the dataset (e.g., by year, partition, and/or file) depending on your use case.\n",
    "3. Iterate and/or parallelize over the slices. For each slice:\n",
    "    1. Use the PyArrow dataset to load data of interest, applying row filters during the read.\n",
    "    2. Process the data as you like (e.g., cluster, classify, etc.).\n",
    "    3. Write your results to disk and/or return them for further processing.\n",
    "4. (Optional) Concatenate your results and continue processing.\n",
    "\n",
    "This notebook covers steps 1 through 3.1 and indicates where to insert your own code to proceed with 3.2.\n",
    "Here we iterate over slices, but the same code can be parallelized using any multi-processing framework.\n",
    "A fully-worked example is shown in the light curve notebook linked below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd4d99",
   "metadata": {},
   "source": [
    "### 1.3 See also\n",
    "\n",
    "- [IRSA Cloud Access Intro](https://irsa.ipac.caltech.edu/docs/notebooks/cloud-access-intro.html)\n",
    "- [AllWISE Source Catalog Demo](https://irsa.ipac.caltech.edu/docs/notebooks/wise-allwise-catalog-demo.html)\n",
    "- [Make Light Curves from NEOWISE Single-exposure Source Table](https://irsa.ipac.caltech.edu/docs/notebooks/neowise-source-table-lightcurves.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1173c",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c200442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to install dependencies if needed.\n",
    "# !pip install hpgeom pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # parse strings\n",
    "import sys  # check size of loaded data\n",
    "\n",
    "import hpgeom  # HEALPix math\n",
    "import pandas as pd  # store and manipulate table data\n",
    "import pyarrow.compute  # construct dataset filters\n",
    "import pyarrow.dataset  # load and query the NEOWISE dataset\n",
    "import pyarrow.fs  # interact with the S3 bucket storing the NEOWISE catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bcc24",
   "metadata": {},
   "source": [
    "## 3. Setup\n",
    "\n",
    "### 3.1 Define variables and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f24a9",
   "metadata": {},
   "source": [
    "Choose which NEOWISE years to include.\n",
    "Expect the notebook to require about 4G RAM and 1 minute of runtime per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All NEOWISE years => about 40G RAM and 10 minutes runtime\n",
    "YEARS = [f\"year{yr}\" for yr in range(1, 12)] + [\"addendum\"]\n",
    "\n",
    "# To reduce the needed RAM or runtime, uncomment the next line and choose your own years.\n",
    "# Years 1 and 8 are needed for the median_file and biggest_file (defined below).\n",
    "# YEARS = [1, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7638e8f",
   "metadata": {},
   "source": [
    "Column and partition variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of columns to load\n",
    "flux_columns = [\"w1flux\", \"w1sigflux\", \"w2flux\", \"w2sigflux\"]\n",
    "COLUMN_SUBSET = [\"cntr\", \"source_id\", \"ra\", \"dec\"] + flux_columns\n",
    "\n",
    "# partitioning info. do not change these values.\n",
    "K = 5  # healpix order of the catalog partitioning\n",
    "KCOLUMN = \"healpix_k5\"  # partitioning column name\n",
    "KFIELD = pyarrow.compute.field(KCOLUMN)  # pyarrow compute field, to be used in filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64a662",
   "metadata": {},
   "source": [
    "Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ef7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to look at several different files, so make a function to return the path.\n",
    "def neowise_path(year, file=\"_metadata\"):\n",
    "    \"\"\"Return the path to a file. Default is \"_metadata\" file of the given year's dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        NEOWISE year for which the path is being generated.\n",
    "    file : str\n",
    "        The name of the file to the returned path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The path to the file.\n",
    "    \"\"\"\n",
    "    # This information can be found at https://irsa.ipac.caltech.edu/cloud_access/.\n",
    "    bucket = \"nasa-irsa-wise\"\n",
    "    base_prefix = \"wise/neowiser/catalogs/p1bs_psd/healpix_k5\"\n",
    "    root_dir = f\"{bucket}/{base_prefix}/{year}/neowiser-healpix_k5-{year}.parquet\"\n",
    "    return f\"{root_dir}/{file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc4a985",
   "metadata": {},
   "source": [
    "Some representative partitions and files (see dataset stats in the Appendix for how we determine these values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee620bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel index of the median partition and the biggest partition by number of rows\n",
    "median_part = 11_831\n",
    "biggest_part = 8_277\n",
    "\n",
    "# path to the median file and the biggest file by file size on disk (see Appendix)\n",
    "median_file = neowise_path(\"year8\", \"healpix_k0=1/healpix_k5=1986/part0.snappy.parquet\")\n",
    "biggest_file = neowise_path(\"year1\", \"healpix_k0=2/healpix_k5=2551/part0.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0d123",
   "metadata": {},
   "source": [
    "Convenience function for displaying a table size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this function throughout the notebook to see how big different tables are.\n",
    "def print_table_size(table, pixel_index=None):\n",
    "    \"\"\"Prints the shape (rows x columns) and size (GiB) of the given table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pyarrow.Table\n",
    "        The table for which to print the size.\n",
    "    pixel_index : int or str or None\n",
    "        The pixel index corresponding to the partition this table was loaded from.\n",
    "    \"\"\"\n",
    "    if pixel_index is not None:\n",
    "        print(f\"pixel index: {pixel_index}\")\n",
    "    print(f\"table shape: {table.num_rows:,} rows x {table.num_columns} columns\")\n",
    "    print(f\"table size: {sys.getsizeof(table) / 1024**3:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe51613",
   "metadata": {},
   "source": [
    "### 3.2 Load NEOWISE metadata as a pyarrow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e7fc9",
   "metadata": {},
   "source": [
    "The metadata contains column names, schema, and row-group statistics for every file in the dataset.\n",
    "Later, we will use this pyarrow dataset object to slice and query the catalog in several different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This catalog is so big that even the metadata is big.\n",
    "# Expect this cell to take about 30 seconds per year.\n",
    "fs = pyarrow.fs.S3FileSystem(region=\"us-west-2\", anonymous=True)\n",
    "\n",
    "# list of datasets, one per year\n",
    "year_datasets = [\n",
    "    pyarrow.dataset.parquet_dataset(neowise_path(yr), filesystem=fs, partitioning=\"hive\")\n",
    "    for yr in YEARS\n",
    "]\n",
    "\n",
    "# unified dataset, all years\n",
    "neowise_ds = pyarrow.dataset.dataset(year_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff504a",
   "metadata": {},
   "source": [
    "`neowise_ds` is a [UnionDataset](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.UnionDataset.html).\n",
    "All methods demonstrated for pyarrow datasets in the AllWISE demo notebook can be used with\n",
    "`neowise_ds` and will be applied to all years as if they were a single dataset.\n",
    "In addition, a separate [Dataset](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html)\n",
    "for each year is stored in the list attribute `neowise_ds.children` (== `year_datasets`, loaded above),\n",
    "and the same methods can be applied to them individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b5ef6",
   "metadata": {},
   "source": [
    "## 4. Example: Slice by partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358cf78",
   "metadata": {},
   "source": [
    "This example shows how to load data from each partition separately.\n",
    "The actual \"slicing\" is done by applying a filter to the pyarrow dataset `neowise_ds`.\n",
    "Constructing filters was discussed in the AllWISE notebook linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddadca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of order K pixels covering the full sky\n",
    "npixels = hpgeom.nside_to_npixel(hpgeom.order_to_nside(order=K))\n",
    "\n",
    "# iterate over all partitions\n",
    "for pix in range(npixels):\n",
    "\n",
    "    # slice and load to get all rows in this partition, subset of columns\n",
    "    pixel_tbl = neowise_ds.to_table(filter=(KFIELD == pix), columns=COLUMN_SUBSET)\n",
    "\n",
    "    # insert your code here to continue processing\n",
    "\n",
    "    # we'll just print the table size to get a sense of how much data has been loaded\n",
    "    print_table_size(table=pixel_tbl, pixel_index=pix)\n",
    "\n",
    "    # when done, you may want to delete pixel_tbl to free the memory\n",
    "    del pixel_tbl\n",
    "    # we'll stop after one partition\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f815efa",
   "metadata": {},
   "source": [
    "`pixel_tbl` is a (pyarrow) [Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html)\n",
    "containing all NEOWISE sources with an ra/dec falling within HEALPix order 5 pixel `pix`.\n",
    "Use `pixel_tbl.to_pandas()` to convert the table to a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e64f3",
   "metadata": {},
   "source": [
    "How big are the partitions? (see Appendix for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median partition\n",
    "median_part_tbl = neowise_ds.to_table(\n",
    "    filter=(KFIELD == median_part), columns=COLUMN_SUBSET\n",
    ")\n",
    "print_table_size(table=median_part_tbl, pixel_index=median_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a5c35",
   "metadata": {},
   "source": [
    "Often only a few columns are needed for processing, so most partitions will fit comfortably in memory.\n",
    "(The recommended maximum for an in-memory table/dataframe is typically 1GB, but there is\n",
    "no strict upper limit -- performance will depend on the compute resources available.)\n",
    "\n",
    "However, beware that the largest partitions are quite large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26979755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest partition\n",
    "# this is very large, so we'll restrict the number of columns to one\n",
    "biggest_part_tbl = neowise_ds.to_table(\n",
    "    filter=(KFIELD == biggest_part), columns=COLUMN_SUBSET[:1]\n",
    ")\n",
    "print_table_size(table=biggest_part_tbl, pixel_index=biggest_part)\n",
    "\n",
    "# Additional filters can be included to reduce the number of rows if desired.\n",
    "# Another option is to load individual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37dd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "del median_part_tbl\n",
    "del biggest_part_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773ecf1",
   "metadata": {},
   "source": [
    "## 5. Example: Slice by file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c054a42",
   "metadata": {},
   "source": [
    "If you don't need data for all years at the same time, you may want to load individual files.\n",
    "Generally, there is 1 file per partition per year, but a few partitions are as large as 6+ files per year.\n",
    "Most of the files are about 0.3GB (compressed on disk) but about 1% are > 1GB.\n",
    "Thus it should be reasonable to load at least a subset of columns for every row in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6c05e",
   "metadata": {},
   "source": [
    "The actual \"slicing\" here is done by using `neowise_ds` to access a\n",
    "dataset [Fragment](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Fragment.html)\n",
    "(`frag` in the code below) which represents a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdede1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice by file and iterate\n",
    "for frag in neowise_ds.get_fragments():\n",
    "    # load the slice to get every row in the file, subset of columns\n",
    "    file_tbl = frag.to_table(columns=COLUMN_SUBSET)\n",
    "\n",
    "    # insert your code here to continue processing the file as desired\n",
    "\n",
    "    # if you need to see which file this is, parse the path\n",
    "    print(f\"file path: {frag.path}\")\n",
    "    # let's see how much data this loaded\n",
    "    print_table_size(table=file_tbl)\n",
    "\n",
    "    # again, we'll stop after one\n",
    "    del file_tbl\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852236e8",
   "metadata": {},
   "source": [
    "This can be combined with the previous example to iterate over the files in a single partition\n",
    "(left as an exercise for the reader)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d32df",
   "metadata": {},
   "source": [
    "How big are the files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median file\n",
    "median_file_frag = [\n",
    "    frag for frag in neowise_ds.get_fragments() if frag.path == median_file\n",
    "][0]\n",
    "median_file_tbl = median_file_frag.to_table(columns=COLUMN_SUBSET)\n",
    "print_table_size(table=median_file_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05553034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest file\n",
    "biggest_file_frag = [\n",
    "    frag for frag in neowise_ds.get_fragments() if frag.path == biggest_file\n",
    "][0]\n",
    "biggest_file_tbl = biggest_file_frag.to_table(columns=COLUMN_SUBSET)\n",
    "print_table_size(table=biggest_file_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92433958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "del median_file_tbl\n",
    "del biggest_file_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de797406",
   "metadata": {},
   "source": [
    "## 6. Example: Slice by year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35699a",
   "metadata": {},
   "source": [
    "If you want to handle the years independently you can work with the per-year datasets.\n",
    "We actually created these \"slices\" in the Setup section with `year_datasets`, and that\n",
    "same list is now accessible in `neowise_ds.children` used below.\n",
    "Any of the techniques shown in this notebook or those listed under \"See also\" can also\n",
    "be applied to the per-year datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice by year and iterate. zip with YEARS so that we know which slice this is.\n",
    "for year, year_ds in zip(YEARS, neowise_ds.children):\n",
    "    # insert your code here to process year_ds as desired.\n",
    "    # filter and load, iterate over partitions or files, etc.\n",
    "\n",
    "    # we'll just look at some basic metadata.\n",
    "    num_rows = sum(frag.metadata.num_rows for frag in year_ds.get_fragments())\n",
    "    num_files = len(year_ds.files)\n",
    "    print(f\"NEOWISE {year} dataset: {num_rows:,} rows in {num_files:,} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbb319",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517d180",
   "metadata": {},
   "source": [
    "### A.1 Considerations when extending to specific use cases\n",
    "\n",
    "Because the catalog is so large, you will need to carefully consider your specific problem and\n",
    "determine how to slice and filter the data most efficiently.\n",
    "There is no one right answer; it will depend on the use case.\n",
    "\n",
    "#### A.1.1 Filtering\n",
    "\n",
    "Filter out as much data as possible as early as possible. Ideas to consider are:\n",
    "\n",
    "1. With the Parquet file format, you can apply filters during the read to avoid loading\n",
    "   rows that you don't need.\n",
    "      - Pandas (not demonstrated here) supports basic filters.\n",
    "      - PyArrow (demonstrated here) also supports complex filters which allow you to compare\n",
    "        values between columns and/or construct new columns on the fly (e.g., subtracting\n",
    "        magnitude columns to construct a new color column, as done in the AllWISE notebook).\n",
    "2. Queries (i.e., loading data by applying filters) will be *much* more efficient when they\n",
    "   include a filter on the partitioning column ('healpix_k5'; demonstrated above).\n",
    "      - Notice both that this is essentially equivalent to slicing by partition and that\n",
    "        you can filter for more than one partition at a time.\n",
    "      - This is highly recommended even if your use case doesn't explicitly care about it.\n",
    "        Exceptions include situations where you're working with individual files and when\n",
    "        it's impractical or counterproductive for the science.\n",
    "3. You should also include filters specific to your use case if possible.\n",
    "4. Exceptions: Sometimes it's not easy to write a dataset filter for the query.\n",
    "  A cone search is a common example.\n",
    "  In principal it could be written as a PyArrow dataset filter, but in practice the correct\n",
    "  formula is much too complicated.\n",
    "  In this case, it's easier to write dataset filters for broad RA and Dec limits and then\n",
    "  do the actual cone search using `astropy`.\n",
    "  This approach is still quite performant (see the NEOWISE light curves notebook).\n",
    "\n",
    "#### A.1.2 Slicing\n",
    "\n",
    "Slice the dataset in some way(s), then iterate and/or parallelize over the slices. Ideas to consider are:\n",
    "\n",
    "1. Choose your slices so that you can:\n",
    "      - Run your processing code on one slice independently. For example, if your code must\n",
    "        see all the data for some target object (RA and Dec) at the same time, you may\n",
    "        slice the dataset by partition, but don't slice it by year.\n",
    "      - Load all data in the slice into memory at once (after applying your filters during the read).\n",
    "        This notebook shows how to determine how big a slice of data is in order to guide this decision.\n",
    "2. By default, slice by partition. If this is too much data, you may also want to slice\n",
    "   by year and/or file.\n",
    "3. If you have enough memory to load more than one slice simultaneously, parallelize over\n",
    "   the slices to speed up your code.\n",
    "\n",
    "### A.2 Inspect dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631d755",
   "metadata": {},
   "source": [
    "When deciding how to slice and filter the dataset, it can be useful to understand\n",
    "dataset statistics like partition and file sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_index_from_path(path, k_column=KCOLUMN):\n",
    "    \"\"\"Parse the path and return the partition pixel index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path to parse.\n",
    "    k_column : str (optional)\n",
    "        Name of the partitioning column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition pixel index parsed from the path.\n",
    "    \"\"\"\n",
    "    pattern = rf\"({k_column}=)([0-9]+)\"  # matches strings like \"healpix_k5=1124\"\n",
    "    return int(re.search(pattern, path).group(2))  # pixel index, e.g., 1124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6896d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some file statistics to a dataframe\n",
    "file_stats = pd.DataFrame(\n",
    "    columns=[\"path\", KCOLUMN, \"numrows\"],\n",
    "    data=[\n",
    "        (frag.path, pixel_index_from_path(frag.path), frag.metadata.num_rows)\n",
    "        for frag in neowise_ds.get_fragments()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddbc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e700fb8",
   "metadata": {},
   "source": [
    "#### A.2.1 Dataset statistics per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3187e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of file sizes (number of rows)\n",
    "ax = file_stats.numrows.hist(log=True)\n",
    "ax.set_xlabel(\"Number of rows\")\n",
    "ax.set_ylabel(\"Number of files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e81746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest file\n",
    "file_stats.loc[file_stats.numrows == file_stats.numrows.max()].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median file\n",
    "file_stats.sort_values(\"numrows\").iloc[len(file_stats.index) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaba41",
   "metadata": {},
   "source": [
    "#### A.2.2 Dataset statistics per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats per partition\n",
    "k_groups = file_stats[[KCOLUMN, \"numrows\"]].groupby(KCOLUMN)\n",
    "per_part = k_groups.sum()\n",
    "per_part[\"numfiles\"] = k_groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_part.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_part.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize number of rows per partition\n",
    "per_part.numrows.plot(\n",
    "    logy=True, xlabel=f\"{KCOLUMN} pixel index\", ylabel=\"Number of rows per partition\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest partition\n",
    "per_part.loc[per_part.numrows == per_part.numrows.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdaf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median partition\n",
    "per_part.sort_values(\"numrows\").iloc[len(per_part.index) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01b461",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "**Author:** Troy Raen (IRSA Developer) and the IPAC Science Platform team\n",
    "\n",
    "**Updated:** 2025-03-07\n",
    "\n",
    "**Contact:** [the IRSA Helpdesk](https://irsa.ipac.caltech.edu/docs/help_desk.html) with questions or reporting problems."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "science_demo",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   17,
   21,
   36,
   40,
   67,
   90,
   108,
   116,
   120,
   125,
   134,
   140,
   145,
   152,
   156,
   165,
   169,
   191,
   195,
   203,
   207,
   223,
   227,
   232,
   245,
   254,
   258,
   264,
   283,
   289,
   293,
   299,
   307,
   319,
   323,
   327,
   334,
   340,
   356,
   361,
   365,
   374,
   383,
   387,
   391,
   399,
   409,
   413,
   464,
   469,
   489,
   500,
   504,
   506,
   510,
   517,
   522,
   525,
   529,
   536,
   540,
   544,
   551,
   556,
   559
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}