{"version":2,"kind":"Notebook","sha256":"cacce51f714e41cc3f302b0723510b08f0f12758cd82f711995a58fd8b1a8acb","slug":"roman-hlss-number-density","location":"/tutorials/roman_simulations/roman_hlss_number_density.md","dependencies":[],"frontmatter":{"title":"Number Density as a Function of Redshift","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"jupytext":{"formats":"md:myst","text_representation":{"extension":".md","format_name":"myst","format_version":"0.13","jupytext_version":"1.15.2"}},"content_includes_title":false,"authors":[{"id":"IRSA Scientists and Developers","name":"IRSA Scientists and Developers"}],"github":"https://github.com/Caltech-IPAC/irsa-tutorials/","subject":"IRSA Tutorials","keywords":["astronomy"],"settings":{"output_matplotlib_strings":"remove"},"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/Caltech-IPAC/irsa-tutorials//blob/main/tutorials/roman_simulations/roman_hlss_number_density.md","exports":[{"format":"md","filename":"roman_hlss_number_density.md","url":"/irsa-tutorials/build/roman_hlss_number_de-acba9b8b39cfeb4247f9dd608965423a.md"}]},"mdast":{"type":"root","children":[{"type":"block","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Learning Goals","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"hrWMlPgPZw"}],"identifier":"learning-goals","label":"Learning Goals","html_id":"learning-goals","implicit":true,"key":"ELCiP7ud6j"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":20,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"learn how to use the mock simulations for Roman Space Telescope ","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"YLvYFMTkCp"},{"type":"link","url":"https://ui.adsabs.harvard.edu/abs/2021MNRAS.501.3490Z/abstract","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Zhai et al. 2021","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"DSB6m3R6pY"}],"urlSource":"https://ui.adsabs.harvard.edu/abs/2021MNRAS.501.3490Z/abstract","key":"VcoBJbAC1V"}],"key":"Je9ZjUuCGd"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"modify code to reduce memory usage when using large catalogs","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ZphVNMSSwu"}],"key":"UDliXxXIQX"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"make a plot of number density of galaxies as a function of redshift (recreate figure 3 from ","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"k4WAXB6AgQ"},{"type":"link","url":"https://arxiv.org/pdf/2110.01829","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Wang et al., 2021","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"z0RGu4gR4W"}],"urlSource":"https://arxiv.org/pdf/2110.01829","key":"NsvkiWdBo9"},{"type":"text","value":").","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"ICjqHEGZDR"}],"key":"hZftrbqnUz"}],"key":"QPZ02TWZet"},{"type":"heading","depth":2,"position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Introduction","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"D9aQhDZSY9"}],"identifier":"introduction","label":"Introduction","html_id":"introduction","implicit":true,"key":"AbVrvZnhw2"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"The Nancy Grace Roman Space Telescope will be a transformative tool for cosmology due to its unprecedented combination of wide field-of-view and high-resolution imaging. We choose to demonstrate the usefulness of the mock Roman suimulations in generating number density plots because they provide crucial insights into the large-scale structure of the universe and the distribution of matter over cosmic time. These plots illustrate how the number density of galaxies, quasars, or dark matter halos changes with redshift, revealing information about the processes of galaxy formation, evolution, and clustering. By analyzing number density plots, cosmologists can test theoretical models of structure formation, constrain cosmological parameters, and understand the influence of dark matter and dark energy on the evolution of the universe.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"LlAxGfsNXb"}],"key":"yVCtfTA4Et"},{"type":"heading","depth":3,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Instructions","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"EzwqdhsAYZ"}],"identifier":"instructions","label":"Instructions","html_id":"instructions","implicit":true,"key":"g6gl94j2zE"},{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Most of the notebook can be run with a single mock galaxy catalog file which easily fits in memory (3G). The final section uses all the data to make a final number density plot.  The dataset in its entirety does not fit into memory all at once(out of memory) so we have carefully designed this function to use the least possible amount of memory.  The dataset in its entirety also takes ~30 minutes to download and requires ~30G of space to store.  We suggest only running download_simulations with download_all= True if you are ready to wait for the download and have enough storage space.  Default is to work with only one catalog which is 10% of the dataset.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"wADUA4AVNB"}],"key":"cLpa6IZAo8"},{"type":"paragraph","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Techniques employed for working with out of memory datasets:","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"RlEVxRXuAp"}],"key":"sAmppaDmzc"},{"type":"code","lang":"","value":"- read in only some columns from the original dataset\n- convert data from float64 to float32 where high precision is not necessary\n- careful management of the code structure to not need to read in all datasets to one dataframe","position":{"start":{"line":32,"column":1},"end":{"line":34,"column":1}},"key":"Zb5E4sPB89"},{"type":"heading","depth":3,"position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"Input","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"bYTx7lw0Yw"}],"identifier":"input","label":"Input","html_id":"input","implicit":true,"key":"s3Xk8qcgqQ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":37,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":37,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"None, the location of the simulated dataset is hardcoded for convenience","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"btJFEdfANY"}],"key":"oT5RE9lMne"}],"key":"EXxfa8YdGf"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Output","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"CI1HA4eNkb"}],"identifier":"output","label":"Output","html_id":"output","implicit":true,"key":"RIy7hiqH1S"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":40,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"plots of number of galaxies per square degree as a function of redshift","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"pwczX831ho"}],"key":"SF2GxmJnja"},{"type":"listItem","spread":true,"position":{"start":{"line":41,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"optional numpy file with the counts saved for ease of working with the information later","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"ISFCoYXHcF"}],"key":"v9PFzZhuhg"}],"key":"AVXZLIDoKH"}],"key":"kja7hqiggu"},{"type":"block","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"Imports","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"mWfF38rlb1"}],"identifier":"imports","label":"Imports","html_id":"imports","implicit":true,"key":"rObZL0pIfC"}],"key":"VdAcBhit1f"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Uncomment the next line to install dependencies if needed.\n# !pip install h5py pandas matplotlib numpy requests","key":"QwgxvuVAgc"},{"type":"output","id":"-riBgRDh4NgoaIbjK5Mzr","data":[],"key":"F5eHmM2wQ5"}],"key":"npLC0k0JFH"},{"type":"block","children":[],"key":"aldJ6q7ZbD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport concurrent.futures\nimport re\nimport h5py\nimport requests\n\n%matplotlib inline","key":"eDxmaqwA0M"},{"type":"output","id":"dactQql6bqzNPSc3epaRJ","data":[],"key":"yEYRQMXkhY"}],"key":"MXH0lyEtW3"},{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"children":[{"type":"text","value":"1. Read in the mock catalogs from IRSA","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"yBbyH9nHU7"}],"identifier":"id-1-read-in-the-mock-catalogs-from-irsa","label":"1. Read in the mock catalogs from IRSA","html_id":"id-1-read-in-the-mock-catalogs-from-irsa","implicit":true,"key":"ADKgcsNV0v"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":66,"column":1},"end":{"line":70,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"text","value":"figure out how to get the files from IRSA","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"cfIo3vQMY9"}],"key":"ckjmSkXkRS"},{"type":"listItem","spread":true,"position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"pull down one of them for testing","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"FjNT5dRb2B"}],"key":"vp4NHhG7KQ"},{"type":"listItem","spread":true,"position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"convert to a pandas df for ease of use","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"iAf7iPQ9xK"}],"key":"gxeKna1Y1w"},{"type":"listItem","spread":true,"position":{"start":{"line":69,"column":1},"end":{"line":70,"column":1}},"children":[{"type":"text","value":"do some data exploration","position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"key":"duQoexuGsE"}],"key":"PzoN1ENyZZ"}],"key":"G4tZ9n7Ki3"}],"key":"kAEDapiqh2"},{"type":"block","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"children":[{"type":"text","value":"1.1 Download the files from IRSA","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"key":"JD0hhn5y4s"}],"identifier":"id-1-1-download-the-files-from-irsa","label":"1.1 Download the files from IRSA","html_id":"id-1-1-download-the-files-from-irsa","implicit":true,"key":"stN6pwoV9w"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":74,"column":1},"end":{"line":75,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":74,"column":1},"end":{"line":75,"column":1}},"children":[{"type":"text","value":"This is about 30G of data in total","position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"key":"rlsH1UMz36"}],"key":"TMxC8FIHWb"}],"key":"U6dA4b0U88"}],"key":"St92fpS6Xw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def download_file(file_url, save_path):\n    \"\"\"\n    Downloads a file from a given URL and saves it to a specified local path.\n    The file is downloaded in chunks to reduce memory usage.\n\n    Parameters:\n    - file_url (str): The URL of the file to download.\n    - save_path (str): The local path where the file will be saved.\n\n    Returns:\n    - str: The save_path if the download succeeds.\n\n    Raises:\n    - requests.HTTPError: If the HTTP request returned an unsuccessful status code.\n    - OSError: If there is an issue creating directories or writing the file.\n    \"\"\"\n    response = requests.get(file_url, stream=True)\n    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx, 5xx)\n\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):  # 8KB chunks\n            if chunk:  # Filter out keep-alive chunks\n                f.write(chunk)\n\n    print(f\"Downloaded: {save_path}\")\n    return save_path\n\n\ndef download_files_in_parallel(file_url_list, save_dir):\n    \"\"\"\n    Downloads multiple files in parallel using a ThreadPoolExecutor.\n\n    Parameters:\n    - file_url_list (list of str): List of file URLs to download.\n    - save_dir (str): Directory where files will be saved.\n\n    Returns:\n    - list of str: List of paths where the files were successfully downloaded.\n    \"\"\"\n    downloaded_files = []\n    #set to a reasonable number for efficiency and scalability\n    max_workers = min(len(file_url_list), os.cpu_count() * 5, 32)  \n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_url = {\n            executor.submit(download_file, url, os.path.join(save_dir, os.path.basename(url))): url \n            for url in file_url_list\n        }\n        \n        for future in concurrent.futures.as_completed(future_to_url):\n            file_path = future.result()\n            if file_path:\n                downloaded_files.append(file_path)\n\n    return downloaded_files\n\ndef download_simulations(download_dir, download_all=True):\n    \"\"\"\n    Download .hdf5 simulation files from the Roman Zhai2021 URL using the provided checksums.md5 file.\n\n    This function reads the list of available .hdf5 files from the checksums.md5 file, filters them,\n    and downloads the requested files to a specified directory. If a file already exists locally,\n    it will be skipped.\n\n    Parameters\n    ----------\n    download_dir : str\n        Directory where the downloaded .hdf5 files will be stored. \n        The directory will be created if it does not already exist.\n\n    download_all : bool, optional\n        If True, download all .hdf5 files listed in the checksum file.\n        If False, download only the first file. Default is True.\n\n    Returns\n    -------\n    None\n        This function does not return any value but prints the download status.\n\n    Notes\n    -----\n    - Downloads are performed in parallel using a maximum of 4 workers.\n    - Files that already exist locally are skipped.\n    \"\"\"\n    base_url = 'https://irsa.ipac.caltech.edu/data/theory/Roman/Zhai2021'\n    checksum_url = f'{base_url}/checksums.md5'\n\n    # Read the checksums file to get the filenames\n    df = pd.read_csv(checksum_url, sep=r\"\\s+\", names=[\"checksum\", \"filename\"])\n    hdf5_files = df[df[\"filename\"].str.endswith(\".hdf5\")][\"filename\"].tolist()\n\n    if not download_all:\n        hdf5_files = hdf5_files[:1]\n\n    file_urls = [f\"{base_url}/{fname}\" for fname in hdf5_files]\n\n    os.makedirs(download_dir, exist_ok=True)\n\n    # Filter out files that already exist\n    files_to_download = []\n    for file_url in file_urls:\n        filename = os.path.basename(file_url)\n        file_path = os.path.join(download_dir, filename)\n        if os.path.exists(file_path):\n            print(f\"The file {filename} already exists. Skipping download.\")\n        else:\n            files_to_download.append(file_url)\n\n    if files_to_download:\n        print(f\"Starting parallel download of {len(files_to_download)} files.\")\n        downloaded_files = download_files_in_parallel(files_to_download, download_dir)\n        print(f\"Successfully downloaded {len(downloaded_files)} files.\")\n    else:\n        print(\"All requested files are already downloaded.\")","key":"rj2POAdIsB"},{"type":"output","id":"FmaDEfEOCcmILQ2-HWSXp","data":[],"key":"BAeVatGVNR"}],"key":"EDwkSRuBcd"},{"type":"block","children":[],"key":"NGiYD7QE3q"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Set download_all to True to download all files, or False to download only the first file\n# only a single downloaded file is required to run the notebook in its entirety, \n# however section 4 will be more accuate with all 10 files\n\n# Downloading 10 files, each 3GB in size, may take between 1 and 30 minutes,\n# or even longer, depending on your proximity to the data.\n\ndownload_dir = 'downloaded_hdf5_files'\ndownload_simulations(download_dir, download_all=False)","key":"ErcJx4ayqu"},{"type":"output","id":"nwx9gpdcwTak95xPnx6J1","data":[{"output_type":"stream","name":"stdout","text":"Starting parallel download of 1 files.\n"},{"output_type":"stream","name":"stdout","text":"Downloaded: downloaded_hdf5_files/Roman_small_V2_0.hdf5\nSuccessfully downloaded 1 files.\n"}],"key":"OUixfBrGLz"}],"key":"BhfNWpHKdG"},{"type":"block","children":[{"type":"heading","depth":3,"position":{"start":{"line":205,"column":1},"end":{"line":205,"column":1}},"children":[{"type":"text","value":"1.2 read files into a pandas dataframe so we can work with them","position":{"start":{"line":205,"column":1},"end":{"line":205,"column":1}},"key":"uSgSaSPhV7"}],"identifier":"id-1-2-read-files-into-a-pandas-dataframe-so-we-can-work-with-them","label":"1.2 read files into a pandas dataframe so we can work with them","html_id":"id-1-2-read-files-into-a-pandas-dataframe-so-we-can-work-with-them","implicit":true,"key":"g06IedYlnR"}],"key":"jhtbUywhVR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def fetch_column_names():\n    \"\"\"\n    Fetches column names from the Readme_small.txt file at the specified URL.\n\n    Returns:\n    - List of column names (str): A list of extracted column names, or an empty list if no column names are found.\n\n    Raises:\n    - HTTPError: If the HTTP request returns an unsuccessful status code.\n    \"\"\"\n    import requests\n    import re\n\n    url = 'https://irsa.ipac.caltech.edu/data/theory/Roman/Zhai2021/Readme_small.txt'\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an HTTPError for bad responses\n\n    lines = response.text.splitlines()\n    column_names = []\n\n    for line in lines:\n        if line.strip().lower().startswith('column'):\n            match = re.search(r':\\s*([^\\s]+)', line)\n            if match:\n                column_names.append(match.group(1))\n\n    if column_names:\n        return column_names\n    else:\n        print(\"No column names found in Readme_small.txt.\")\n        return []\n\n\n\ndef read_hdf5_to_pandas(file_path, columns_to_keep, columns_to_convert):\n    \"\"\"\n    Read selected columns from an HDF5 file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the HDF5 file.\n    columns_to_keep : list of str\n        List of column names to extract from the dataset.\n    columns_to_convert : list of str\n        List of column names to convert to float32 for memory efficiency.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DataFrame containing the selected columns from the HDF5 file, \n        with specified columns cast to float32.\n    \"\"\"\n    # Map of column names to their corresponding index in the HDF5 dataset\n    col_map = {\n        \"RA\": 0,\n        \"DEC\": 1,\n        \"redshift_cosmological\": 2,\n        \"redshift_observed\": 3,\n        \"velocity\": 4,\n        \"stellar\": 5,\n        \"SFR\": 6,\n        \"halo\": 7,\n        \"flux_Halpha6563\": 8,\n        \"flux_OIII5007\": 9,\n        \"M_F158_Av1.6523\": 10,\n        \"nodeIsIsolated\": 11\n    }\n\n    # Open the HDF5 file and extract only the requested columns\n    with h5py.File(file_path, \"r\") as file:\n        dataset = file[\"data\"]\n        data = {}\n        for col in columns_to_keep:\n            idx = col_map[col]\n            data[col] = dataset[:, idx]\n\n    # Create a DataFrame from the selected columns\n    df = pd.DataFrame(data)\n\n    # Convert specified columns to float32 to reduce memory usage\n    conversion_map = {col: \"float32\" for col in columns_to_convert}\n    df = df.astype(conversion_map)\n\n    return df\n\ndef assign_redshift_bins(df, z_min, z_max, dz=0.1):\n    \"\"\"\n    Assign redshift bins to a DataFrame based on 'redshift_observed'.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The input DataFrame containing a 'redshift_observed' column.\n    z_min : float\n        The minimum redshift for binning.\n    z_max : float\n        The maximum redshift for binning.\n    dz : float, optional\n        The redshift bin width (default is 0.1).\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The input DataFrame with a new 'z_bin' categorical column.\n    z_bins : numpy.ndarray\n        The redshift bin edges used to define bins.\n    z_bin_centers : list of float\n        The central redshift value for each bin.\n    \"\"\"\n    z_bins = np.arange(z_min, z_max + dz, dz)\n    df[\"z_bin\"] = pd.cut(df[\"redshift_observed\"], bins=z_bins, include_lowest=True, right=False)\n    z_bin_centers = [np.mean([b.left, b.right]) for b in df[\"z_bin\"].cat.categories]\n    z_bin_centers = sorted(z_bin_centers)\n    return df, z_bins, z_bin_centers","key":"NEFoSigFwx"},{"type":"output","id":"2Tw3eL-P6S9tyeDF-tZrr","data":[],"key":"NJrCbRQsTb"}],"key":"EDVigx16G9"},{"type":"block","children":[],"key":"NRA12SLSEV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#don't change this, the above code downloads the files to this directory\nfile_path = 'downloaded_hdf5_files/Roman_small_V2_0.hdf5'\n\n#out of consideration for the size of these files and the amount of memory required to work with them\n# we only keep the columns that we are going to use and convert some to 32bit instead of 64 where \n# the higher precision is not necessary to the science\ncolumns_to_keep = ['RA', 'DEC', 'redshift_observed', 'flux_Halpha6563']\ncolumns_to_convert = ['redshift_observed', 'flux_Halpha6563']\n\ndf = read_hdf5_to_pandas(file_path, columns_to_keep, columns_to_convert)","key":"hsNWK3Mgq5"},{"type":"output","id":"QZeGQRGmGF5l1W3urQADW","data":[],"key":"MfUgu2w7x7"}],"key":"Lqecg38uN1"},{"type":"block","children":[{"type":"heading","depth":3,"position":{"start":{"line":338,"column":1},"end":{"line":338,"column":1}},"children":[{"type":"text","value":"1.3 Pre-process dataset","position":{"start":{"line":338,"column":1},"end":{"line":338,"column":1}},"key":"Wt1nUA8xdi"}],"identifier":"id-1-3-pre-process-dataset","label":"1.3 Pre-process dataset","html_id":"id-1-3-pre-process-dataset","implicit":true,"key":"Oq5unCfMNH"},{"type":"paragraph","position":{"start":{"line":339,"column":1},"end":{"line":339,"column":1}},"children":[{"type":"text","value":"Add some value-added columns to the dataframe, and calculate some variables which are used by mulitple functions","position":{"start":{"line":339,"column":1},"end":{"line":339,"column":1}},"key":"jjS4D9dqke"}],"key":"vONZc9V6pv"}],"key":"KBIUb6I0S9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#add a binned redshift column and calculate the edges of the bins\nz_min = 1.0\nz_max = 3.0\ndz = 0.1\ndf, z_bins, z_bin_centers = assign_redshift_bins(df, z_min=z_min, z_max=z_max, dz=dz)\n\n#Shift RA values to continuous range for the entire DataFrame \n#This is necessary because the current range goes over 360 which appears to the code \n#as a non-contiguous section of the sky.  By shifting the RA values, we don't have the \n#jump from RA = 360 to RA = 0.  \n    \nra_min1=330\ndf['RA_shifted'] = (df['RA'] - ra_min1) % 360  # Shift RA to continuous range","key":"aq1cuBQAxy"},{"type":"output","id":"kpcAJq2ybBKOhaLStwttY","data":[],"key":"BgmiuuRtv9"}],"key":"H311akqJ8x"},{"type":"block","children":[],"key":"JktJqp4ERU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#quick check to see what we've got:\ndf","key":"wKlDi9DNar"},{"type":"output","id":"0tsKMuETjRFO4vxWhThAp","data":[{"output_type":"execute_result","execution_count":8,"metadata":{},"data":{"text/plain":{"content":"                  RA        DEC  redshift_observed  flux_Halpha6563 z_bin  \\\n0         341.509710   7.693022           4.090425     2.438628e-16   NaN   \n1         357.329659  -4.205828           4.200562     2.290115e-16   NaN   \n2         348.470261  19.136800           3.704146     3.427580e-16   NaN   \n3           5.120387   6.377602           3.750228     3.328315e-16   NaN   \n4          21.632080   2.866344           3.677756     3.486357e-16   NaN   \n...              ...        ...                ...              ...   ...   \n29382938  338.240811  16.178767           0.067884     1.876826e-15   NaN   \n29382939  346.041731   4.151812           0.059724     2.956311e-15   NaN   \n29382940   13.633021 -18.561325           0.080929     4.737627e-14   NaN   \n29382941    4.662985   4.162972           0.050005     4.001319e-15   NaN   \n29382942   20.088851  18.854207           0.030603     2.113698e-14   NaN   \n\n          RA_shifted  \n0          11.509710  \n1          27.329659  \n2          18.470261  \n3          35.120387  \n4          51.632080  \n...              ...  \n29382938    8.240811  \n29382939   16.041731  \n29382940   43.633021  \n29382941   34.662985  \n29382942   50.088851  \n\n[29382943 rows x 6 columns]","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RA</th>\n      <th>DEC</th>\n      <th>redshift_observed</th>\n      <th>flux_Halpha6563</th>\n      <th>z_bin</th>\n      <th>RA_shifted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>341.509710</td>\n      <td>7.693022</td>\n      <td>4.090425</td>\n      <td>2.438628e-16</td>\n      <td>NaN</td>\n      <td>11.509710</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>357.329659</td>\n      <td>-4.205828</td>\n      <td>4.200562</td>\n      <td>2.290115e-16</td>\n      <td>NaN</td>\n      <td>27.329659</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>348.470261</td>\n      <td>19.136800</td>\n      <td>3.704146</td>\n      <td>3.427580e-16</td>\n      <td>NaN</td>\n      <td>18.470261</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.120387</td>\n      <td>6.377602</td>\n      <td>3.750228</td>\n      <td>3.328315e-16</td>\n      <td>NaN</td>\n      <td>35.120387</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21.632080</td>\n      <td>2.866344</td>\n      <td>3.677756</td>\n      <td>3.486357e-16</td>\n      <td>NaN</td>\n      <td>51.632080</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29382938</th>\n      <td>338.240811</td>\n      <td>16.178767</td>\n      <td>0.067884</td>\n      <td>1.876826e-15</td>\n      <td>NaN</td>\n      <td>8.240811</td>\n    </tr>\n    <tr>\n      <th>29382939</th>\n      <td>346.041731</td>\n      <td>4.151812</td>\n      <td>0.059724</td>\n      <td>2.956311e-15</td>\n      <td>NaN</td>\n      <td>16.041731</td>\n    </tr>\n    <tr>\n      <th>29382940</th>\n      <td>13.633021</td>\n      <td>-18.561325</td>\n      <td>0.080929</td>\n      <td>4.737627e-14</td>\n      <td>NaN</td>\n      <td>43.633021</td>\n    </tr>\n    <tr>\n      <th>29382941</th>\n      <td>4.662985</td>\n      <td>4.162972</td>\n      <td>0.050005</td>\n      <td>4.001319e-15</td>\n      <td>NaN</td>\n      <td>34.662985</td>\n    </tr>\n    <tr>\n      <th>29382942</th>\n      <td>20.088851</td>\n      <td>18.854207</td>\n      <td>0.030603</td>\n      <td>2.113698e-14</td>\n      <td>NaN</td>\n      <td>50.088851</td>\n    </tr>\n  </tbody>\n</table>\n<p>29382943 rows × 6 columns</p>\n</div>","content_type":"text/html"}}}],"key":"uqmt2VOjpJ"}],"key":"yJnVfc9BTO"},{"type":"block","children":[{"type":"heading","depth":3,"position":{"start":{"line":362,"column":1},"end":{"line":362,"column":1}},"children":[{"type":"text","value":"1.3 Data Exploration","position":{"start":{"line":362,"column":1},"end":{"line":362,"column":1}},"key":"oYXfKOGGHs"}],"identifier":"id-1-3-data-exploration","label":"1.3 Data Exploration","html_id":"id-1-3-data-exploration","implicit":true,"key":"mvJIvR4VdE"},{"type":"paragraph","position":{"start":{"line":363,"column":1},"end":{"line":363,"column":1}},"children":[{"type":"text","value":"Let’s explore the dataset a bit to see what we are working with.","position":{"start":{"line":363,"column":1},"end":{"line":363,"column":1}},"key":"XAIJ3XFg9P"}],"key":"NiWqGKZCPJ"},{"type":"paragraph","position":{"start":{"line":365,"column":1},"end":{"line":365,"column":1}},"children":[{"type":"text","value":"This function prints the following information:","position":{"start":{"line":365,"column":1},"end":{"line":365,"column":1}},"key":"OJc87Oj9a4"}],"key":"KAAdvrmg8T"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":366,"column":1},"end":{"line":372,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":366,"column":1},"end":{"line":366,"column":1}},"children":[{"type":"text","value":"First 5 rows of the DataFrame.","position":{"start":{"line":366,"column":1},"end":{"line":366,"column":1}},"key":"GIlZjg1UgM"}],"key":"JOKwiTRqvO"},{"type":"listItem","spread":true,"position":{"start":{"line":367,"column":1},"end":{"line":367,"column":1}},"children":[{"type":"text","value":"DataFrame info (data types, non-null counts).","position":{"start":{"line":367,"column":1},"end":{"line":367,"column":1}},"key":"z7pHaf1lGj"}],"key":"zp58IJUqWP"},{"type":"listItem","spread":true,"position":{"start":{"line":368,"column":1},"end":{"line":368,"column":1}},"children":[{"type":"text","value":"Summary statistics for numeric columns.","position":{"start":{"line":368,"column":1},"end":{"line":368,"column":1}},"key":"JKYDt1oo3r"}],"key":"DjJPV9QWpU"},{"type":"listItem","spread":true,"position":{"start":{"line":369,"column":1},"end":{"line":369,"column":1}},"children":[{"type":"text","value":"Memory usage of the DataFrame and each column.","position":{"start":{"line":369,"column":1},"end":{"line":369,"column":1}},"key":"QT9R7troVg"}],"key":"vFcYwOI1n6"},{"type":"listItem","spread":true,"position":{"start":{"line":370,"column":1},"end":{"line":370,"column":1}},"children":[{"type":"text","value":"Column names.","position":{"start":{"line":370,"column":1},"end":{"line":370,"column":1}},"key":"DNcRjSFzw1"}],"key":"F5UXIYpIZx"},{"type":"listItem","spread":true,"position":{"start":{"line":371,"column":1},"end":{"line":372,"column":1}},"children":[{"type":"text","value":"Check for missing (NaN) values, including columns with NaNs and their counts.","position":{"start":{"line":371,"column":1},"end":{"line":371,"column":1}},"key":"Pyrx2cCruD"}],"key":"fTBuqIrn7G"}],"key":"MJkZxBTYqY"},{"type":"paragraph","position":{"start":{"line":373,"column":1},"end":{"line":373,"column":1}},"children":[{"type":"text","value":"Note that quite a few of the z_bin values are NaN because they are outside the redshift range of interest for this plot (1 < z < 3)","position":{"start":{"line":373,"column":1},"end":{"line":373,"column":1}},"key":"kgQX986ML5"}],"key":"CxmJvzukY6"}],"key":"bPSL5rlB15"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Display the first 5 rows of the dataframe\ndf.head()","key":"lQlPzzph56"},{"type":"output","id":"BWX469sYFMX8eeaX63OQl","data":[{"output_type":"execute_result","execution_count":9,"metadata":{},"data":{"text/plain":{"content":"           RA        DEC  redshift_observed  flux_Halpha6563 z_bin  RA_shifted\n0  341.509710   7.693022           4.090425     2.438628e-16   NaN   11.509710\n1  357.329659  -4.205828           4.200562     2.290115e-16   NaN   27.329659\n2  348.470261  19.136800           3.704146     3.427580e-16   NaN   18.470261\n3    5.120387   6.377602           3.750228     3.328315e-16   NaN   35.120387\n4   21.632080   2.866344           3.677756     3.486357e-16   NaN   51.632080","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RA</th>\n      <th>DEC</th>\n      <th>redshift_observed</th>\n      <th>flux_Halpha6563</th>\n      <th>z_bin</th>\n      <th>RA_shifted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>341.509710</td>\n      <td>7.693022</td>\n      <td>4.090425</td>\n      <td>2.438628e-16</td>\n      <td>NaN</td>\n      <td>11.509710</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>357.329659</td>\n      <td>-4.205828</td>\n      <td>4.200562</td>\n      <td>2.290115e-16</td>\n      <td>NaN</td>\n      <td>27.329659</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>348.470261</td>\n      <td>19.136800</td>\n      <td>3.704146</td>\n      <td>3.427580e-16</td>\n      <td>NaN</td>\n      <td>18.470261</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.120387</td>\n      <td>6.377602</td>\n      <td>3.750228</td>\n      <td>3.328315e-16</td>\n      <td>NaN</td>\n      <td>35.120387</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21.632080</td>\n      <td>2.866344</td>\n      <td>3.677756</td>\n      <td>3.486357e-16</td>\n      <td>NaN</td>\n      <td>51.632080</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"}}}],"key":"sAYQ2P8OWl"}],"key":"wLF0jLfDVQ"},{"type":"block","children":[],"key":"zAZxQAXOq0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Get general information about the dataframe (e.g., data types, non-null counts)\nprint(\"\\nDataFrame info (Data Types, Non-null counts):\")\ndf.info()","key":"EruAPjO9cA"},{"type":"output","id":"0K1UJkLPzJ7c116tjwIKS","data":[{"output_type":"stream","name":"stdout","text":"\nDataFrame info (Data Types, Non-null counts):\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 29382943 entries, 0 to 29382942\nData columns (total 6 columns):\n #   Column             Dtype   \n---  ------             -----   \n 0   RA                 float64 \n 1   DEC                float64 \n 2   redshift_observed  float32 \n 3   flux_Halpha6563    float32 \n 4   z_bin              category\n 5   RA_shifted         float64 \ndtypes: category(1), float32(2), float64(3)\nmemory usage: 924.7 MB\n"}],"key":"pUnmflRL5c"}],"key":"ioirH0hOgD"},{"type":"block","children":[],"key":"UaSX7tC1Ml"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Display summary statistics for numeric columns\nprint(\"\\nSummary statistics for numeric columns:\")\ndf.describe()","key":"uaTrgr7EIn"},{"type":"output","id":"BdikQRDiMepB_9ZoyBK9D","data":[{"output_type":"stream","name":"stdout","text":"\nSummary statistics for numeric columns:\n"},{"output_type":"execute_result","execution_count":11,"metadata":{},"data":{"text/plain":{"content":"                 RA           DEC  redshift_observed  flux_Halpha6563  \\\ncount  2.938294e+07  2.938294e+07       2.938294e+07     2.938294e+07   \nmean   1.802166e+02 -2.749608e-03       1.096546e+00     2.976948e-16   \nstd    1.689970e+02  1.257703e+01       5.805780e-01     4.253355e-15   \nmin    1.164220e-06 -2.249985e+01       3.921282e-03     1.660502e-18   \n25%    1.105467e+01 -1.078845e+01       6.446358e-01     5.264885e-17   \n50%    3.375331e+02  2.420867e-02       9.980963e-01     1.034983e-16   \n75%    3.489236e+02  1.077922e+01       1.473502e+00     2.202762e-16   \nmax    3.600000e+02  2.249999e+01       5.055577e+00     8.686074e-12   \n\n         RA_shifted  \ncount  2.938294e+07  \nmean   2.996636e+01  \nstd    1.287226e+01  \nmin    7.500002e+00  \n25%    1.889303e+01  \n50%    2.997017e+01  \n75%    4.102419e+01  \nmax    5.250000e+01  ","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RA</th>\n      <th>DEC</th>\n      <th>redshift_observed</th>\n      <th>flux_Halpha6563</th>\n      <th>RA_shifted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.938294e+07</td>\n      <td>2.938294e+07</td>\n      <td>2.938294e+07</td>\n      <td>2.938294e+07</td>\n      <td>2.938294e+07</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.802166e+02</td>\n      <td>-2.749608e-03</td>\n      <td>1.096546e+00</td>\n      <td>2.976948e-16</td>\n      <td>2.996636e+01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.689970e+02</td>\n      <td>1.257703e+01</td>\n      <td>5.805780e-01</td>\n      <td>4.253355e-15</td>\n      <td>1.287226e+01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.164220e-06</td>\n      <td>-2.249985e+01</td>\n      <td>3.921282e-03</td>\n      <td>1.660502e-18</td>\n      <td>7.500002e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.105467e+01</td>\n      <td>-1.078845e+01</td>\n      <td>6.446358e-01</td>\n      <td>5.264885e-17</td>\n      <td>1.889303e+01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.375331e+02</td>\n      <td>2.420867e-02</td>\n      <td>9.980963e-01</td>\n      <td>1.034983e-16</td>\n      <td>2.997017e+01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.489236e+02</td>\n      <td>1.077922e+01</td>\n      <td>1.473502e+00</td>\n      <td>2.202762e-16</td>\n      <td>4.102419e+01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.600000e+02</td>\n      <td>2.249999e+01</td>\n      <td>5.055577e+00</td>\n      <td>8.686074e-12</td>\n      <td>5.250000e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"}}}],"key":"sTPuqmlN62"}],"key":"TvhAYGcUiF"},{"type":"block","children":[],"key":"bIqJ3NJfvX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Print memory usage of the DataFrame\nprint(\"\\nMemory usage of the DataFrame:\")\nprint(df.memory_usage(deep=True))  # Shows memory usage of each column\nprint(f\"Total memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")  # Total memory usage in MB","key":"Ph8f9eG6op"},{"type":"output","id":"uU5dkFo08ePFTgwrKVOdw","data":[{"output_type":"stream","name":"stdout","text":"\nMemory usage of the DataFrame:\nIndex                      132\nRA                   235063544\nDEC                  235063544\nredshift_observed    117531772\nflux_Halpha6563      117531772\nz_bin                 29383819\nRA_shifted           235063544\ndtype: int64\nTotal memory usage: 924.72 MB\n"}],"key":"y0LYyIhDlg"}],"key":"DUrAfQiXzL"},{"type":"block","children":[],"key":"WXkXHTY16C"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Check if there are any NaN (missing) values in the DataFrame\nprint(\"\\nChecking for NaN (missing) values in the DataFrame:\")\nif df.isnull().any().any():\n    print(\"There are NaN values in the DataFrame.\")\n    print(\"\\nColumns with NaN values and the number of missing values in each:\")\n    print(df.isnull().sum())\nelse:\n    print(\"There are no NaN values in the DataFrame.\")","key":"Ooc3FTr8qD"},{"type":"output","id":"embU-O8tmMwuftQiPeOYt","data":[{"output_type":"stream","name":"stdout","text":"\nChecking for NaN (missing) values in the DataFrame:\nThere are NaN values in the DataFrame.\n\nColumns with NaN values and the number of missing values in each:\n"},{"output_type":"stream","name":"stdout","text":"RA                          0\nDEC                         0\nredshift_observed           0\nflux_Halpha6563             0\nz_bin                14781902\nRA_shifted                  0\ndtype: int64\n"}],"key":"qcJt5GXnXj"}],"key":"tKBZz38ux7"},{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":410,"column":1},"end":{"line":410,"column":1}},"children":[{"type":"text","value":"2.0 Make some helpful histograms","position":{"start":{"line":410,"column":1},"end":{"line":410,"column":1}},"key":"zs6VhUpAkP"}],"identifier":"id-2-0-make-some-helpful-histograms","label":"2.0 Make some helpful histograms","html_id":"id-2-0-make-some-helpful-histograms","implicit":true,"key":"xtuEAm0b9j"},{"type":"paragraph","position":{"start":{"line":411,"column":1},"end":{"line":411,"column":1}},"children":[{"type":"text","value":"Do the numbers we are getting and their distributions make sense?","position":{"start":{"line":411,"column":1},"end":{"line":411,"column":1}},"key":"ayt3mJxZAx"}],"key":"jwcq5ZuGXK"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":412,"column":1},"end":{"line":414,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":412,"column":1},"end":{"line":412,"column":1}},"children":[{"type":"text","value":"plot of the number of galaxies as a function of dec bin","position":{"start":{"line":412,"column":1},"end":{"line":412,"column":1}},"key":"v1kbTD66FG"}],"key":"q7QHwTanyI"},{"type":"listItem","spread":true,"position":{"start":{"line":413,"column":1},"end":{"line":414,"column":1}},"children":[{"type":"text","value":"plot the number of galaxies per square degree as a function of dec bin","position":{"start":{"line":413,"column":1},"end":{"line":413,"column":1}},"key":"h5J679qbLN"}],"key":"jXQb5iB9F0"}],"key":"ty98xXLwcP"}],"key":"KMB4d6V9IG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def get_bin_area(dec_edges, ra_width):\n    \"\"\"\n    Calculate the area of each Dec bin in square degrees, given the RA width.\n    \n    Parameters:\n    - dec_edges: The edges of the Dec bins (1D array).\n    - ra_width: The width of the RA bins in degrees.\n    \n    Returns:\n    - areas: List of areas of the Dec bins in square degrees.\n    \"\"\"\n    dec_heights = np.diff(dec_edges)  # Heights of the Dec bins\n    areas = []\n    for i in range(len(dec_edges) - 1):\n        dec_center = (dec_edges[i] + dec_edges[i+1]) / 2  # Center of the Dec bin\n        area = ra_width * dec_heights[i] * np.cos(np.radians(dec_center))\n        areas.append(area)\n\n    return np.array(areas)\n    \ndef calculate_counts_per_deg2(df):\n    \"\"\"\n    Computes galaxy number counts per square degree by binning data in declination.\n\n    This function shifts Right Ascension (RA) values into a continuous range, \n    bins galaxies based on Declination (DEC), and calculates the number of galaxies \n    per square degree for each declination bin. It assumes a fixed declination range \n    and divides it into 10 equal bins. The function also computes bin areas to normalize \n    the galaxy counts.\n\n    Parameters:\n    - df (pd.DataFrame): A Pandas DataFrame containing at least the 'RA' and 'DEC' columns.\n\n    Returns:\n    - galaxy_counts (np.ndarray): An array containing the number of galaxies in each declination bin.\n    - counts_per_deg2 (np.ndarray): An array of galaxy counts per square degree for each declination bin.\n    - dec_bins (np.ndarray): The edges of the declination bins.\n\n    \"\"\"\n    \n    # Compute RA edges based on the data\n    ra_min = df['RA_shifted'].min()\n    ra_max = df['RA_shifted'].max()\n    ra_edges = np.linspace(ra_min, ra_max, 10 + 1)\n    \n    # Set up Declination bins for constant declination binning\n    #manually fixing these here for this particular survey\n    dec_min =  -20# df['DEC'].min()\n    dec_max =  20# df['DEC'].max()\n    dec_bins = np.linspace(dec_min, dec_max, 10 + 1)  # Create declination bins\n    \n    # Calculate bin areas for constant declination bins\n    ra_width = np.diff(ra_edges)[0]  # Width of RA bins (all bins assumed equal width)\n    bin_areas = get_bin_area(dec_bins, ra_width)\n\n    df['dec_bin'] = pd.cut(df['DEC'], bins=dec_bins, include_lowest=True, right=False).astype('category')\n\n    # Count galaxies in each Declination bin\n    galaxy_counts = df.groupby('dec_bin', observed=False).size().values\n\n    # Calculate galaxy counts per square degree\n    counts_per_deg2 = galaxy_counts / bin_areas\n\n    return galaxy_counts, counts_per_deg2, dec_bins\n\ndef plot_dec_bins(dec_bins, galaxy_counts):\n    \"\"\"\n    Plots a histogram of galaxy counts across declination bins.\n\n    This function visualizes the number of galaxies in different declination (DEC) bins \n    using a bar plot. The y-axis is set to a logarithmic scale to enhance visibility of \n    variations in galaxy counts. The function assumes uniform bin sizes.\n\n    Parameters:\n    - dec_bins (np.ndarray): An array containing the edges of the declination bins.\n    - galaxy_counts (np.ndarray): An array containing the number of galaxies in each declination bin.\n\n     \"\"\"\n    # Calculate the width of each bin (assumes uniform bin sizes)\n    bin_width = dec_bins[1] - dec_bins[0]  \n    \n    \n    # Plot the histogram for the DEC column with 10 bins\n    plt.figure(figsize=(8, 6))\n    plt.bar(dec_bins[:-1], galaxy_counts, width=bin_width, color='skyblue', edgecolor='black', align='edge')\n    plt.xlabel('Declination (DEC)')\n    plt.ylabel('Number of Galaxies')\n    plt.title('Galaxy counts as a function of declination')\n    plt.yscale('log')  # Set y-axis to logarithmic scale\n\n    #plt.ylim(1e5, 1.6e5)  # Set y-axis range\n    plt.grid(False)\n    plt.show()\n    plt.close()  # Explicitly close the plot\n\n    \ndef plot_dec_bins_per_deg2(dec_bins, counts_per_deg2):\n    \"\"\"\n    Plots a histogram of galaxy number density across declination bins.\n\n    This function visualizes the number of galaxies per square degree as a function \n    of declination using a bar plot. The y-axis is set to a logarithmic scale to \n    improve visibility of variations in galaxy density. The function assumes uniform \n    bin sizes.\n\n    Parameters:\n    - dec_bins (np.ndarray): An array containing the edges of the declination bins.\n    - counts_per_deg2 (np.ndarray): An array containing the number of galaxies per square degree \n      for each declination bin.\n\n    \"\"\"    # Calculate the width of each bin (assumes uniform bin sizes)\n    bin_width = dec_bins[1] - dec_bins[0]  \n    \n    \n    # Plot the histogram for the DEC column with 10 bins\n    plt.figure(figsize=(8, 6))\n    plt.bar(dec_bins[:-1], counts_per_deg2, width=bin_width, color='skyblue', edgecolor='black', align='edge')\n    plt.xlabel('Declination (DEC)')\n    plt.ylabel('Number of galaxies per square degree')\n    plt.title('Number density as a function of declination')\n    plt.yscale('log')  # Set y-axis to logarithmic scale\n    #plt.ylim(1e5, 1.6e5)  # Set y-axis range\n    plt.grid(False)\n    plt.show()\n    plt.close()","key":"gHaeM6j96L"},{"type":"output","id":"eShAjPeenF2rsNmiRtRKp","data":[],"key":"Ly7D1zyB3U"}],"key":"byTHra0WoX"},{"type":"block","children":[],"key":"Ot6B0wgjS3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"galaxy_counts, counts_per_deg2, dec_bins = calculate_counts_per_deg2(df)\nplot_dec_bins(dec_bins, galaxy_counts)\nplot_dec_bins_per_deg2(dec_bins, counts_per_deg2)","key":"IaFbg85IxL"},{"type":"output","id":"6w1V1GRyl_erE5-FmDo4T","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 800x600 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"823baca4641eeaed0ed8c67d841036db","path":"/irsa-tutorials/build/823baca4641eeaed0ed8c67d841036db.png"}}},{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 800x600 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"fe4d4169cef9fda443720e2ca31b282e","path":"/irsa-tutorials/build/fe4d4169cef9fda443720e2ca31b282e.png"}}}],"key":"ooedKfeaGv"}],"key":"y2AxZpXmXC"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":549,"column":1},"end":{"line":549,"column":1}},"children":[{"type":"text","value":"Figure Caption: Both plots have Declination on their X-axis.  The top panel shows the total number of galaxies.  The bottom panel shows the number of galaxies per square degree.  Note that they y-axis is displayed in a log scale, and shows a quite small range.  The goal of this plot is to explore the data a bit and see what variation we have as a function of declination bins.","position":{"start":{"line":549,"column":1},"end":{"line":549,"column":1}},"key":"ItFfRvTxzG"}],"key":"GhaTA0Erbo"}],"key":"imI0bZQGPl"},{"type":"block","position":{"start":{"line":551,"column":1},"end":{"line":551,"column":1}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":553,"column":1},"end":{"line":553,"column":1}},"children":[{"type":"text","value":"3.0 Make Number density plot","position":{"start":{"line":553,"column":1},"end":{"line":553,"column":1}},"key":"MpSgf4fBhL"}],"identifier":"id-3-0-make-number-density-plot","label":"3.0 Make Number density plot","html_id":"id-3-0-make-number-density-plot","implicit":true,"key":"B1hm2CkJbR"},{"type":"paragraph","position":{"start":{"line":554,"column":1},"end":{"line":554,"column":1}},"children":[{"type":"text","value":"This is a first pass at making a number density plot with just a single data file.  We divide the sample into 10 declination bins in order to do a jackknife sampling and obtain error bars for the plot. We choose to use constant dec bins instead of constant RA bins since area is function of cosine dec so we don’t want to average over too much dec.","position":{"start":{"line":554,"column":1},"end":{"line":554,"column":1}},"key":"G6WSVXk6nI"}],"key":"euAEgib3PF"}],"key":"tnnsdNaUHA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def jackknife_galaxy_count(df, grid_cells=10):\n    \"\"\"\n    Calculate mean galaxy counts per square degree for each redshift bin, using jackknife resampling to estimate uncertainties.\n\n    Parameters:\n    df : pandas.DataFrame\n        The DataFrame containing galaxy data, including 'RA', 'DEC', and 'redshift_observed' columns.\n    grid_cells : int, optional\n        Number of bins to divide the declination range into for jackknife resampling (default: 10).\n    \n    Returns:\n    - mean_counts: List of mean galaxy counts per square degree for each redshift bin.\n    - std_dev_counts: List of jackknife uncertainties (standard deviations) for each redshift bin.\n    - z_bin_centers: central values of the redshift bins\n    \"\"\"\n    original_galaxy_count = len(df)\n    print(f\"Original galaxy count: {original_galaxy_count}\")\n    \n\n \n    # compute RA edges based on the data\n    #keeping this in here in case we make bins over RA in the future\n    ra_min = df['RA_shifted'].min()\n    ra_max = df['RA_shifted'].max()\n    ra_edges = np.linspace(ra_min, ra_max, grid_cells + 1)\n    \n    # Set up Declination bins for constant declination binning\n    #manually fixing these here for this particular survey\n    dec_min =  -20# df['DEC'].min()\n    dec_max =  20# df['DEC'].max()\n    dec_bins = np.linspace(dec_min, dec_max, grid_cells + 1)  # Create declination bins\n    \n    # Calculate bin areas for constant declination bins\n    ra_width = np.diff(ra_edges)[0]  # Width of RA bins (all bins assumed equal width)\n    bin_areas = get_bin_area(dec_bins, ra_width)\n\n    # Initialize lists \n    mean_counts = []\n    std_dev_counts = []\n    sum_counts = []\n    \n    # Loop over the redshift bins\n    for z_bin in df['z_bin'].cat.categories:\n        # Step 8: Select galaxies within the current redshift bin\n        df_z_bin = df[df['z_bin'] == z_bin].copy()\n\n        # Step 9: Bin galaxies into Declination bins using precomputed dec_bins\n        # Explicitly cast the 'ra_bin' column to 'category' before assignment\n        df_z_bin['dec_bin'] = pd.cut(df_z_bin['DEC'], bins=dec_bins, include_lowest=True, right=False).astype('category')\n\n        #df_z_bin.loc[:, 'dec_bin'] = pd.cut(df_z_bin['DEC'], bins=dec_bins, include_lowest=True, right=False)\n        #df_z_bin.loc[:, 'dec_bin'] = df_z_bin['dec_bin'].astype('category')  # Use .loc to avoid SettingWithCopyWarning\n\n        # Step 10: Count galaxies in each Declination bin\n        galaxy_counts = df_z_bin.groupby('dec_bin', observed=False).size().values\n\n        # Step 11: Calculate mean galaxy counts per square degree\n        counts_per_deg2 = galaxy_counts / bin_areas\n\n        # Step 12: Compute jackknife resampling to estimate uncertainties\n        jackknife_means = []\n        for i in range(len(counts_per_deg2)):\n            jackknife_sample = np.delete(counts_per_deg2, i)\n            jackknife_means.append(np.mean(jackknife_sample))\n\n        # Step 13: Calculate mean and standard deviation of galaxy counts per square degree\n        mean_counts.append(np.mean(counts_per_deg2))\n        std_dev_counts.append(np.std(jackknife_means))\n\n    return mean_counts, std_dev_counts\n\n\ndef plot_galaxy_counts_vs_redshift_with_jackknife(counts, std_dev_counts, z_bin_centers):\n    \"\"\"\n    Plot the galaxy counts per square degree as a function of redshift with jackknife error bars.\n\n    Parameters:\n    - counts (numpy.ndarray): Array of galaxy counts per square degree for each redshift bin.\n      Shape: (number of redshift bins,) or (1, number of redshift bins).\n    - std_dev_counts (numpy.ndarray): Array of jackknife standard deviations for each redshift bin, \n      representing uncertainties. Shape: (number of redshift bins,).\n    - z_bin_centers (numpy.ndarray): Array of central values of the redshift bins. \n      Shape: (number of redshift bins,).\n\n    Notes:\n    - The input arrays (`counts`, `std_dev_counts`, and `z_bin_centers`) must have the same length, \n      corresponding to the number of redshift bins.\n    - The plot is designed specifically for redshifts in the range of 1.0 to 3.0 and may need adjustments \n      for datasets with different redshift ranges.\n    \"\"\"\n    # Ensure counts is a NumPy array\n    if isinstance(counts, list):\n        counts = np.array(counts)\n    \n    # Flatten counts if it has a shape of [1, N]\n    if counts.ndim == 2 and counts.shape[0] == 1:\n        counts = counts.flatten()\n\n    # setup figure\n    plt.figure(figsize=(10, 6))\n\n    # Plot with error bars (with jackknife standard deviation as error bars)\n    plt.errorbar(z_bin_centers, counts, yerr=std_dev_counts, fmt='o', color='blue', \n                 ecolor='gray', elinewidth=2, capsize=3, label='Galaxy Counts per Square Degree (Jackknife)')\n    plt.yscale('log')\n\n    # Set x-range to redshifts between 1 and 3\n    plt.xlim(1, 3.0)\n    plt.ylim(1E1, 3E5)\n\n    # Add labels and title\n    plt.xlabel('Redshift', fontsize=12)\n    plt.ylabel('Galaxy Count per Square Degree', fontsize=12)\n    plt.title('Galaxy Counts per Square Degree vs Redshift with Jackknife Error Bars', fontsize=14)\n\n    # Show the plot\n    plt.legend()\n    plt.show()\n    plt.close()","key":"ziQhMfSsbS"},{"type":"output","id":"R9LMoBPNWu8FZE25UDrXP","data":[],"key":"QL7aVOqgF1"}],"key":"IzBELmO00R"},{"type":"block","children":[],"key":"Gmqqv8pDvf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\n#setup redshift binning here for consistency in the rest of the code\n#these match our dataset, so only change if you want to reduce the range \n# or if you change the dataset\n\nz_min = 1.0\nz_max = 3.0\ndz = 0.1\n\n#do the counting\nmean_counts, std_dev_counts = jackknife_galaxy_count(df, grid_cells=10)\n#make the plot\nplot_galaxy_counts_vs_redshift_with_jackknife(mean_counts, std_dev_counts, z_bin_centers)","key":"DNcZJRG1JK"},{"type":"output","id":"r62wLk5gqBRe84cDgnCuy","data":[{"output_type":"stream","name":"stdout","text":"Original galaxy count: 29382943\n"},{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 1000x600 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"4ff0ee0ed91854c9eaf731031178c001","path":"/irsa-tutorials/build/4ff0ee0ed91854c9eaf731031178c001.png"}}},{"output_type":"stream","name":"stdout","text":"CPU times: user 2.57 s, sys: 59.5 ms, total: 2.63 s\nWall time: 2.65 s\n"}],"key":"jV4eSe9lMx"}],"key":"tBjCBdajMw"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":694,"column":1},"end":{"line":694,"column":1}},"children":[{"type":"text","value":"Figure Caption: Number density plot.  Note that the jackknife error bars are displayed on the plot but they are smaller than the point size.  We see the expected shape of this plot, with a fall off toward higher redshift.","position":{"start":{"line":694,"column":1},"end":{"line":694,"column":1}},"key":"FqcnjvAPkL"}],"key":"B3dauSWRZW"}],"key":"Xp6E32Wmhm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#cleanup memory when done with this file:\ndel df","key":"cMcOsrU9yk"},{"type":"output","id":"__3Dn5J1bWs8x7iCy8BLX","data":[],"key":"f2cJJujV8D"}],"key":"pQY8UsJ71y"},{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":701,"column":1},"end":{"line":701,"column":1}},"children":[{"type":"text","value":"4.0 Expand to out-of-memory sized catalogs","position":{"start":{"line":701,"column":1},"end":{"line":701,"column":1}},"key":"bm5E5ggHwk"}],"identifier":"id-4-0-expand-to-out-of-memory-sized-catalogs","label":"4.0 Expand to out-of-memory sized catalogs","html_id":"id-4-0-expand-to-out-of-memory-sized-catalogs","implicit":true,"key":"vzYqu4286b"},{"type":"paragraph","position":{"start":{"line":702,"column":1},"end":{"line":702,"column":1}},"children":[{"type":"text","value":"Originally this section attempted to use dask instead of pandas to hold the data and be able to do this work.  Dask is supposed to be able to do this sort of magic in the background with out of memory catalogs, but in reality, this does not seem to be possible, or at least not possible with a week or so of working on it.  Instead, we will read in the catalogs one at a time, store counts of mean and standard deviation per square degree and then move on to the next catalog.","position":{"start":{"line":702,"column":1},"end":{"line":702,"column":1}},"key":"vRiUrgpdEK"}],"key":"qBjugFJnQU"}],"key":"dHApPyKIRf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def calculate_area(df):\n    \"\"\"\n    Calculate the total survey area in square degrees based on DEC range and RA width.\n\n    Parameters:\n    - df: The DataFrame containing galaxy data.\n\n    Returns:\n    - area: Total survey area in square degrees.\n    \"\"\"\n    ra_min = df['RA_shifted'].min()\n    ra_max = df['RA_shifted'].max()\n    dec_min = df['DEC'].min()\n    dec_max = df['DEC'].max()\n     \n    # Step 4: Calculate area\n    ra_width = ra_max - ra_min \n    dec_height = dec_max - dec_min\n \n    # Approximate area calculation\n    dec_center = (dec_min + dec_max) / 2\n    area = ra_width * dec_height * np.cos(np.radians(dec_center))\n\n    return area","key":"XH0dF3NTcP"},{"type":"output","id":"OgmKWui_JnkRtvsY1Vk4k","data":[],"key":"k9zbBhlTQ3"}],"key":"BnDDX1LyvI"},{"type":"block","children":[],"key":"CXDhiNkaA0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def galaxy_counts_per_hdf5_binned(\n    df,\n    z_bins,\n    z_bin_centers,\n    find_halpha_bins=False,\n    halpha_flux_thresholds=None,\n):\n    \"\"\"\n    Calculate the galaxy counts per square degree for each redshift bin.\n    Optionally calculate counts for Halpha flux bins.\n\n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing galaxy data.\n    - zbins (): edges of the redshift bins\n    - z_bin_centers (numpy.ndarray): Array of central values of the redshift bins.\n    - find_halpha_bins (bool, optional): Whether to calculate counts for Halpha bins (default: False).\n    - halpha_flux_thresholds (list, optional): Custom thresholds for Halpha flux binning. If None, default thresholds are used.\n\n    Returns:\n    - counts_per_deg2 (numpy.ndarray): Array of galaxy counts per square degree for each redshift bin.\n    - halpha_counts (numpy.ndarray): Counts for each Halpha bin (if `find_halpha_bins` is True).\n    \"\"\"\n    # Default Halpha flux thresholds if not provided\n    if find_halpha_bins and halpha_flux_thresholds is None:\n        halpha_flux_thresholds = [0.5e-16, 0.7e-16, 0.9e-16, 1.1e-16, 1.3e-16, 1.5e-16, 1.7e-16, 1.9e-16]\n        \n    # Calculate survey area\n    area = calculate_area(df)\n\n    # Initialize counts\n    counts = np.zeros((1, len(z_bins) - 1))\n    halpha_counts = None\n\n    # Prepare for Halpha binning if requested\n    if find_halpha_bins:\n        halpha_counts = np.zeros((len(halpha_flux_thresholds), len(z_bins) - 1))\n\n    # Group data by redshift bin\n    grouped = df.groupby(\"z_bin\", observed= False)\n\n    for i, (z_bin, group) in enumerate(grouped):\n        # Count galaxies in the redshift bin\n        galaxy_count = len(group)\n        counts[0, i] = galaxy_count\n\n        if find_halpha_bins:\n            # Count galaxies for each Halpha bin within the current redshift bin\n            for j, threshold in enumerate(halpha_flux_thresholds):\n                halpha_count = len(group[group[\"flux_Halpha6563\"] > threshold])\n                halpha_counts[j, i] = halpha_count\n                #print(f\"redshift: {z_bin}, threshold: {threshold}, halpha_count: {halpha_count}, count: {galaxy_count}\")\n\n\n    # Sort results based on z_bin_centers\n    sorted_indices = np.argsort(z_bin_centers)\n    counts_per_deg2 = (counts / area)[:, sorted_indices]\n\n    if find_halpha_bins:\n        # Normalize Halpha counts to counts per square degree\n        halpha_counts_per_deg2 = (halpha_counts / area)[:, sorted_indices]\n    else:\n        halpha_counts_per_deg2 = None\n\n    return counts_per_deg2, halpha_counts_per_deg2\n  \ndef plot_binned_galaxy_counts_vs_redshift_with_jackknife(\n    counts_summary, z_bin_centers, halpha_flux_thresholds, halpha_summary,\n    ):\n    \"\"\"\n    Plots galaxy counts per square degree as a function of redshift with jackknife error bars.\n\n    This function visualizes the galaxy number density using binned redshift data. It includes \n    jackknife-based uncertainty estimates and optionally plots counts for Hα flux-selected bins.\n\n    Parameters:\n    - counts_summary (dict): A dictionary containing:\n        - 'sum_counts' (numpy.ndarray): Array of total galaxy counts per square degree for each redshift bin.\n        - 'std_dev_counts' (numpy.ndarray): Array of jackknife standard deviations for each redshift bin.\n    - z_bin_centers (numpy.ndarray): Array of central values of the redshift bins.\n    - halpha_flux_thresholds (list, optional): List of Hα flux threshold values (used for labeling the lines).\n    - halpha_summary (dict, optional): A dictionary containing:\n        - 'halpha_sum_counts' (numpy.ndarray): Counts for each Hα flux-selected bin.\n        - 'halpha_std_dev_counts' (numpy.ndarray): Jackknife standard deviations for each Hα flux-selected bin.\n\n    Notes:\n    - The input arrays in `counts_summary` and `halpha_summary` must have the same length as `z_bin_centers`.\n    - The function automatically sets the y-axis to a logarithmic scale (`plt.yscale(\"log\")`).\n    - The x-axis is limited to the redshift range [1, 3].\n    - The function ensures proper memory management by explicitly closing the plot.\n    \"\"\"\n    # Unpack counts_summary\n    counts = counts_summary[\"sum_counts\"]\n    std_dev_counts = counts_summary[\"std_dev_counts\"]\n\n    # Unpack halpha_summary if provided, otherwise set to None\n    if halpha_summary is not None:\n        halpha_counts = halpha_summary.get(\"halpha_sum_counts\")\n        halpha_std_dev_counts = halpha_summary.get(\"halpha_std_dev_counts\")\n    else:\n        halpha_counts = None\n        halpha_std_dev_counts = None\n\n    # Ensure counts is a NumPy array\n    if isinstance(counts, list):\n        counts = np.array(counts)\n\n    # Flatten counts if it has a shape of [1, N]\n    if counts.ndim == 2 and counts.shape[0] == 1:\n        counts = counts.flatten()\n\n    # Setup figure\n    plt.figure(figsize=(10, 6))\n\n    # Plot the main counts with error bars\n    jackknife_line = plt.errorbar(\n        z_bin_centers, counts, yerr=std_dev_counts, fmt=\"o-\", color=\"blue\",\n        ecolor=\"blue\", elinewidth=2, capsize=3, label=\"All Galaxy Counts per Square Degree (Jackknife)\"\n    )\n\n    # Plot Hα bins if the data is present\n    halpha_lines = []\n    if halpha_flux_thresholds is not None and halpha_counts is not None:\n        for i in range(len(halpha_flux_thresholds)):\n            label = f\"Flux Hα > {halpha_flux_thresholds[i]:.1e} erg/s/cm²\"\n            line, = plt.plot(z_bin_centers, halpha_counts[i], marker=\"o\", label=label)\n            halpha_lines.append(line)\n\n    # Adjust legend order if Hα lines were added\n    if halpha_lines:\n        handles, labels = plt.gca().get_legend_handles_labels()\n        new_order = [jackknife_line] + halpha_lines  \n        plt.legend(new_order, [labels[handles.index(h)] for h in new_order])\n    else:\n        plt.legend()\n\n    # Set log scale for y-axis and limit the x-axis\n    plt.yscale(\"log\")\n    plt.xlim(1, 3.0)\n    \n    # Add labels and title\n    plt.xlabel(\"Redshift\", fontsize=12)\n    plt.ylabel(\"Galaxy Count per Square Degree\", fontsize=12)\n    plt.title(\"Number Density with Jackknife Error Bars\", fontsize=14)\n    \n    # Display the plot\n    plt.show()\n    plt.close()\n\n\ndef jackknife_wrapper(file_list,  halpha_flux_thresholds, find_halpha_bins=False):\n    \"\"\"\n    Performs jackknife resampling over multiple HDF5 files to estimate uncertainties \n    in galaxy counts per square degree for each redshift bin. Optionally calculates \n    counts for Hα flux-selected bins.\n\n    Parameters:\n    - file_list (list of str): List of file paths to HDF5 datasets to process.\n    - halpha_flux_thresholds (list): List of Hα flux thresholds used for binning (if `find_halpha_bins` is True).\n    - find_halpha_bins (bool, optional): Whether to calculate counts for Hα bins (default: False).\n\n    Returns:\n    - counts_summary (dict): A dictionary containing:\n        - 'sum_counts' (numpy.ndarray): Summed galaxy counts across all HDF5 files for each redshift bin.\n          Shape: (number of redshift bins,).\n        - 'std_dev_counts' (numpy.ndarray): Jackknife standard deviations for galaxy counts per square degree.\n          Shape: (number of redshift bins,).\n       Shape: (number of redshift bins,).\n    - halpha_summary (dict or None): A dictionary containing Hα bin statistics if `find_halpha_bins` is True, otherwise `None`. \n      When present, the dictionary includes:\n        - 'halpha_sum_counts' (numpy.ndarray): Summed counts for each Hα bin.\n          Shape: (number of Hα bins, number of redshift bins).\n        - 'halpha_std_dev_counts' (numpy.ndarray): Jackknife standard deviations for Hα bins.\n          Shape: (number of Hα bins, number of redshift bins).\n\n    Notes:\n    - The function loads multiple HDF5 files and extracts galaxy counts using `galaxy_counts_per_hdf5_binned()`.\n    - Jackknife resampling is applied across files to compute standard deviations.\n    - The function saves `all_counts` to a NumPy file for debugging purposes.\n    - If `find_halpha_bins` is False, `halpha_summary` is returned as `None` to indicate that Hα binning was not performed.\n    \"\"\"\n    #fix these in case the above cells are not run\n    ra_min1=330\n    z_min = 1.0\n    z_max = 3.0\n    dz = 0.1\n    columns_to_keep = ['RA', 'DEC', 'redshift_observed', 'flux_Halpha6563']\n    columns_to_convert = ['redshift_observed', 'flux_Halpha6563']\n\n    #setup to track counts\n    all_counts = []\n    all_halpha_counts = [] if find_halpha_bins else None\n\n    for file in file_list:\n        # Load data from the file\n        df = read_hdf5_to_pandas(file, columns_to_keep, columns_to_convert)\n\n        #pre-process dataset\n        df, z_bins, z_bin_centers = assign_redshift_bins(df, z_min, z_max, dz)\n        df['RA_shifted'] = (df['RA'] - ra_min1) % 360  # Shift RA to continuous range\n\n        # Get counts (with or without Halpha binning)\n        result = galaxy_counts_per_hdf5_binned(\n            df, z_bins, z_bin_centers, find_halpha_bins=find_halpha_bins,\n            halpha_flux_thresholds=halpha_flux_thresholds\n        )\n        \n        counts_per_deg2, halpha_counts_per_deg2 = result \n        all_counts.append(counts_per_deg2.flatten())  #make sure shape is correct\n        \n        if find_halpha_bins:\n            all_halpha_counts.append(halpha_counts_per_deg2)\n\n        # Explicitly delete DataFrame for improved memory management\n        del df\n\n    # Save all_counts to a file for debugging or post-processing\n    os.makedirs(\"output\", exist_ok=True)\n    np.save(\"output/all_counts.npy\", all_counts)\n\n    # Convert to NumPy array for easier manipulation\n    all_counts = np.array(all_counts)\n\n    #debugging\n    print(\"all_counts shape:\", all_counts.shape)\n    print(\"all_counts sample:\", all_counts[:3])  # preview first 3 entries\n    print(\"all_counts variance:\", np.var(all_counts, axis=0))\n    print(\"all_counts dtype:\", all_counts.dtype)\n    \n    if find_halpha_bins:\n        all_halpha_counts = np.array(all_halpha_counts)\n\n    # Compute sum counts\n    sum_counts = np.sum(all_counts, axis=0)\n\n    # Compute jackknife standard deviations for galaxy counts\n    std_dev_counts = np.sqrt(\n        (len(all_counts) - 1) * np.var(all_counts, axis=0, ddof=0)\n    )\n\n    # Create counts summary dictionary\n    counts_summary = {\n        \"sum_counts\": sum_counts,\n        \"std_dev_counts\": std_dev_counts\n    }\n\n    # Compute jackknife standard deviations for Halpha counts (if applicable)\n    if find_halpha_bins:\n        halpha_sum_counts = np.sum(all_halpha_counts, axis=0)\n        halpha_std_dev_counts = np.sqrt(\n            (len(all_halpha_counts) - 1) * np.var(all_halpha_counts, axis=0, ddof=0)\n        )\n        # Create halpha counts summary dictionary\n        halpha_summary = {\n            \"halpha_sum_counts\": halpha_sum_counts,\n            \"halpha_std_dev_counts\": halpha_std_dev_counts\n        }\n    else:\n        halpha_summary = None  # No Halpha binning\n\n    return counts_summary, halpha_summary","key":"YPz2q3cIkf"},{"type":"output","id":"Fo1sJR2KLobp8Xf2M1Rti","data":[],"key":"fbLchhWQaP"}],"key":"UfxGZ7oiZC"},{"type":"block","children":[],"key":"x0G7joZbLd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\n\n# This uses data downloaded in section 1 above.  \n# If you want to run this with more data, return to section 1 to get that downloaded.\ndownload_dir = 'downloaded_hdf5_files'\n# Get all HDF5 files from the directory\nfile_paths = [os.path.join(download_dir, f) for f in os.listdir(download_dir) if f.endswith('.hdf5')]\n\n#out of consideration for the size of these files and the amount of memory required to work with them\n# we only keep the columns that we are going to use and convert some to 32bit instead of 64 where \n# the higher precision is not necessary to the science\ncolumns_to_keep = ['RA', 'DEC', 'redshift_observed', 'flux_Halpha6563']\ncolumns_to_convert = ['redshift_observed', 'flux_Halpha6563']\n\n# Run jackknife with Halpha binning enabled\nhalpha_flux_thresholds=[0.5e-16, 0.7e-16, 0.9e-16, 1.1e-16, 1.3e-16, 1.5e-16, 1.7e-16, 1.9e-16]\nresults = jackknife_wrapper(\n    file_paths, halpha_flux_thresholds, find_halpha_bins=True\n)\n\n# Unpack results\ncounts_summary, halpha_summary = results\n\n# Plot the results\nplot_binned_galaxy_counts_vs_redshift_with_jackknife(\n    counts_summary, z_bin_centers, halpha_flux_thresholds, halpha_summary)","key":"T5L7wCDk5Q"},{"type":"output","id":"JKDfPidwj9PbxmaRJoc1U","data":[{"output_type":"stream","name":"stdout","text":"all_counts shape: (1, 20)\nall_counts sample: [[908.19889809 747.2333674  794.9342837  676.17631576 655.19747312\n  588.94982189 513.17670351 456.29600034 383.57721419 330.13751134\n  267.43061388 217.22499701 175.30681804 144.16250588 112.40633963\n   85.80426641  63.65060463  43.55867883  29.37146612  17.62327474]]\nall_counts variance: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nall_counts dtype: float64\n"},{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 1000x600 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"63b0962c8fa364a02bdd6e4f049b146e","path":"/irsa-tutorials/build/63b0962c8fa364a02bdd6e4f049b146e.png"}}},{"output_type":"stream","name":"stdout","text":"CPU times: user 5.53 s, sys: 10.1 s, total: 15.6 s\nWall time: 32.7 s\n"}],"key":"uowiGWAYZX"}],"key":"aXcN0ZzYRN"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":1023,"column":1},"end":{"line":1023,"column":1}},"children":[{"type":"text","value":"Figure Caption: Number Density plots color coded by Halpha flux.  Slight variations are noted in the shape of the curves as a function of Halpha flux, especially at higher redshifts.  This plot may be a factor of 10 below the section 3 number density plot if you have not let it download all 10 input files.  The 10 input files cover the same area on the sky, so the density will look smaller if fewer files are used.","position":{"start":{"line":1023,"column":1},"end":{"line":1023,"column":1}},"key":"XXRmrbNgOM"}],"key":"BvsFMnis9z"}],"key":"ToicZaaqtf"},{"type":"block","position":{"start":{"line":1025,"column":1},"end":{"line":1025,"column":1}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1027,"column":1},"end":{"line":1027,"column":1}},"children":[{"type":"text","value":"4.1 Explore jackknife uncertainties","position":{"start":{"line":1027,"column":1},"end":{"line":1027,"column":1}},"key":"uIQ1Vn6RpZ"}],"identifier":"id-4-1-explore-jackknife-uncertainties","label":"4.1 Explore jackknife uncertainties","html_id":"id-4-1-explore-jackknife-uncertainties","implicit":true,"key":"h2aDyr1MyZ"},{"type":"paragraph","position":{"start":{"line":1028,"column":1},"end":{"line":1028,"column":1}},"children":[{"type":"text","value":"The uncertainties are plotted as error bars on the data points above.  There are no jackknife uncertainties if you are only using one file, so they will in that case not be visible on the plot.  This section is for the case where you are using more than one downloaded input file.  Here we explore and plot the jackknife uncertainties in a way that makes them visible.","position":{"start":{"line":1028,"column":1},"end":{"line":1028,"column":1}},"key":"c36ONaQvSs"}],"key":"HTe3vc6szR"}],"key":"HZCJeoOExs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def plot_jackknife_fractional_uncertainty(z_bin_centers, counts, std_dev_counts):\n    \"\"\"\n    Plots the fractional jackknife uncertainty (σ / N) as a function of redshift.\n    \n    Parameters\n    ----------\n    z_bin_centers : array-like\n        The center of each redshift bin.\n    counts : array-like\n        The total galaxy counts per redshift bin.\n    std_dev_counts : array-like\n        The jackknife standard deviation per redshift bin.\n    \"\"\"\n    fractional_uncertainty = std_dev_counts / counts\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(z_bin_centers, fractional_uncertainty, marker='o', linestyle='-', color='purple')\n    #plt.ylim(0, 0.01)\n    plt.xlabel('Redshift')\n    plt.ylabel('Fractional Uncertainty')\n    plt.title('Jackknife Fractional Uncertainty vs. Redshift')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    plt.close()","key":"R6HjLGAiFI"},{"type":"output","id":"wsEFAx_DTQWO_Iq-CwPlP","data":[],"key":"ubaNToWhAn"}],"key":"MbcUUb3Yxo"},{"type":"block","children":[],"key":"xjvjKgegfd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"#make fractional uncertainty plot\ncounts_per_deg2 = counts_summary[\"sum_counts\"].flatten()\nstd_dev_counts = counts_summary[\"std_dev_counts\"].flatten()\nplot_jackknife_fractional_uncertainty(z_bin_centers, counts_per_deg2, std_dev_counts)","key":"V0WPzzR0r8"},{"type":"output","id":"2O6fnrnleSyC4J8afW_Zd","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 800x500 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"5607e99ff2829fc0c30bebd4be6c8607","path":"/irsa-tutorials/build/5607e99ff2829fc0c30bebd4be6c8607.png"}}}],"key":"dotrj6N33P"}],"key":"eq2htuWrRm"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":1065,"column":1},"end":{"line":1065,"column":1}},"children":[{"type":"text","value":"Figure Caption:  The fractional uncertainty is the standard deviation from jackknife sampling divided by the number of galaxies per square degree, here plotted as a function of redshift.  Note that if you are using only 1 downloaded file this will be a flat plot with no uncertainties.  However, if you are using all 10 input files, you will see the expected rise in uncertainty with increasing redshift.","position":{"start":{"line":1065,"column":1},"end":{"line":1065,"column":1}},"key":"f6ZPf4mHYB"}],"key":"ltqXnR9xuf"}],"key":"LIj67NWK8N"},{"type":"block","position":{"start":{"line":1067,"column":1},"end":{"line":1067,"column":1}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1069,"column":1},"end":{"line":1069,"column":1}},"children":[{"type":"text","value":"Acknowledgements","position":{"start":{"line":1069,"column":1},"end":{"line":1069,"column":1}},"key":"yBCmTiIHFg"}],"identifier":"acknowledgements","label":"Acknowledgements","html_id":"acknowledgements","implicit":true,"key":"e1yAL1GImD"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1071,"column":1},"end":{"line":1072,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1071,"column":1},"end":{"line":1072,"column":1}},"children":[{"type":"link","url":"https://irsa.ipac.caltech.edu/","position":{"start":{"line":1071,"column":1},"end":{"line":1071,"column":1}},"children":[{"type":"text","value":"IPAC-IRSA","position":{"start":{"line":1071,"column":1},"end":{"line":1071,"column":1}},"key":"VeMJZg2YS6"}],"urlSource":"https://irsa.ipac.caltech.edu/","key":"BWrFkcc9WV"}],"key":"Y5ourYnTNF"}],"key":"wbvJ1nSid4"}],"key":"Lq31WSjjVM"},{"type":"block","position":{"start":{"line":1073,"column":1},"end":{"line":1073,"column":1}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1075,"column":1},"end":{"line":1075,"column":1}},"children":[{"type":"text","value":"About this notebook","position":{"start":{"line":1075,"column":1},"end":{"line":1075,"column":1}},"key":"s8ni7KG2wD"}],"identifier":"about-this-notebook","label":"About this notebook","html_id":"about-this-notebook","implicit":true,"key":"p8lDBhPMRg"},{"type":"paragraph","position":{"start":{"line":1076,"column":1},"end":{"line":1076,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1076,"column":1},"end":{"line":1076,"column":1}},"children":[{"type":"text","value":"Authors","position":{"start":{"line":1076,"column":1},"end":{"line":1076,"column":1}},"key":"z8pCVViT23"}],"key":"gmuU6AOlwT"},{"type":"text","value":": Jessica Krick in conjunction with the IPAC Science Platform Team","position":{"start":{"line":1076,"column":1},"end":{"line":1076,"column":1}},"key":"bvVW8aCTkm"}],"key":"ynCQ0ekG3r"},{"type":"paragraph","position":{"start":{"line":1078,"column":1},"end":{"line":1079,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1078,"column":1},"end":{"line":1078,"column":1}},"children":[{"type":"text","value":"Contact:","position":{"start":{"line":1078,"column":1},"end":{"line":1078,"column":1}},"key":"ZeGUz7lYkj"}],"key":"AZBNF3Yrlg"},{"type":"text","value":" ","position":{"start":{"line":1078,"column":1},"end":{"line":1078,"column":1}},"key":"lK99Rd0Xae"},{"type":"link","url":"https://irsa.ipac.caltech.edu/docs/help_desk.html","position":{"start":{"line":1078,"column":1},"end":{"line":1078,"column":1}},"children":[{"type":"text","value":"IRSA Helpdesk","position":{"start":{"line":1078,"column":1},"end":{"line":1078,"column":1}},"key":"Kc3RakjshP"}],"urlSource":"https://irsa.ipac.caltech.edu/docs/help_desk.html","key":"QSHR0Z026j"},{"type":"text","value":" with questions\nor problems.","position":{"start":{"line":1078,"column":1},"end":{"line":1078,"column":1}},"key":"BoQg3rFXM5"}],"key":"wssOjOMBPa"},{"type":"paragraph","position":{"start":{"line":1081,"column":1},"end":{"line":1081,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1081,"column":1},"end":{"line":1081,"column":1}},"children":[{"type":"text","value":"Updated:","position":{"start":{"line":1081,"column":1},"end":{"line":1081,"column":1}},"key":"A0VKt4c7eh"}],"key":"o9ESRmQla0"},{"type":"text","value":" 2025-04-01","position":{"start":{"line":1081,"column":1},"end":{"line":1081,"column":1}},"key":"suiH9fY5B2"}],"key":"kWz2ThacGY"},{"type":"paragraph","position":{"start":{"line":1083,"column":1},"end":{"line":1085,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1083,"column":1},"end":{"line":1083,"column":1}},"children":[{"type":"text","value":"Runtime:","position":{"start":{"line":1083,"column":1},"end":{"line":1083,"column":1}},"key":"CvKPUMdaDD"}],"key":"HbOYWCtyJl"},{"type":"text","value":" As of the date above, running on the ","position":{"start":{"line":1083,"column":1},"end":{"line":1083,"column":1}},"key":"lq7Wzu3ume"},{"type":"link","url":"https://pcos.gsfc.nasa.gov/Fornax/","position":{"start":{"line":1083,"column":1},"end":{"line":1083,"column":1}},"children":[{"type":"text","value":"Fornax Science Platform","position":{"start":{"line":1083,"column":1},"end":{"line":1083,"column":1}},"key":"fbrYIO8YfJ"}],"urlSource":"https://pcos.gsfc.nasa.gov/Fornax/","key":"WjSLTUvCNP"},{"type":"text","value":", this notebook takes about 9 minutes to run to completion on\na machine with 8GB RAM and 4 CPU.\nThis runtime inlcudes the notebook as is, using only one datafile out of 10.  Runtime will be greater than 30 min. longer if the code is allowed to download and work with all 10 datafiles.","position":{"start":{"line":1083,"column":1},"end":{"line":1083,"column":1}},"key":"GrmAnBwMDc"}],"key":"gNH3a9OoaA"}],"key":"OqFBxA2QFq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"","key":"NZg3lR71RZ"},{"type":"output","id":"rXMUlKsEksguEnfsVf9Ib","data":[],"key":"zP7KX3oZcT"}],"key":"zwufBg4Q1B"}],"key":"A0iEbKINnJ"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Searching for contributed COSMOS images","url":"/sia-cosmos","group":"COSMOS"},"next":{"title":"Analyzing cloud-hosted simulated Roman coadded images","url":"/openuniverse2024-roman-simulated-wideareasurvey","group":"OpenUniverse 2024"}}},"domain":"http://localhost:3000"}